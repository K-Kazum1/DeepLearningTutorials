{"metadata":{"colab":{"name":"keras_demo.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Approximating Arbitrary Functions with Keras\nAll neural networks are in essense, trainable output approximation function.\n\nEvery input passed through a neural network will have some amount of error associated to the output.\n\nTraining is done by passing an input to the neural network to get the error, and minimizing the error, by tweaking the internal parameters.\n\nThis can be done because every computation done in a neural network is fully differentiable, so you can calculate how much the error will change if you change any parameter.\n\nIn this notebook, we will construct a simple function that takes in a number of inputs, and gives back a single number from it. We will be using a neural network to approximate that function.\n\nThis kind of task is called regression.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nfrom keras.models import Model\nfrom keras.layers import Input,Dense,Activation","metadata":{"id":"5xduuKk4U-Xh"},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Define our data","metadata":{}},{"cell_type":"code","source":"def data(n):\n    x=np.random.uniform(size=(n,3))\n    y=x[:,0]*2**x[:,1]-(x[:,1]+x[:,2])**2\n\n    return x,np.expand_dims(y,1)","metadata":{},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Defining our Model\nThe neural network will take a tensor with 3 dimension and outputs a tensor with 1 dimension. \n\nIn a regression problem, the cost function to use is the mean squared error loss, given by $\\frac{1}{n}\\sum_{i=1}^{n}(y_{i} - x_{i})^2$.\n\nThis assigns a larger loss to datapoints that are more off, compared to similar loss functions like l1 norm, which are means of absolute","metadata":{}},{"cell_type":"code","source":"def model():\n    inp=Input(3)\n    y = Dense(10)(inp)\n    x = Activation('tanh')(y)\n\n    x=x+y\n\n    y = Dense(10)(x)\n    x = Activation('tanh')(y)\n\n    x=x+y\n\n    y = Dense(10)(x)\n    x = Activation('tanh')(y)\n\n    x = x + y\n\n    x = Dense(1)(x)\n\n    model=Model(inp,x)\n    model.compile('rmsprop',loss='mse')\n\n    return model\n\nmodel=model()\nmodel.summary()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k4pDaqmfVaOP","outputId":"f827c27f-f96a-4682-c430-e624985ac0e3"},"execution_count":9,"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 3)]          0                                            \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 10)           40          input_1[0][0]                    \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 10)           0           dense[0][0]                      \n__________________________________________________________________________________________________\ntf_op_layer_AddV2 (TensorFlowOp [(None, 10)]         0           activation[0][0]                 \n                                                                 dense[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 10)           110         tf_op_layer_AddV2[0][0]          \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 10)           0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ntf_op_layer_AddV2_1 (TensorFlow [(None, 10)]         0           activation_1[0][0]               \n                                                                 dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 10)           110         tf_op_layer_AddV2_1[0][0]        \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 10)           0           dense_2[0][0]                    \n__________________________________________________________________________________________________\ntf_op_layer_AddV2_2 (TensorFlow [(None, 10)]         0           activation_2[0][0]               \n                                                                 dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 1)            11          tf_op_layer_AddV2_2[0][0]        \n==================================================================================================\nTotal params: 271\nTrainable params: 271\nNon-trainable params: 0\n__________________________________________________________________________________________________\n"}]},{"cell_type":"markdown","source":"In Keras, if everything is set up right, you only need to get the data and call `fit` on your model.","metadata":{}},{"cell_type":"code","source":"for i in range(20):\n    model.fit(*data(10000))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZY5OEOecViVE","outputId":"1baa615e-d740-4e96-add5-4072932ee1fc"},"execution_count":10,"outputs":[{"name":"stdout","output_type":"stream","text":"313/313 [==============================] - 3s 8ms/step - loss: 0.2728\n313/313 [==============================] - 4s 11ms/step - loss: 0.0313\n313/313 [==============================] - 4s 12ms/step - loss: 0.0218\n313/313 [==============================] - 4s 12ms/step - loss: 0.0164\n313/313 [==============================] - 4s 11ms/step - loss: 0.0131\n313/313 [==============================] - 4s 12ms/step - loss: 0.0100\n313/313 [==============================] - 4s 13ms/step - loss: 0.0087\n313/313 [==============================] - 4s 11ms/step - loss: 0.0078\n313/313 [==============================] - 3s 11ms/step - loss: 0.0071\n313/313 [==============================] - 3s 11ms/step - loss: 0.0066\n313/313 [==============================] - 3s 10ms/step - loss: 0.0059\n313/313 [==============================] - 3s 11ms/step - loss: 0.0058\n313/313 [==============================] - 4s 13ms/step - loss: 0.0054: 0s - loss: 0.005 - ETA: 0s - loss: 0.00\n313/313 [==============================] - 4s 12ms/step - loss: 0.0050: 0s - loss: 0.00\n313/313 [==============================] - 4s 13ms/step - loss: 0.0047\n313/313 [==============================] - 3s 10ms/step - loss: 0.0046\n313/313 [==============================] - 5s 15ms/step - loss: 0.0043\n313/313 [==============================] - 4s 13ms/step - loss: 0.0043\n313/313 [==============================] - 4s 12ms/step - loss: 0.0041\n313/313 [==============================] - 4s 11ms/step - loss: 0.0040\n"}]},{"cell_type":"code","source":"# Plot the loss function given a list\ndef plot(data):\n    display.clear_output(wait=True)\n    plt.clf()\n    \n    ax = plt.gca()\n    ax.yaxis.tick_right()\n    ax.yaxis.set_ticks_position('both')\n    ax.yaxis.grid(True)\n    \n    plt.plot(data)\n    plt.legend(['Loss'], loc='lower left')\n    \n    \n    display.display(plt.gcf())\n\n# Given 3 numbers and one axis, plots the model and the function when you move along one axis but hold the rest in place\ndef manipulate(x0,x1,x2,d):\n    ranges = torch.arange(-1,1.2,0.2)\n    \n    x = torch.cat([torch.full( (len(ranges),1), i ) for i in [x0,x1,x2]],1)\n    \n    x[:,d] = ranges\n    x_values = value(x)\n    values = model(x).detach()\n\n    plt.plot(ranges,values)\n    plt.plot(ranges,x_values)\n    plt.legend(['model','data','difference'],loc='lower right')\n    plt.show()\n    \n    mean_squared_error = torch.mean((values-x_values)**2)\n    \n    print(f\"mean_squared_error = {mean_squared_error}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BFUg2RAvVwAF","outputId":"61650709-8fcf-4cb8-c1eb-4eed75296240"},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\ninteract(manipulate,x0=(-1,1,0.1),x1=(-1,1,0.1),x2=(-1,1,0.1),d=(0,2))","metadata":{"id":"g-uafc7AWHDe"},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"afc8c488cb62479799f9d51e371ac5e2","version_major":2,"version_minor":0},"text/plain":"interactive(children=(FloatSlider(value=0.0, description='x0', max=1.0, min=-1.0), FloatSlider(value=0.0, descâ€¦"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"<function __main__.manipulate(x0, x1, x2, d)>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}