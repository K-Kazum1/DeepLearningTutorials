{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Embedding Images into Neural Networks Using Positional Encoding\n\nEncoding data into a small network is useful for a variety of applications. \n\nOne reason might be to compress data. It's possible to encode the entire data in a model that is much smaller, with minimal quality loss.\n\nAnother might be to interpolate data. It's possible to train the network with only a few examples you have in hand, and make it generate all the data interpolated, like NeRF and SIREN.\n\n# Model inputs and outputs\n\nIn this notebook, we will train a model that decodes x and y coordinates of pixels to predict the color value of that coordinate.\n\nThe input for our neural network will be pair of x and y coordinates, (batch_size, x_encoding_length) and (batch_size, y_encoding_length).\nEncoding length will be the log base 2 of the width and height shape + 2. We add 2 to have more redudant information for the network to use.\n\nThe output will be the RGB value, (batch_size, 3).\n\n# Library imports\n\nWe will use the PyTorch library for our model, and define everything else we need.","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.optim as optim\nimport numpy as np\nimport math\n\nfrom PIL import Image","metadata":{},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Data Setup\n\nDrop an image in the images folder that we want to embed in a network.\nWe will first convert the image into a numpy array, and divide by 255 so the color values range from 0 to 1.\n\nWe need the image width and height, and define our encoding length as the log base 2 of the length, and add 2 to it.\n\nWe will define the our positional encoding for x and y coordinates.\n\n# Positional Encoding\n\nIn machine learning, sometimes you need to represent coordinates, or just numbers. As an example, let's see how we can encode a number from 0 to N.\n\nYou can encode every number you want as one hot encoding, meaning you have a matrix with dimension (batch_size, N), and encode the number N as the Nth dimension being 1 and the rest is 0.\nBut this is very wasteful use of a matrix, since the dimension grows linearly with N. We can do better.\n\nWe can instead encode the number as binary, meaning now we can use a matrix with dimension (batch_size, log2(N)). This is much better in terms of the size of the matrix, but in practice, networks trained this way does not perform very well. This is because the encoding is very not smooth. We want an encoding where if a and b are close, then encode(a) and encode(b) is also close. With binary, binary(7) is 111, while binary(8) is 1000. Every bit has changed, so it is hard for the network to see those numbers as close.\n\nInstead, we can use positional encoding. Positional encoding is like a smooth version of a binary expansion of numbers. \n\nThe formula for positional encoding, where pos is the number representing the position, i is the i-th row of the encoding, and a is an arbitrary number, is\n\n$PE_i(pos)=\\sin(a^i pos)$\n\n","metadata":{}},{"cell_type":"code","source":"imname=\"images/Darga117.jpeg\"\nimage=np.array(Image.open(imname))/255\n\nimgshape=list(image.shape[:2])\n\ndatashape=(int(math.log(imgshape[0],2)+2),int(math.log(imgshape[1],2))+2)\n\nencode_x = np.vectorize(lambda i:np.array([math.sin(i*2**-(j-1)) for j in range(datashape[0])]), signature=f\"()->(datashape)\")\nencode_y = np.vectorize(lambda i:np.array([math.sin(i*2**-(j-1)) for j in range(datashape[1])]), signature=f\"()->(datashape)\")","metadata":{},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Generate all positional encodings for every coordinates first, so that it doesn't need to generate every sample\n\ndef alldata():\n    x = np.arange(0,imgshape[0])\n    y = np.arange(0,imgshape[1])\n\n    x=encode_x(x)\n    y=encode_y(y)\n\n    return x,y\n\nalldata=alldata()\n\n# Sample from positional encodings, and corresponding RGB values of the image\n\ndef data(num):\n    s1=np.random.randint(0,imgshape[0],num)\n    s2=np.random.randint(0,imgshape[1],num)    \n    \n    x1=torch.from_numpy(alldata[0][s1]).type(torch.float)\n    x2=torch.from_numpy(alldata[1][s2]).type(torch.float)\n    \n    y=torch.from_numpy(image[s1,s2]).type(torch.float)\n    \n    return (x1,x2),y\n\n# Reconstruct the image from the model by runniing all positional encodings and reshaping them into \n\ndef test():\n    x1 = torch.from_numpy(alldata[0].repeat(imgshape[1],0)).type(torch.float)\n    x2 = torch.from_numpy(np.concatenate([alldata[1]]*imgshape[0],0)).type(torch.float)\n    \n    x = model(x1,x2)\n    x = np.uint8(x.reshape(imgshape+[-1]).detach().numpy()*255)\n\n    Image.fromarray(x).save('images/out.png')","metadata":{},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Defining Our Model\n\nThe input to our model are pairs of x and y coordinates, (batch_size, x_encoding_length) and (batch_size, y_encoding_length).\n\nThe output will be the RGB value, (batch_size, 3).\n\nHow do we get use the input to get the output? we will need to define a model that does that. \n\n# ReZero\n\nReZero, or Residual with zero initialization, is a residual connection, except with a trainable parameter multiplying the shortcut initialized as 0. It trains better than a vanilla residual connection. The formula for ReZero is just $x + a y $ where $a$ is a parameter initialized as 0 before training, and $y = F(x)$, where $F$ is any network you want to do a shortcut.\n\n# Gating\n\nGating information is a good way of only propagating information that is needed to the next layer, and discarding everything else. LSTM layers heavily use gating so the network can learn long distance information.\n\nHere, we define a network that gets its inspiration from Attention layers. It runs the input through 2 linear layer and then do an elementwise multiplication. The result is passed through a Tanh layer, gating the information that gets multiplied by V.\n\nYou don't need to have ReZero or Gating just to make a neural network, but it's better than only using linear layers.","metadata":{}},{"cell_type":"code","source":"class ReZero(nn.Module):\n    def __init__(self, alpha=0.01):\n        super(ReZeroShortcut, self).__init__()\n        self.alpha = nn.parameter.Parameter(torch.ones(1) * alpha)\n\n    def forward(self, shortcut, x):\n        return shortcut + self.alpha * x\n\nclass Gating(nn.Module):\n    def __init__(self,i,m,o=-1):\n        super(Gating,self).__init__()\n        self.Q = nn.Linear(i,m)\n        self.K = nn.Linear(i,m)\n        self.V = nn.Linear(i,m)\n\n        self.tanh = nn.Tanh()\n        self.norm = nn.BatchNorm1d(m)\n        self.gelu = nn.GELU()\n\n    def forward(self,x):\n\n        q = self.Q(x)\n        k = self.K(x)\n        v = self.V(x)\n        \n        gate = self.tanh(q*k)\n        x = gate*v        \n        x = self.norm(x)\n        x = self.gelu(x)\n        \n        return x    \n\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model,self).__init__()\n        size=100\n\n        self.fcx = Gating(datashape[0],size)\n        self.fcy = Gating(datashape[1],size)\n        \n        self.fc1 = Gating(2*size, size)\n        self.fc2 = Gating(size, size)\n        self.fc3 = Gating(size, size)\n        self.fc4 = Gating(size, size)\n        self.fc5 = Gating(size, size)\n        self.fc6 = nn.Linear(size, 3)\n        \n\n        self.sc1 = ReZero()\n        self.sc2 = ReZero()\n\n    def forward(self,x,y):\n        x = self.fcx(x)\n        y = self.fcy(y)\n\n        x = torch.cat((x,y),1)\n        \n        x = self.fc1(x)\n        \n        y = self.fc2(x)\n        y = self.fc3(y)\n        \n        x = self.sc1(x,y)\n        \n        y = self.fc4(x)\n        y = self.fc5(y)\n        \n        x = self.sc2(x,y)\n        \n        x = self.fc6(x)\n        x = torch.sigmoid(x)\n        \n        return x\n    \nmodel=Model()\noptimizer = optim.RMSprop(model.parameters(), lr=0.001)\nmse_loss = nn.MSELoss()","metadata":{},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Train loop\n Now we get to setup a train loop for our network. In pytorch, there are 4 basic step to train a model. \n \n First, you need to prepare the data. You can either generate data, or load from a dataset for the input and output.\n \n Next, you need to get the output prediction of the data from the model, and compute the loss. The loss is a single number that measures how far off from the target, given the data. In this case, we're using an L2 loss, or a mean squared error. It is calculated by taking the average of the squared difference between model prediction and the target. \n \n We then take the gradient of the loss that we just computed, which will tell the network how to best tweak all its parameters so that the loss would be minimized, so that the predicted output would be closer to the target. \n \n Finally, we update the parameters of the network using the oprimizer. Different optimizer keeps track of past gradients it already saw, and tries to be smart about how to update the network. For example, by having a moving average so that it can keep moving when it gets to a flat gradient area, or ignore small noise, or update quicker when all the past updates strongly suggest the direction is where it needs to go, etc.","metadata":{}},{"cell_type":"code","source":"i = 0\nlosses = []\nwhile True:\n    i+=1\n    x,y=data(10000)    \n            \n    optimizer.zero_grad()\n\n    loss = mse_loss(model(*x),y)\n\n    loss.backward()\n\n    optimizer.step()\n    \n    losses.append(loss.item())    \n    \n    if i%10==0:\n        print(i, loss.item(), sum(losses)/len(losses))\n    if i%100==0:\n        test()","metadata":{},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"10 0.05127786099910736 0.051755746454000474\n20 0.049994029104709625 0.051350790448486804\n30 0.0490322969853878 0.05093179817001025\n40 0.04837393760681152 0.05045821899548173\n50 0.04791867733001709 0.05001378245651722\n60 0.04659176990389824 0.04958857757349809\n70 0.04690074175596237 0.049172211119106836\n80 0.04644962400197983 0.048823737911880015\n90 0.045071497559547424 0.04845580392413669\n100 0.043894167989492416 0.04810748875141144\n"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}