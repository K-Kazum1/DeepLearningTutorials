{"metadata":{"colab":{"name":"KerasRL_demo.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.8"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Making an agent to play Open AI's Cartpool with Reinforcement Learning\n\nReinforcement learning is a method for controlling an agent to do any tasks, given observations of the environment state and the rewards associated to actions it takes responding to it. \n\nThe CartPole environment provided by Open AI, where there is a freely swinging pole pointing upwards attached to a cart that can be controlled by move horizontally, and the goal is for the pole to not fall down.\n\nThe observation of the envirement is a list of 4 numbers, the position of cart, the velocity of cart, the angle of pole, and the rotation rate of pole.","metadata":{"id":"BLUHEr6vmXdm"}},{"cell_type":"code","source":"import gym\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Activation, Flatten, Reshape\n\n\nfrom rl.agents.cem import CEMAgent\nfrom rl.memory import EpisodeParameterMemory\n\nenv = gym.make('CartPole-v0')","metadata":{"id":"ihfE7qV3XXyg"},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"#Define our reinforcement learning agent\n\nThe reinforcement learning agent will observe the environment state, and return a score for each action it can take. \n\nFor this problem, we can get the observation shape from `env.observation_space.shape`, and the action space from `env.action_space.n`.","metadata":{"id":"h9vy7U16JZmn"}},{"cell_type":"code","source":"def model():\n    inp=Input(env.observation_space.shape)\n\n    y=Dense(20)(inp)\n    x=Activation('elu')(y)\n\n    x = Dense(20)(x)\n    x = Activation('elu')(x) + y\n\n    y = Dense(20)(x)\n    x = Activation('elu')(x)\n\n    x = Dense(20)(x)\n    x = Activation('elu')(x) + y\n\n    x = Dense(20)(x)\n    x = Activation('elu')(x)\n\n    x = Dense(env.action_space.n)(x)\n    x = Activation('softmax')(x)\n\n    return Model(inp,x)\n\nmodel=model()\n\nprint(model.summary())","metadata":{"id":"vTUiQRgFXmA6"},"execution_count":2,"outputs":[{"name":"stdout","output_type":"stream","text":"Model: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 4)]          0                                            \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 20)           100         input_1[0][0]                    \n__________________________________________________________________________________________________\nactivation (Activation)         (None, 20)           0           dense[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 20)           420         activation[0][0]                 \n__________________________________________________________________________________________________\nactivation_1 (Activation)       (None, 20)           0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ntf_op_layer_AddV2 (TensorFlowOp [(None, 20)]         0           activation_1[0][0]               \n                                                                 dense[0][0]                      \n__________________________________________________________________________________________________\nactivation_2 (Activation)       (None, 20)           0           tf_op_layer_AddV2[0][0]          \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 20)           420         activation_2[0][0]               \n__________________________________________________________________________________________________\nactivation_3 (Activation)       (None, 20)           0           dense_3[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 20)           420         tf_op_layer_AddV2[0][0]          \n__________________________________________________________________________________________________\ntf_op_layer_AddV2_1 (TensorFlow [(None, 20)]         0           activation_3[0][0]               \n                                                                 dense_2[0][0]                    \n__________________________________________________________________________________________________\ndense_4 (Dense)                 (None, 20)           420         tf_op_layer_AddV2_1[0][0]        \n__________________________________________________________________________________________________\nactivation_4 (Activation)       (None, 20)           0           dense_4[0][0]                    \n__________________________________________________________________________________________________\ndense_5 (Dense)                 (None, 2)            42          activation_4[0][0]               \n__________________________________________________________________________________________________\nactivation_5 (Activation)       (None, 2)            0           dense_5[0][0]                    \n==================================================================================================\nTotal params: 1,822\nTrainable params: 1,822\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n"}]},{"cell_type":"markdown","source":"# Training the Model\nKeras RL makes reinforcement learning very simple. You define the environment, which in this case is provided, you set the memory, and the reinforcement learning algorithm, compile, and fit.\n\nThe finished model should give the best action to take the highest score.","metadata":{"id":"5LHo_jQq8nXm"}},{"cell_type":"code","source":"memory = EpisodeParameterMemory(limit=1000, window_length=1)\ncem = CEMAgent(model=model, nb_actions=2, memory=memory,\n               batch_size=50, nb_steps_warmup=2000, train_interval=50, elite_frac=0.05)\ncem.compile()\n\ncem.fit(env, nb_steps=100000, visualize=False, verbose=2)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yKiPcrL-YK4-","outputId":"7afb5fa7-bcdb-4035-e9fb-9918ca85973e"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"Training for 100000 steps ...\nWARNING:tensorflow:Model was constructed with shape (None, 4) for input Tensor(\"input_1:0\", shape=(None, 4), dtype=float32), but it was called on an input with incompatible shape (1, 1, 4).\n    31/100000: episode: 1, duration: 4.292s, episode steps: 31, steps per second: 7, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.419 [0.000, 1.000], mean observation: -0.023 [-1.392, 1.947], mean_best_reward: --\n    45/100000: episode: 2, duration: 0.802s, episode steps: 14, steps per second: 17, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.095 [-1.938, 1.134], mean_best_reward: --\n    57/100000: episode: 3, duration: 0.703s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.113 [-0.960, 1.523], mean_best_reward: --\n    69/100000: episode: 4, duration: 0.884s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.093 [-1.619, 2.581], mean_best_reward: --\n    86/100000: episode: 5, duration: 1.118s, episode steps: 17, steps per second: 15, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.095 [-2.378, 1.402], mean_best_reward: --\n    97/100000: episode: 6, duration: 0.979s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.158 [-1.518, 2.531], mean_best_reward: --\n   108/100000: episode: 7, duration: 0.810s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.096 [-2.264, 1.416], mean_best_reward: --\n   119/100000: episode: 8, duration: 0.908s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.088 [-1.020, 1.614], mean_best_reward: --\n   133/100000: episode: 9, duration: 1.609s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.857 [0.000, 1.000], mean observation: -0.072 [-2.999, 2.001], mean_best_reward: --\n   149/100000: episode: 10, duration: 1.698s, episode steps: 16, steps per second: 9, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.101 [-1.544, 2.514], mean_best_reward: --\n   181/100000: episode: 11, duration: 2.190s, episode steps: 32, steps per second: 15, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.104 [-0.551, 0.956], mean_best_reward: --\n   192/100000: episode: 12, duration: 1.304s, episode steps: 11, steps per second: 8, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.138 [-1.373, 2.336], mean_best_reward: --\n   208/100000: episode: 13, duration: 1.508s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.188 [0.000, 1.000], mean observation: 0.091 [-1.901, 2.974], mean_best_reward: --\n   254/100000: episode: 14, duration: 4.187s, episode steps: 46, steps per second: 11, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.135 [-0.600, 1.541], mean_best_reward: --\n   387/100000: episode: 15, duration: 9.411s, episode steps: 133, steps per second: 14, episode reward: 133.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.534 [0.000, 1.000], mean observation: 0.448 [-1.215, 2.424], mean_best_reward: --\n   403/100000: episode: 16, duration: 1.202s, episode steps: 16, steps per second: 13, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.069 [-1.190, 2.023], mean_best_reward: --\n   414/100000: episode: 17, duration: 0.898s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.126 [-2.266, 1.368], mean_best_reward: --\n   423/100000: episode: 18, duration: 0.483s, episode steps: 9, steps per second: 19, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.131 [-1.358, 2.241], mean_best_reward: --\n   445/100000: episode: 19, duration: 1.212s, episode steps: 22, steps per second: 18, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.094 [-0.565, 1.128], mean_best_reward: --\n   455/100000: episode: 20, duration: 0.797s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.149 [-1.908, 3.049], mean_best_reward: --\n   467/100000: episode: 21, duration: 0.808s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.120 [-1.967, 3.061], mean_best_reward: --\n   477/100000: episode: 22, duration: 0.886s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.128 [-2.051, 1.230], mean_best_reward: --\n   489/100000: episode: 23, duration: 0.911s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.087 [-1.213, 1.965], mean_best_reward: --\n   505/100000: episode: 24, duration: 1.179s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.084 [-0.761, 1.428], mean_best_reward: --\n   517/100000: episode: 25, duration: 1.107s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.078 [-1.784, 1.217], mean_best_reward: --\n   551/100000: episode: 26, duration: 2.800s, episode steps: 34, steps per second: 12, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.119 [-0.818, 0.467], mean_best_reward: --\n   591/100000: episode: 27, duration: 3.003s, episode steps: 40, steps per second: 13, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: -0.014 [-1.887, 2.493], mean_best_reward: --\n   602/100000: episode: 28, duration: 0.803s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.130 [-1.746, 2.800], mean_best_reward: --\n   615/100000: episode: 29, duration: 1.189s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.106 [-2.339, 1.401], mean_best_reward: --\n   624/100000: episode: 30, duration: 0.611s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.142 [-1.352, 2.252], mean_best_reward: --\n   636/100000: episode: 31, duration: 0.901s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.110 [-2.054, 1.165], mean_best_reward: --\n   646/100000: episode: 32, duration: 0.819s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.146 [-2.580, 1.579], mean_best_reward: --\n   655/100000: episode: 33, duration: 0.679s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.482, 1.571], mean_best_reward: --\n   671/100000: episode: 34, duration: 1.015s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.092 [-1.476, 0.962], mean_best_reward: --\n   682/100000: episode: 35, duration: 0.905s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.119 [-1.388, 2.210], mean_best_reward: --\n   703/100000: episode: 36, duration: 1.783s, episode steps: 21, steps per second: 12, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.090 [-2.404, 1.356], mean_best_reward: --\n   717/100000: episode: 37, duration: 1.495s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.097 [-1.846, 1.037], mean_best_reward: --\n   728/100000: episode: 38, duration: 0.812s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.108 [-2.184, 3.283], mean_best_reward: --\n   737/100000: episode: 39, duration: 1.002s, episode steps: 9, steps per second: 9, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.136 [-1.405, 2.195], mean_best_reward: --\n   756/100000: episode: 40, duration: 1.983s, episode steps: 19, steps per second: 10, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.104 [-0.747, 1.546], mean_best_reward: --\n   782/100000: episode: 41, duration: 1.803s, episode steps: 26, steps per second: 14, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.093 [-0.786, 1.758], mean_best_reward: --\n   794/100000: episode: 42, duration: 0.891s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.110 [-1.548, 2.517], mean_best_reward: --\n   804/100000: episode: 43, duration: 0.914s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.139 [-1.963, 3.033], mean_best_reward: --\n   821/100000: episode: 44, duration: 1.199s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.050 [-1.588, 2.326], mean_best_reward: --\n   835/100000: episode: 45, duration: 1.007s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.093 [-1.606, 0.956], mean_best_reward: --\n   851/100000: episode: 46, duration: 1.176s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.043 [-2.597, 1.793], mean_best_reward: --\n   864/100000: episode: 47, duration: 0.810s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.096 [-1.369, 2.147], mean_best_reward: --\n   896/100000: episode: 48, duration: 2.392s, episode steps: 32, steps per second: 13, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.594 [0.000, 1.000], mean observation: -0.060 [-2.195, 1.344], mean_best_reward: --\n   906/100000: episode: 49, duration: 1.007s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.110 [-1.997, 3.022], mean_best_reward: --\n   939/100000: episode: 50, duration: 2.506s, episode steps: 33, steps per second: 13, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.697 [0.000, 1.000], mean observation: 0.014 [-3.396, 2.526], mean_best_reward: --\n   949/100000: episode: 51, duration: 0.995s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.126 [-0.972, 1.555], mean_best_reward: --\n   978/100000: episode: 52, duration: 2.587s, episode steps: 29, steps per second: 11, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.690 [0.000, 1.000], mean observation: 0.013 [-2.961, 2.266], mean_best_reward: --\n   991/100000: episode: 53, duration: 1.091s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.110 [-0.976, 1.652], mean_best_reward: --\n  1006/100000: episode: 54, duration: 1.413s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.111 [-1.927, 1.017], mean_best_reward: --\n  1017/100000: episode: 55, duration: 0.984s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.093 [-2.242, 1.422], mean_best_reward: --\n  1036/100000: episode: 56, duration: 1.507s, episode steps: 19, steps per second: 13, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.088 [-1.378, 0.633], mean_best_reward: --\n  1054/100000: episode: 57, duration: 1.297s, episode steps: 18, steps per second: 14, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.087 [-0.749, 1.299], mean_best_reward: --\n  1068/100000: episode: 58, duration: 1.117s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.125 [-1.580, 0.752], mean_best_reward: --\n  1076/100000: episode: 59, duration: 0.879s, episode steps: 8, steps per second: 9, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.138 [-1.604, 2.523], mean_best_reward: --\n  1096/100000: episode: 60, duration: 1.012s, episode steps: 20, steps per second: 20, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.056 [-0.826, 1.101], mean_best_reward: --\n  1121/100000: episode: 61, duration: 2.097s, episode steps: 25, steps per second: 12, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.029 [-1.640, 1.030], mean_best_reward: --\n  1131/100000: episode: 62, duration: 0.694s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.122 [-2.449, 1.539], mean_best_reward: --\n  1154/100000: episode: 63, duration: 1.897s, episode steps: 23, steps per second: 12, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.071 [-1.486, 0.959], mean_best_reward: --\n  1165/100000: episode: 64, duration: 1.103s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-2.126, 3.313], mean_best_reward: --\n  1175/100000: episode: 65, duration: 0.987s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.129 [-1.953, 3.051], mean_best_reward: --\n  1185/100000: episode: 66, duration: 0.817s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.149 [-2.560, 1.535], mean_best_reward: --\n  1210/100000: episode: 67, duration: 2.206s, episode steps: 25, steps per second: 11, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.067 [-0.798, 1.222], mean_best_reward: --\n  1221/100000: episode: 68, duration: 0.986s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.140 [-2.833, 1.760], mean_best_reward: --\n  1245/100000: episode: 69, duration: 1.809s, episode steps: 24, steps per second: 13, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.064 [-1.550, 2.607], mean_best_reward: --\n  1256/100000: episode: 70, duration: 0.802s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.134 [-1.748, 0.967], mean_best_reward: --\n  1271/100000: episode: 71, duration: 1.187s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.085 [-1.794, 2.778], mean_best_reward: --\n  1288/100000: episode: 72, duration: 1.694s, episode steps: 17, steps per second: 10, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.080 [-1.319, 0.786], mean_best_reward: --\n  1301/100000: episode: 73, duration: 1.004s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.118 [-2.037, 1.176], mean_best_reward: --\n  1311/100000: episode: 74, duration: 0.990s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.173, 2.062], mean_best_reward: --\n  1335/100000: episode: 75, duration: 1.608s, episode steps: 24, steps per second: 15, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.042 [-2.557, 1.763], mean_best_reward: --\n  1348/100000: episode: 76, duration: 1.404s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.066 [-2.002, 1.360], mean_best_reward: --\n  1369/100000: episode: 77, duration: 1.488s, episode steps: 21, steps per second: 14, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.075 [-2.862, 1.793], mean_best_reward: --\n  1391/100000: episode: 78, duration: 2.102s, episode steps: 22, steps per second: 10, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.052 [-0.823, 1.252], mean_best_reward: --\n  1402/100000: episode: 79, duration: 1.001s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.114 [-2.757, 1.796], mean_best_reward: --\n  1443/100000: episode: 80, duration: 2.912s, episode steps: 41, steps per second: 14, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.463 [0.000, 1.000], mean observation: -0.094 [-0.785, 0.452], mean_best_reward: --\n  1455/100000: episode: 81, duration: 0.785s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.101 [-1.905, 1.160], mean_best_reward: --\n  1466/100000: episode: 82, duration: 0.890s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.109 [-1.398, 2.280], mean_best_reward: --\n  1489/100000: episode: 83, duration: 1.820s, episode steps: 23, steps per second: 13, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.060 [-1.421, 2.370], mean_best_reward: --\n  1516/100000: episode: 84, duration: 1.892s, episode steps: 27, steps per second: 14, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.112 [-0.608, 1.040], mean_best_reward: --\n  1526/100000: episode: 85, duration: 1.089s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.923, 3.011], mean_best_reward: --\n  1535/100000: episode: 86, duration: 0.704s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.115 [-1.752, 1.033], mean_best_reward: --\n  1657/100000: episode: 87, duration: 9.307s, episode steps: 122, steps per second: 13, episode reward: 122.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.371 [-2.812, 1.483], mean_best_reward: --\n  1670/100000: episode: 88, duration: 0.803s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.094 [-2.394, 1.600], mean_best_reward: --\n  1733/100000: episode: 89, duration: 5.298s, episode steps: 63, steps per second: 12, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.040 [-1.159, 1.529], mean_best_reward: --\n  1744/100000: episode: 90, duration: 1.001s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.134 [-2.004, 1.143], mean_best_reward: --\n  1763/100000: episode: 91, duration: 1.786s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.123 [-0.964, 2.012], mean_best_reward: --\n  1777/100000: episode: 92, duration: 1.699s, episode steps: 14, steps per second: 8, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.103 [-0.936, 1.564], mean_best_reward: --\n  1795/100000: episode: 93, duration: 1.590s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.085 [-0.629, 1.043], mean_best_reward: --\n  1807/100000: episode: 94, duration: 1.212s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.116 [-2.759, 1.752], mean_best_reward: --\n  1821/100000: episode: 95, duration: 1.102s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.097 [-1.943, 3.068], mean_best_reward: --\n  1834/100000: episode: 96, duration: 1.091s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.095 [-1.803, 2.804], mean_best_reward: --\n  1846/100000: episode: 97, duration: 1.001s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.086 [-1.226, 1.886], mean_best_reward: --\n  1861/100000: episode: 98, duration: 1.393s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.091 [-1.402, 2.354], mean_best_reward: --\n  2005/100000: episode: 99, duration: 8.707s, episode steps: 144, steps per second: 17, episode reward: 144.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.479 [0.000, 1.000], mean observation: -0.202 [-1.691, 1.135], mean_best_reward: --\n  2024/100000: episode: 100, duration: 1.218s, episode steps: 19, steps per second: 16, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.071 [-2.283, 1.387], mean_best_reward: --\n  2035/100000: episode: 101, duration: 0.898s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.098 [-1.367, 2.202], mean_best_reward: 48.000000\n  2059/100000: episode: 102, duration: 2.096s, episode steps: 24, steps per second: 11, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.041 [-1.686, 0.974], mean_best_reward: --\n  2070/100000: episode: 103, duration: 0.698s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.100 [-1.524, 1.024], mean_best_reward: --\n  2088/100000: episode: 104, duration: 1.699s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.081 [-1.160, 2.084], mean_best_reward: --\n  2098/100000: episode: 105, duration: 1.091s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.156 [-2.489, 1.515], mean_best_reward: --\n  2109/100000: episode: 106, duration: 1.295s, episode steps: 11, steps per second: 8, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.096 [-2.209, 1.394], mean_best_reward: --\n  2120/100000: episode: 107, duration: 1.598s, episode steps: 11, steps per second: 7, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.119 [-1.250, 0.777], mean_best_reward: --\n  2135/100000: episode: 108, duration: 1.211s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.115 [-0.751, 1.321], mean_best_reward: --\n  2145/100000: episode: 109, duration: 0.805s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.540, 1.554], mean_best_reward: --\n  2160/100000: episode: 110, duration: 0.885s, episode steps: 15, steps per second: 17, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.092 [-0.956, 1.703], mean_best_reward: --\n  2172/100000: episode: 111, duration: 1.199s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.126 [-2.086, 1.137], mean_best_reward: --\n  2223/100000: episode: 112, duration: 4.109s, episode steps: 51, steps per second: 12, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: -0.054 [-0.900, 0.579], mean_best_reward: --\n  2245/100000: episode: 113, duration: 1.893s, episode steps: 22, steps per second: 12, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.034 [-1.185, 1.797], mean_best_reward: --\n  2258/100000: episode: 114, duration: 1.008s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.124 [-1.819, 0.951], mean_best_reward: --\n  2268/100000: episode: 115, duration: 1.088s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.161 [-1.536, 2.634], mean_best_reward: --\n  2285/100000: episode: 116, duration: 1.491s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.101 [-1.282, 0.767], mean_best_reward: --\n  2303/100000: episode: 117, duration: 1.811s, episode steps: 18, steps per second: 10, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.062 [-1.502, 0.983], mean_best_reward: --\n  2335/100000: episode: 118, duration: 2.900s, episode steps: 32, steps per second: 11, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.035 [-0.965, 0.606], mean_best_reward: --\n  2372/100000: episode: 119, duration: 2.793s, episode steps: 37, steps per second: 13, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.541 [0.000, 1.000], mean observation: 0.115 [-0.596, 1.015], mean_best_reward: --\n  2382/100000: episode: 120, duration: 0.990s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.119 [-1.217, 2.056], mean_best_reward: --\n  2401/100000: episode: 121, duration: 1.302s, episode steps: 19, steps per second: 15, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.056 [-1.811, 1.029], mean_best_reward: --\n  2410/100000: episode: 122, duration: 0.808s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.148 [-2.339, 1.390], mean_best_reward: --\n  2445/100000: episode: 123, duration: 2.890s, episode steps: 35, steps per second: 12, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.003 [-1.397, 2.238], mean_best_reward: --\n  2458/100000: episode: 124, duration: 1.196s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.098 [-2.728, 1.762], mean_best_reward: --\n  2478/100000: episode: 125, duration: 1.705s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.077 [-1.129, 1.963], mean_best_reward: --\n  2490/100000: episode: 126, duration: 0.892s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.103 [-1.155, 1.877], mean_best_reward: --\n  2501/100000: episode: 127, duration: 0.900s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.136 [-1.846, 1.006], mean_best_reward: --\n  2531/100000: episode: 128, duration: 2.299s, episode steps: 30, steps per second: 13, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.113 [-1.184, 0.424], mean_best_reward: --\n  2550/100000: episode: 129, duration: 1.019s, episode steps: 19, steps per second: 19, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.048 [-1.617, 1.024], mean_best_reward: --\n  2565/100000: episode: 130, duration: 0.590s, episode steps: 15, steps per second: 25, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.070 [-2.372, 1.559], mean_best_reward: --\n  2577/100000: episode: 131, duration: 1.288s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.123 [-1.224, 0.549], mean_best_reward: --\n  2589/100000: episode: 132, duration: 0.919s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.102 [-1.881, 1.151], mean_best_reward: --\n  2600/100000: episode: 133, duration: 1.182s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.127 [-2.092, 1.345], mean_best_reward: --\n  2618/100000: episode: 134, duration: 1.498s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.088 [-1.975, 1.146], mean_best_reward: --\n  2630/100000: episode: 135, duration: 0.902s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.138 [-0.944, 1.680], mean_best_reward: --\n  2686/100000: episode: 136, duration: 5.502s, episode steps: 56, steps per second: 10, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.393 [0.000, 1.000], mean observation: -0.111 [-2.316, 2.399], mean_best_reward: --\n  2703/100000: episode: 137, duration: 1.094s, episode steps: 17, steps per second: 16, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.095 [-0.799, 1.479], mean_best_reward: --\n  2714/100000: episode: 138, duration: 0.516s, episode steps: 11, steps per second: 21, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.121 [-2.430, 1.608], mean_best_reward: --\n  2746/100000: episode: 139, duration: 2.014s, episode steps: 32, steps per second: 16, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.078 [-0.587, 1.257], mean_best_reward: --\n  2762/100000: episode: 140, duration: 1.291s, episode steps: 16, steps per second: 12, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.112 [-2.632, 1.546], mean_best_reward: --\n  2777/100000: episode: 141, duration: 1.197s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.089 [-2.006, 1.332], mean_best_reward: --\n  2793/100000: episode: 142, duration: 1.699s, episode steps: 16, steps per second: 9, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.086 [-1.500, 0.810], mean_best_reward: --\n  2804/100000: episode: 143, duration: 0.698s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.120 [-2.043, 1.326], mean_best_reward: --\n  2812/100000: episode: 144, duration: 0.796s, episode steps: 8, steps per second: 10, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.158 [-2.541, 1.523], mean_best_reward: --\n  2833/100000: episode: 145, duration: 1.814s, episode steps: 21, steps per second: 12, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.078 [-2.342, 1.332], mean_best_reward: --\n  2846/100000: episode: 146, duration: 1.502s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.125 [-1.390, 2.353], mean_best_reward: --\n  2858/100000: episode: 147, duration: 1.496s, episode steps: 12, steps per second: 8, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.128 [-2.539, 1.523], mean_best_reward: --\n  2868/100000: episode: 148, duration: 0.786s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.132 [-0.945, 1.586], mean_best_reward: --\n  2893/100000: episode: 149, duration: 1.510s, episode steps: 25, steps per second: 17, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.640 [0.000, 1.000], mean observation: -0.052 [-2.140, 1.345], mean_best_reward: --\n  2903/100000: episode: 150, duration: 1.100s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.132 [-2.202, 1.382], mean_best_reward: --\n  2918/100000: episode: 151, duration: 0.887s, episode steps: 15, steps per second: 17, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.091 [-1.758, 2.752], mean_best_reward: 103.500000\n  2930/100000: episode: 152, duration: 0.894s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.140 [-1.134, 2.111], mean_best_reward: --\n  2962/100000: episode: 153, duration: 2.699s, episode steps: 32, steps per second: 12, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.406 [0.000, 1.000], mean observation: 0.066 [-1.193, 2.069], mean_best_reward: --\n  2979/100000: episode: 154, duration: 1.119s, episode steps: 17, steps per second: 15, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.041 [-2.150, 1.386], mean_best_reward: --\n  2990/100000: episode: 155, duration: 1.002s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.109 [-2.482, 1.588], mean_best_reward: --\n  3003/100000: episode: 156, duration: 0.984s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.118 [-1.744, 0.961], mean_best_reward: --\n  3011/100000: episode: 157, duration: 0.609s, episode steps: 8, steps per second: 13, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.156 [-2.586, 1.613], mean_best_reward: --\n  3030/100000: episode: 158, duration: 1.598s, episode steps: 19, steps per second: 12, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.099 [-0.750, 1.589], mean_best_reward: --\n  3038/100000: episode: 159, duration: 0.701s, episode steps: 8, steps per second: 11, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.158 [-1.574, 2.555], mean_best_reward: --\n  3049/100000: episode: 160, duration: 1.381s, episode steps: 11, steps per second: 8, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.130 [-2.289, 1.358], mean_best_reward: --\n  3062/100000: episode: 161, duration: 1.102s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.097 [-1.784, 2.756], mean_best_reward: --\n  3084/100000: episode: 162, duration: 1.995s, episode steps: 22, steps per second: 11, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.085 [-2.271, 1.340], mean_best_reward: --\n  3094/100000: episode: 163, duration: 0.915s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.172 [-1.530, 2.605], mean_best_reward: --\n  3136/100000: episode: 164, duration: 3.392s, episode steps: 42, steps per second: 12, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.093 [-0.372, 0.746], mean_best_reward: --\n  3153/100000: episode: 165, duration: 1.296s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.073 [-1.234, 0.796], mean_best_reward: --\n  3180/100000: episode: 166, duration: 2.309s, episode steps: 27, steps per second: 12, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.090 [-0.586, 1.100], mean_best_reward: --\n  3214/100000: episode: 167, duration: 2.503s, episode steps: 34, steps per second: 14, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.117 [-0.343, 0.912], mean_best_reward: --\n  3224/100000: episode: 168, duration: 1.096s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.137 [-1.724, 0.983], mean_best_reward: --\n  3233/100000: episode: 169, duration: 0.791s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.154 [-1.809, 2.878], mean_best_reward: --\n  3252/100000: episode: 170, duration: 1.308s, episode steps: 19, steps per second: 15, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.111 [-0.980, 0.427], mean_best_reward: --\n  3264/100000: episode: 171, duration: 0.799s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.100 [-1.570, 2.435], mean_best_reward: --\n  3289/100000: episode: 172, duration: 1.686s, episode steps: 25, steps per second: 15, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.071 [-0.634, 1.379], mean_best_reward: --\n  3299/100000: episode: 173, duration: 1.003s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.126 [-1.201, 2.075], mean_best_reward: --\n  3313/100000: episode: 174, duration: 1.291s, episode steps: 14, steps per second: 11, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.090 [-2.080, 1.364], mean_best_reward: --\n  3331/100000: episode: 175, duration: 2.019s, episode steps: 18, steps per second: 9, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.093 [-0.811, 1.613], mean_best_reward: --\n  3353/100000: episode: 176, duration: 1.708s, episode steps: 22, steps per second: 13, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.057 [-1.561, 2.586], mean_best_reward: --\n  3394/100000: episode: 177, duration: 3.194s, episode steps: 41, steps per second: 13, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.512 [0.000, 1.000], mean observation: -0.044 [-1.267, 0.629], mean_best_reward: --\n  3411/100000: episode: 178, duration: 1.319s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.088 [-2.302, 1.379], mean_best_reward: --\n  3427/100000: episode: 179, duration: 1.020s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.109 [-2.632, 1.555], mean_best_reward: --\n  3443/100000: episode: 180, duration: 1.584s, episode steps: 16, steps per second: 10, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.088 [-1.314, 0.619], mean_best_reward: --\n  3465/100000: episode: 181, duration: 1.598s, episode steps: 22, steps per second: 14, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.075 [-1.139, 1.943], mean_best_reward: --\n  3479/100000: episode: 182, duration: 1.406s, episode steps: 14, steps per second: 10, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.094 [-1.222, 2.131], mean_best_reward: --\n  3494/100000: episode: 183, duration: 1.100s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.105 [-2.887, 1.784], mean_best_reward: --\n  3512/100000: episode: 184, duration: 1.487s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.065 [-1.877, 1.141], mean_best_reward: --\n  3528/100000: episode: 185, duration: 1.012s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.064 [-2.541, 1.600], mean_best_reward: --\n  3542/100000: episode: 186, duration: 1.186s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.077 [-2.412, 1.611], mean_best_reward: --\n  3553/100000: episode: 187, duration: 1.112s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.102 [-2.257, 1.420], mean_best_reward: --\n  3574/100000: episode: 188, duration: 1.798s, episode steps: 21, steps per second: 12, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.105 [-0.570, 1.126], mean_best_reward: --\n  3595/100000: episode: 189, duration: 1.791s, episode steps: 21, steps per second: 12, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.079 [-1.537, 0.758], mean_best_reward: --\n  3606/100000: episode: 190, duration: 1.009s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.144 [-1.722, 2.867], mean_best_reward: --\n  3622/100000: episode: 191, duration: 1.513s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.081 [-1.219, 1.929], mean_best_reward: --\n  3633/100000: episode: 192, duration: 0.977s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.118 [-1.793, 0.962], mean_best_reward: --\n  3656/100000: episode: 193, duration: 1.800s, episode steps: 23, steps per second: 13, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.058 [-1.711, 2.730], mean_best_reward: --\n  3665/100000: episode: 194, duration: 0.700s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.122 [-1.218, 1.827], mean_best_reward: --\n  3674/100000: episode: 195, duration: 0.703s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.418, 2.314], mean_best_reward: --\n  3686/100000: episode: 196, duration: 0.806s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.112 [-1.211, 1.888], mean_best_reward: --\n  3706/100000: episode: 197, duration: 1.698s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.078 [-1.143, 1.991], mean_best_reward: --\n  3716/100000: episode: 198, duration: 0.899s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.122 [-1.218, 2.053], mean_best_reward: --\n  3728/100000: episode: 199, duration: 1.103s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.088 [-1.976, 1.374], mean_best_reward: --\n  3738/100000: episode: 200, duration: 0.979s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.134 [-2.251, 1.353], mean_best_reward: --\n  3753/100000: episode: 201, duration: 1.118s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.073 [-0.997, 1.691], mean_best_reward: 103.500000\n  3771/100000: episode: 202, duration: 1.592s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.105 [-1.543, 2.682], mean_best_reward: --\n  3783/100000: episode: 203, duration: 0.887s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.118 [-2.710, 1.764], mean_best_reward: --\n  3801/100000: episode: 204, duration: 1.505s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.044 [-1.601, 2.361], mean_best_reward: --\n  3831/100000: episode: 205, duration: 2.594s, episode steps: 30, steps per second: 12, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.065 [-2.277, 1.196], mean_best_reward: --\n  3843/100000: episode: 206, duration: 1.498s, episode steps: 12, steps per second: 8, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.106 [-1.778, 2.711], mean_best_reward: --\n  3858/100000: episode: 207, duration: 1.120s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.072 [-2.793, 1.806], mean_best_reward: --\n  3871/100000: episode: 208, duration: 1.079s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.101 [-1.345, 2.169], mean_best_reward: --\n  3898/100000: episode: 209, duration: 2.220s, episode steps: 27, steps per second: 12, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.135 [-0.641, 1.108], mean_best_reward: --\n  3921/100000: episode: 210, duration: 2.002s, episode steps: 23, steps per second: 11, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.652 [0.000, 1.000], mean observation: -0.068 [-2.476, 1.394], mean_best_reward: --\n  3931/100000: episode: 211, duration: 0.883s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.110 [-1.613, 2.552], mean_best_reward: --\n  3945/100000: episode: 212, duration: 1.202s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.093 [-2.036, 1.195], mean_best_reward: --\n  3956/100000: episode: 213, duration: 0.810s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.101 [-1.418, 2.081], mean_best_reward: --\n  3969/100000: episode: 214, duration: 1.094s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.086 [-1.762, 2.730], mean_best_reward: --\n  3988/100000: episode: 215, duration: 1.402s, episode steps: 19, steps per second: 14, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.071 [-1.027, 1.786], mean_best_reward: --\n  4007/100000: episode: 216, duration: 1.593s, episode steps: 19, steps per second: 12, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.083 [-1.405, 0.612], mean_best_reward: --\n  4017/100000: episode: 217, duration: 0.802s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.164 [-1.949, 3.107], mean_best_reward: --\n  4033/100000: episode: 218, duration: 1.384s, episode steps: 16, steps per second: 12, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.102 [-2.527, 1.523], mean_best_reward: --\n  4042/100000: episode: 219, duration: 0.606s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.153 [-1.331, 2.261], mean_best_reward: --\n  4059/100000: episode: 220, duration: 1.491s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.087 [-1.383, 2.314], mean_best_reward: --\n  4072/100000: episode: 221, duration: 0.999s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.108 [-2.330, 1.407], mean_best_reward: --\n  4083/100000: episode: 222, duration: 0.717s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.118 [-2.296, 1.371], mean_best_reward: --\n  4096/100000: episode: 223, duration: 0.886s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.100 [-2.422, 1.571], mean_best_reward: --\n  4109/100000: episode: 224, duration: 1.302s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.101 [-1.796, 1.036], mean_best_reward: --\n  4120/100000: episode: 225, duration: 1.107s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.150 [-2.309, 1.325], mean_best_reward: --\n  4136/100000: episode: 226, duration: 1.489s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.107 [-0.961, 1.587], mean_best_reward: --\n  4147/100000: episode: 227, duration: 1.109s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.131 [-2.724, 1.731], mean_best_reward: --\n  4162/100000: episode: 228, duration: 1.385s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.078 [-1.174, 1.890], mean_best_reward: --\n  4186/100000: episode: 229, duration: 2.005s, episode steps: 24, steps per second: 12, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.087 [-1.153, 0.792], mean_best_reward: --\n  4223/100000: episode: 230, duration: 2.491s, episode steps: 37, steps per second: 15, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.649 [0.000, 1.000], mean observation: 0.039 [-2.508, 2.067], mean_best_reward: --\n  4237/100000: episode: 231, duration: 1.009s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.106 [-1.559, 0.752], mean_best_reward: --\n  4286/100000: episode: 232, duration: 3.811s, episode steps: 49, steps per second: 13, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.119 [-0.441, 0.868], mean_best_reward: --\n  4299/100000: episode: 233, duration: 0.895s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.107 [-1.796, 0.964], mean_best_reward: --\n  4310/100000: episode: 234, duration: 0.901s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.123 [-1.277, 0.776], mean_best_reward: --\n  4329/100000: episode: 235, duration: 1.592s, episode steps: 19, steps per second: 12, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.100 [-0.968, 1.747], mean_best_reward: --\n  4339/100000: episode: 236, duration: 0.808s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.121 [-0.981, 1.718], mean_best_reward: --\n  4348/100000: episode: 237, duration: 0.795s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.130 [-1.414, 2.303], mean_best_reward: --\n  4362/100000: episode: 238, duration: 1.489s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.106 [-2.531, 1.565], mean_best_reward: --\n  4371/100000: episode: 239, duration: 0.891s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.140 [-1.755, 2.820], mean_best_reward: --\n  4386/100000: episode: 240, duration: 1.303s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.084 [-1.418, 0.763], mean_best_reward: --\n  4397/100000: episode: 241, duration: 0.798s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.100 [-2.437, 1.573], mean_best_reward: --\n  4413/100000: episode: 242, duration: 1.203s, episode steps: 16, steps per second: 13, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.740, 1.286], mean_best_reward: --\n  4427/100000: episode: 243, duration: 1.310s, episode steps: 14, steps per second: 11, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.102 [-2.579, 1.573], mean_best_reward: --\n  4439/100000: episode: 244, duration: 1.199s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.113 [-3.080, 2.004], mean_best_reward: --\n  4451/100000: episode: 245, duration: 0.994s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.093 [-2.024, 1.215], mean_best_reward: --\n  4462/100000: episode: 246, duration: 0.692s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.134 [-2.262, 1.358], mean_best_reward: --\n  4477/100000: episode: 247, duration: 0.895s, episode steps: 15, steps per second: 17, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.066 [-1.026, 1.540], mean_best_reward: --\n  4492/100000: episode: 248, duration: 1.001s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-1.038, 1.857], mean_best_reward: --\n  4513/100000: episode: 249, duration: 1.993s, episode steps: 21, steps per second: 11, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.108 [-1.020, 0.384], mean_best_reward: --\n  4522/100000: episode: 250, duration: 0.619s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.157 [-2.547, 1.573], mean_best_reward: --\n  4531/100000: episode: 251, duration: 1.096s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.137 [-1.901, 1.182], mean_best_reward: 92.500000\n  4603/100000: episode: 252, duration: 4.389s, episode steps: 72, steps per second: 16, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: -0.351 [-1.995, 0.806], mean_best_reward: --\n  4614/100000: episode: 253, duration: 0.610s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.106 [-1.948, 1.215], mean_best_reward: --\n  4624/100000: episode: 254, duration: 0.996s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.114 [-1.573, 2.462], mean_best_reward: --\n  4640/100000: episode: 255, duration: 1.392s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.133, 0.607], mean_best_reward: --\n  4658/100000: episode: 256, duration: 1.607s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.091 [-0.806, 1.635], mean_best_reward: --\n  4673/100000: episode: 257, duration: 1.199s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.100 [-1.414, 2.368], mean_best_reward: --\n  4682/100000: episode: 258, duration: 1.196s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.150 [-2.819, 1.791], mean_best_reward: --\n  4698/100000: episode: 259, duration: 1.401s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.089 [-1.587, 2.578], mean_best_reward: --\n  4715/100000: episode: 260, duration: 1.504s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.065 [-0.823, 1.206], mean_best_reward: --\n  4724/100000: episode: 261, duration: 0.876s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.151 [-2.284, 1.349], mean_best_reward: --\n  4735/100000: episode: 262, duration: 1.000s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.115 [-2.843, 1.780], mean_best_reward: --\n  4835/100000: episode: 263, duration: 7.099s, episode steps: 100, steps per second: 14, episode reward: 100.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.510 [0.000, 1.000], mean observation: 0.080 [-0.934, 1.182], mean_best_reward: --\n  4848/100000: episode: 264, duration: 1.206s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.106 [-1.721, 2.742], mean_best_reward: --\n  4871/100000: episode: 265, duration: 1.902s, episode steps: 23, steps per second: 12, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.073 [-0.752, 1.546], mean_best_reward: --\n  4893/100000: episode: 266, duration: 1.298s, episode steps: 22, steps per second: 17, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.062 [-2.925, 1.914], mean_best_reward: --\n  4905/100000: episode: 267, duration: 0.889s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.101 [-1.371, 0.813], mean_best_reward: --\n  4917/100000: episode: 268, duration: 0.708s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.118 [-2.448, 1.519], mean_best_reward: --\n  4927/100000: episode: 269, duration: 0.904s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.160 [-2.631, 1.572], mean_best_reward: --\n  4936/100000: episode: 270, duration: 1.112s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.126 [-1.835, 1.158], mean_best_reward: --\n  4977/100000: episode: 271, duration: 3.791s, episode steps: 41, steps per second: 11, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.415 [0.000, 1.000], mean observation: 0.016 [-1.372, 2.077], mean_best_reward: --\n  4996/100000: episode: 272, duration: 1.814s, episode steps: 19, steps per second: 10, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.050 [-1.162, 1.833], mean_best_reward: --\n  5011/100000: episode: 273, duration: 1.208s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.082 [-1.318, 2.099], mean_best_reward: --\n  5021/100000: episode: 274, duration: 1.008s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.131 [-1.162, 2.010], mean_best_reward: --\n  5032/100000: episode: 275, duration: 1.107s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.126 [-1.370, 2.286], mean_best_reward: --\n  5042/100000: episode: 276, duration: 0.786s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.921, 3.083], mean_best_reward: --\n  5055/100000: episode: 277, duration: 1.089s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.125 [-1.987, 1.143], mean_best_reward: --\n  5066/100000: episode: 278, duration: 0.616s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.147 [-1.352, 2.388], mean_best_reward: --\n  5088/100000: episode: 279, duration: 1.889s, episode steps: 22, steps per second: 12, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.055 [-0.928, 0.608], mean_best_reward: --\n  5108/100000: episode: 280, duration: 1.799s, episode steps: 20, steps per second: 11, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.760, 1.300], mean_best_reward: --\n  5123/100000: episode: 281, duration: 1.309s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.075 [-0.970, 1.591], mean_best_reward: --\n  5133/100000: episode: 282, duration: 0.790s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.124 [-2.168, 1.365], mean_best_reward: --\n  5146/100000: episode: 283, duration: 1.312s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.109 [-1.744, 2.734], mean_best_reward: --\n  5158/100000: episode: 284, duration: 1.099s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.106 [-2.505, 1.536], mean_best_reward: --\n  5169/100000: episode: 285, duration: 1.179s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.121 [-1.149, 1.946], mean_best_reward: --\n  5180/100000: episode: 286, duration: 1.103s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.129 [-2.266, 1.376], mean_best_reward: --\n  5190/100000: episode: 287, duration: 1.100s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.121 [-1.214, 2.000], mean_best_reward: --\n  5227/100000: episode: 288, duration: 3.510s, episode steps: 37, steps per second: 11, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.622 [0.000, 1.000], mean observation: 0.050 [-2.019, 1.714], mean_best_reward: --\n  5237/100000: episode: 289, duration: 1.016s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.146 [-1.342, 2.231], mean_best_reward: --\n  5248/100000: episode: 290, duration: 1.191s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.106 [-1.190, 1.745], mean_best_reward: --\n  5259/100000: episode: 291, duration: 1.408s, episode steps: 11, steps per second: 8, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.127 [-1.974, 1.176], mean_best_reward: --\n  5271/100000: episode: 292, duration: 1.310s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-0.992, 1.773], mean_best_reward: --\n  5284/100000: episode: 293, duration: 1.289s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.124 [-0.778, 1.490], mean_best_reward: --\n  5297/100000: episode: 294, duration: 0.918s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.080 [-2.263, 1.414], mean_best_reward: --\n  5307/100000: episode: 295, duration: 0.896s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.139 [-1.178, 2.029], mean_best_reward: --\n  5332/100000: episode: 296, duration: 2.492s, episode steps: 25, steps per second: 10, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.073 [-0.609, 1.455], mean_best_reward: --\n  5344/100000: episode: 297, duration: 0.801s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.119 [-0.604, 1.215], mean_best_reward: --\n  5355/100000: episode: 298, duration: 0.993s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.103 [-1.414, 2.308], mean_best_reward: --\n  5364/100000: episode: 299, duration: 0.416s, episode steps: 9, steps per second: 22, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.169 [-1.558, 2.543], mean_best_reward: --\n  5384/100000: episode: 300, duration: 1.776s, episode steps: 20, steps per second: 11, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.066 [-1.632, 0.977], mean_best_reward: --\n  5395/100000: episode: 301, duration: 1.100s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.129 [-1.177, 1.931], mean_best_reward: 37.000000\n  5414/100000: episode: 302, duration: 1.601s, episode steps: 19, steps per second: 12, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.051 [-1.558, 1.024], mean_best_reward: --\n  5437/100000: episode: 303, duration: 2.094s, episode steps: 23, steps per second: 11, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.079 [-0.611, 1.121], mean_best_reward: --\n  5461/100000: episode: 304, duration: 3.006s, episode steps: 24, steps per second: 8, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.041 [-2.553, 1.566], mean_best_reward: --\n  5478/100000: episode: 305, duration: 1.391s, episode steps: 17, steps per second: 12, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.095 [-0.984, 1.792], mean_best_reward: --\n  5490/100000: episode: 306, duration: 1.198s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.104 [-2.539, 1.556], mean_best_reward: --\n  5508/100000: episode: 307, duration: 1.307s, episode steps: 18, steps per second: 14, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.068 [-2.495, 1.544], mean_best_reward: --\n  5517/100000: episode: 308, duration: 0.806s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.150 [-2.263, 1.377], mean_best_reward: --\n  5535/100000: episode: 309, duration: 1.414s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.075 [-0.993, 1.687], mean_best_reward: --\n  5556/100000: episode: 310, duration: 1.611s, episode steps: 21, steps per second: 13, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.102 [-0.405, 0.990], mean_best_reward: --\n  5564/100000: episode: 311, duration: 0.910s, episode steps: 8, steps per second: 9, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.128 [-2.195, 1.373], mean_best_reward: --\n  5579/100000: episode: 312, duration: 1.581s, episode steps: 15, steps per second: 9, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.119 [-1.082, 0.586], mean_best_reward: --\n  5595/100000: episode: 313, duration: 1.805s, episode steps: 16, steps per second: 9, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.085 [-0.841, 1.553], mean_best_reward: --\n  5609/100000: episode: 314, duration: 1.104s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.101 [-2.064, 1.152], mean_best_reward: --\n  5619/100000: episode: 315, duration: 0.807s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.110 [-2.190, 1.410], mean_best_reward: --\n  5633/100000: episode: 316, duration: 0.795s, episode steps: 14, steps per second: 18, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.060 [-1.586, 2.402], mean_best_reward: --\n  5648/100000: episode: 317, duration: 1.101s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.093 [-1.777, 1.000], mean_best_reward: --\n  5661/100000: episode: 318, duration: 0.704s, episode steps: 13, steps per second: 18, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.103 [-1.372, 2.182], mean_best_reward: --\n  5675/100000: episode: 319, duration: 0.804s, episode steps: 14, steps per second: 17, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.112 [-0.819, 1.602], mean_best_reward: --\n  5747/100000: episode: 320, duration: 4.787s, episode steps: 72, steps per second: 15, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-0.818, 1.217], mean_best_reward: --\n  5756/100000: episode: 321, duration: 0.595s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.155 [-1.740, 2.848], mean_best_reward: --\n  5773/100000: episode: 322, duration: 1.495s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.089 [-0.750, 1.379], mean_best_reward: --\n  5782/100000: episode: 323, duration: 0.599s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.143 [-1.951, 1.168], mean_best_reward: --\n  5796/100000: episode: 324, duration: 1.099s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.096 [-2.449, 1.527], mean_best_reward: --\n  5805/100000: episode: 325, duration: 0.606s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.130 [-2.318, 1.410], mean_best_reward: --\n  5821/100000: episode: 326, duration: 1.016s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.084 [-0.806, 1.468], mean_best_reward: --\n  5845/100000: episode: 327, duration: 1.394s, episode steps: 24, steps per second: 17, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.114 [-1.166, 0.386], mean_best_reward: --\n  5856/100000: episode: 328, duration: 0.979s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.126 [-2.108, 1.359], mean_best_reward: --\n  5877/100000: episode: 329, duration: 1.500s, episode steps: 21, steps per second: 14, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.055 [-1.944, 1.186], mean_best_reward: --\n  5897/100000: episode: 330, duration: 1.508s, episode steps: 20, steps per second: 13, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.078 [-1.154, 1.960], mean_best_reward: --\n  5910/100000: episode: 331, duration: 1.095s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.098 [-2.291, 1.415], mean_best_reward: --\n  5934/100000: episode: 332, duration: 1.900s, episode steps: 24, steps per second: 13, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.054 [-0.796, 1.200], mean_best_reward: --\n  5944/100000: episode: 333, duration: 0.602s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.545, 1.524], mean_best_reward: --\n  5961/100000: episode: 334, duration: 0.607s, episode steps: 17, steps per second: 28, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.068 [-1.583, 0.999], mean_best_reward: --\n  5978/100000: episode: 335, duration: 1.179s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.077 [-0.815, 1.442], mean_best_reward: --\n  6009/100000: episode: 336, duration: 1.712s, episode steps: 31, steps per second: 18, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.010 [-1.512, 1.141], mean_best_reward: --\n  6038/100000: episode: 337, duration: 2.491s, episode steps: 29, steps per second: 12, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: 0.038 [-1.376, 2.252], mean_best_reward: --\n  6050/100000: episode: 338, duration: 0.800s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.131 [-1.577, 0.770], mean_best_reward: --\n  6062/100000: episode: 339, duration: 0.691s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.106 [-1.575, 2.554], mean_best_reward: --\n  6074/100000: episode: 340, duration: 0.721s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.105 [-2.152, 1.373], mean_best_reward: --\n  6088/100000: episode: 341, duration: 0.981s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.105 [-2.035, 1.137], mean_best_reward: --\n  6103/100000: episode: 342, duration: 1.009s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.098 [-1.858, 1.005], mean_best_reward: --\n  6122/100000: episode: 343, duration: 1.183s, episode steps: 19, steps per second: 16, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.052 [-1.605, 2.434], mean_best_reward: --\n  6138/100000: episode: 344, duration: 0.915s, episode steps: 16, steps per second: 17, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.095 [-2.657, 1.602], mean_best_reward: --\n  6180/100000: episode: 345, duration: 2.594s, episode steps: 42, steps per second: 16, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.067 [-0.567, 0.936], mean_best_reward: --\n  6193/100000: episode: 346, duration: 1.091s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.114 [-2.370, 1.401], mean_best_reward: --\n  6216/100000: episode: 347, duration: 1.602s, episode steps: 23, steps per second: 14, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.070 [-1.579, 2.541], mean_best_reward: --\n  6253/100000: episode: 348, duration: 2.401s, episode steps: 37, steps per second: 15, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.068 [-1.152, 0.434], mean_best_reward: --\n  6263/100000: episode: 349, duration: 0.898s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-3.082, 1.933], mean_best_reward: --\n  6275/100000: episode: 350, duration: 0.614s, episode steps: 12, steps per second: 20, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.112 [-2.538, 1.545], mean_best_reward: --\n  6293/100000: episode: 351, duration: 1.284s, episode steps: 18, steps per second: 14, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.071 [-1.296, 0.786], mean_best_reward: 41.000000\n  6312/100000: episode: 352, duration: 1.298s, episode steps: 19, steps per second: 15, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: 0.117 [-0.612, 0.999], mean_best_reward: --\n  6337/100000: episode: 353, duration: 1.801s, episode steps: 25, steps per second: 14, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.004 [-1.600, 1.193], mean_best_reward: --\n  6352/100000: episode: 354, duration: 1.093s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.104 [-1.305, 0.748], mean_best_reward: --\n  6377/100000: episode: 355, duration: 1.519s, episode steps: 25, steps per second: 16, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.360 [0.000, 1.000], mean observation: 0.054 [-1.412, 2.327], mean_best_reward: --\n  6396/100000: episode: 356, duration: 1.381s, episode steps: 19, steps per second: 14, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.263 [0.000, 1.000], mean observation: 0.035 [-1.801, 2.626], mean_best_reward: --\n  6410/100000: episode: 357, duration: 0.997s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.081 [-2.414, 1.534], mean_best_reward: --\n  6444/100000: episode: 358, duration: 2.098s, episode steps: 34, steps per second: 16, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: 0.051 [-0.934, 1.656], mean_best_reward: --\n  6460/100000: episode: 359, duration: 1.097s, episode steps: 16, steps per second: 15, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.087 [-1.544, 0.983], mean_best_reward: --\n  6496/100000: episode: 360, duration: 2.803s, episode steps: 36, steps per second: 13, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.047 [-0.795, 1.590], mean_best_reward: --\n  6518/100000: episode: 361, duration: 2.095s, episode steps: 22, steps per second: 10, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.146 [-0.538, 0.934], mean_best_reward: --\n  6535/100000: episode: 362, duration: 0.818s, episode steps: 17, steps per second: 21, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.105 [-1.811, 0.942], mean_best_reward: --\n  6548/100000: episode: 363, duration: 0.900s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.102 [-1.976, 1.180], mean_best_reward: --\n  6564/100000: episode: 364, duration: 1.686s, episode steps: 16, steps per second: 9, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.093 [-1.249, 0.802], mean_best_reward: --\n  6586/100000: episode: 365, duration: 1.592s, episode steps: 22, steps per second: 14, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.087 [-0.412, 0.960], mean_best_reward: --\n  6599/100000: episode: 366, duration: 0.911s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.103 [-1.373, 2.217], mean_best_reward: --\n  6610/100000: episode: 367, duration: 0.806s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.117 [-1.827, 1.128], mean_best_reward: --\n  6625/100000: episode: 368, duration: 1.301s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.091 [-1.578, 2.475], mean_best_reward: --\n  6634/100000: episode: 369, duration: 0.579s, episode steps: 9, steps per second: 16, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.164 [-2.519, 1.542], mean_best_reward: --\n  6645/100000: episode: 370, duration: 0.613s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.118 [-1.168, 1.787], mean_best_reward: --\n  6653/100000: episode: 371, duration: 0.896s, episode steps: 8, steps per second: 9, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.153 [-2.565, 1.560], mean_best_reward: --\n  6663/100000: episode: 372, duration: 0.801s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.133 [-2.065, 1.151], mean_best_reward: --\n  6678/100000: episode: 373, duration: 1.286s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.090 [-2.911, 1.803], mean_best_reward: --\n  6689/100000: episode: 374, duration: 1.195s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.124 [-1.375, 2.297], mean_best_reward: --\n  6704/100000: episode: 375, duration: 1.107s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.083 [-1.950, 1.204], mean_best_reward: --\n  6750/100000: episode: 376, duration: 2.911s, episode steps: 46, steps per second: 16, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: 0.073 [-2.014, 1.872], mean_best_reward: --\n  6814/100000: episode: 377, duration: 5.276s, episode steps: 64, steps per second: 12, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.547 [0.000, 1.000], mean observation: -0.006 [-1.885, 1.318], mean_best_reward: --\n  6826/100000: episode: 378, duration: 0.703s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.132 [-1.925, 3.115], mean_best_reward: --\n  6859/100000: episode: 379, duration: 1.996s, episode steps: 33, steps per second: 17, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.072 [-0.790, 1.561], mean_best_reward: --\n  6870/100000: episode: 380, duration: 0.916s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.154 [-1.714, 2.850], mean_best_reward: --\n  6885/100000: episode: 381, duration: 1.093s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.078 [-1.138, 1.853], mean_best_reward: --\n  6911/100000: episode: 382, duration: 2.001s, episode steps: 26, steps per second: 13, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.082 [-0.811, 1.284], mean_best_reward: --\n  6951/100000: episode: 383, duration: 2.783s, episode steps: 40, steps per second: 14, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.105 [-0.774, 0.347], mean_best_reward: --\n  6981/100000: episode: 384, duration: 2.312s, episode steps: 30, steps per second: 13, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.123 [-0.397, 0.963], mean_best_reward: --\n  6995/100000: episode: 385, duration: 0.993s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.084 [-1.383, 2.203], mean_best_reward: --\n  7019/100000: episode: 386, duration: 1.604s, episode steps: 24, steps per second: 15, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.072 [-0.406, 0.897], mean_best_reward: --\n  7035/100000: episode: 387, duration: 1.795s, episode steps: 16, steps per second: 9, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.312 [0.000, 1.000], mean observation: 0.066 [-1.167, 1.892], mean_best_reward: --\n  7046/100000: episode: 388, duration: 1.085s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.104 [-1.030, 1.721], mean_best_reward: --\n  7055/100000: episode: 389, duration: 0.500s, episode steps: 9, steps per second: 18, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.141 [-1.389, 2.252], mean_best_reward: --\n  7090/100000: episode: 390, duration: 1.624s, episode steps: 35, steps per second: 22, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.044 [-0.971, 1.679], mean_best_reward: --\n  7099/100000: episode: 391, duration: 0.483s, episode steps: 9, steps per second: 19, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.143 [-1.786, 2.829], mean_best_reward: --\n  7109/100000: episode: 392, duration: 0.791s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.131 [-1.571, 2.537], mean_best_reward: --\n  7133/100000: episode: 393, duration: 1.713s, episode steps: 24, steps per second: 14, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.058 [-2.192, 1.226], mean_best_reward: --\n  7149/100000: episode: 394, duration: 1.305s, episode steps: 16, steps per second: 12, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: 0.088 [-0.583, 0.917], mean_best_reward: --\n  7168/100000: episode: 395, duration: 1.796s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.080 [-2.210, 1.322], mean_best_reward: --\n  7185/100000: episode: 396, duration: 1.488s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.085 [-2.235, 1.347], mean_best_reward: --\n  7198/100000: episode: 397, duration: 1.092s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.102 [-1.263, 0.794], mean_best_reward: --\n  7210/100000: episode: 398, duration: 1.199s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.115 [-1.988, 3.062], mean_best_reward: --\n  7225/100000: episode: 399, duration: 1.219s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.100 [-1.045, 0.631], mean_best_reward: --\n  7234/100000: episode: 400, duration: 0.983s, episode steps: 9, steps per second: 9, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.133 [-1.756, 1.020], mean_best_reward: --\n  7254/100000: episode: 401, duration: 1.688s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.123 [-1.233, 0.543], mean_best_reward: 49.000000\n  7269/100000: episode: 402, duration: 1.406s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.097 [-1.251, 0.597], mean_best_reward: --\n  7282/100000: episode: 403, duration: 1.307s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.110 [-1.589, 2.473], mean_best_reward: --\n  7292/100000: episode: 404, duration: 0.693s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.132 [-1.147, 2.017], mean_best_reward: --\n  7303/100000: episode: 405, duration: 0.912s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.150 [-2.832, 1.736], mean_best_reward: --\n  7313/100000: episode: 406, duration: 0.882s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.140 [-2.601, 1.601], mean_best_reward: --\n  7323/100000: episode: 407, duration: 0.700s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-1.202, 1.886], mean_best_reward: --\n  7336/100000: episode: 408, duration: 1.212s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.085 [-1.317, 2.010], mean_best_reward: --\n  7347/100000: episode: 409, duration: 1.100s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.114 [-1.722, 2.776], mean_best_reward: --\n  7358/100000: episode: 410, duration: 1.002s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.121 [-1.787, 2.848], mean_best_reward: --\n  7388/100000: episode: 411, duration: 2.497s, episode steps: 30, steps per second: 12, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.028 [-0.967, 1.285], mean_best_reward: --\n  7412/100000: episode: 412, duration: 1.713s, episode steps: 24, steps per second: 14, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.062 [-1.127, 0.773], mean_best_reward: --\n  7422/100000: episode: 413, duration: 1.088s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.157 [-1.540, 2.590], mean_best_reward: --\n  7430/100000: episode: 414, duration: 1.023s, episode steps: 8, steps per second: 8, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.529, 2.526], mean_best_reward: --\n  7451/100000: episode: 415, duration: 1.978s, episode steps: 21, steps per second: 11, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.050 [-1.456, 0.816], mean_best_reward: --\n  7459/100000: episode: 416, duration: 0.813s, episode steps: 8, steps per second: 10, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.125 [0.000, 1.000], mean observation: 0.162 [-1.338, 2.238], mean_best_reward: --\n  7472/100000: episode: 417, duration: 1.101s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.090 [-1.731, 0.980], mean_best_reward: --\n  7482/100000: episode: 418, duration: 0.794s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.144 [-2.211, 1.346], mean_best_reward: --\n  7499/100000: episode: 419, duration: 1.300s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.096 [-2.853, 1.763], mean_best_reward: --\n  7509/100000: episode: 420, duration: 0.507s, episode steps: 10, steps per second: 20, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.120 [-1.184, 1.986], mean_best_reward: --\n  7526/100000: episode: 421, duration: 1.279s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.045 [-2.589, 1.735], mean_best_reward: --\n  7537/100000: episode: 422, duration: 0.903s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.106 [-1.904, 1.141], mean_best_reward: --\n  7547/100000: episode: 423, duration: 0.721s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.126 [-2.046, 1.218], mean_best_reward: --\n  7559/100000: episode: 424, duration: 0.690s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.108 [-1.589, 2.539], mean_best_reward: --\n  7568/100000: episode: 425, duration: 0.700s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.138 [-1.959, 1.216], mean_best_reward: --\n  7596/100000: episode: 426, duration: 1.797s, episode steps: 28, steps per second: 16, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.085 [-1.105, 0.461], mean_best_reward: --\n  7610/100000: episode: 427, duration: 0.801s, episode steps: 14, steps per second: 17, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.095 [-1.555, 0.792], mean_best_reward: --\n  7620/100000: episode: 428, duration: 0.888s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.148 [-2.077, 1.180], mean_best_reward: --\n  7639/100000: episode: 429, duration: 1.713s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.058 [-2.195, 1.349], mean_best_reward: --\n  7664/100000: episode: 430, duration: 2.397s, episode steps: 25, steps per second: 10, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.051 [-1.915, 1.020], mean_best_reward: --\n  7687/100000: episode: 431, duration: 1.799s, episode steps: 23, steps per second: 13, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.059 [-1.392, 2.346], mean_best_reward: --\n  7706/100000: episode: 432, duration: 2.297s, episode steps: 19, steps per second: 8, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.082 [-2.176, 1.329], mean_best_reward: --\n  7735/100000: episode: 433, duration: 2.597s, episode steps: 29, steps per second: 11, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.089 [-0.610, 1.220], mean_best_reward: --\n  7751/100000: episode: 434, duration: 0.993s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.080 [-1.880, 1.139], mean_best_reward: --\n  7777/100000: episode: 435, duration: 1.893s, episode steps: 26, steps per second: 14, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.009 [-1.963, 2.828], mean_best_reward: --\n  7803/100000: episode: 436, duration: 1.597s, episode steps: 26, steps per second: 16, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.060 [-0.644, 1.350], mean_best_reward: --\n  7814/100000: episode: 437, duration: 1.302s, episode steps: 11, steps per second: 8, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.154 [-1.715, 2.893], mean_best_reward: --\n  7858/100000: episode: 438, duration: 3.610s, episode steps: 44, steps per second: 12, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.001 [-1.154, 0.758], mean_best_reward: --\n  7876/100000: episode: 439, duration: 1.485s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.077 [-1.264, 0.632], mean_best_reward: --\n  7892/100000: episode: 440, duration: 1.307s, episode steps: 16, steps per second: 12, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.090 [-3.053, 1.909], mean_best_reward: --\n  7908/100000: episode: 441, duration: 1.202s, episode steps: 16, steps per second: 13, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.074 [-1.556, 2.475], mean_best_reward: --\n  7929/100000: episode: 442, duration: 1.306s, episode steps: 21, steps per second: 16, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.099 [-0.763, 1.382], mean_best_reward: --\n  7953/100000: episode: 443, duration: 1.784s, episode steps: 24, steps per second: 13, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.061 [-1.406, 0.655], mean_best_reward: --\n  7964/100000: episode: 444, duration: 0.810s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.121 [-1.145, 1.767], mean_best_reward: --\n  7977/100000: episode: 445, duration: 1.190s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.097 [-1.559, 2.434], mean_best_reward: --\n  8004/100000: episode: 446, duration: 2.405s, episode steps: 27, steps per second: 11, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.630 [0.000, 1.000], mean observation: 0.007 [-2.078, 1.413], mean_best_reward: --\n  8035/100000: episode: 447, duration: 2.594s, episode steps: 31, steps per second: 12, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.581 [0.000, 1.000], mean observation: -0.105 [-2.208, 0.956], mean_best_reward: --\n  8044/100000: episode: 448, duration: 0.693s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.146 [-2.255, 1.334], mean_best_reward: --\n  8055/100000: episode: 449, duration: 0.409s, episode steps: 11, steps per second: 27, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.123 [-2.336, 1.385], mean_best_reward: --\n  8065/100000: episode: 450, duration: 0.982s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.126 [-2.514, 1.540], mean_best_reward: --\n  8081/100000: episode: 451, duration: 1.620s, episode steps: 16, steps per second: 10, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.098 [-1.950, 1.137], mean_best_reward: 58.000000\n  8103/100000: episode: 452, duration: 1.977s, episode steps: 22, steps per second: 11, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.409 [0.000, 1.000], mean observation: 0.027 [-0.827, 1.446], mean_best_reward: --\n  8113/100000: episode: 453, duration: 1.011s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.150 [-1.554, 2.565], mean_best_reward: --\n  8122/100000: episode: 454, duration: 0.882s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.140 [-1.559, 2.468], mean_best_reward: --\n  8156/100000: episode: 455, duration: 2.810s, episode steps: 34, steps per second: 12, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.013 [-1.133, 1.552], mean_best_reward: --\n  8176/100000: episode: 456, duration: 1.508s, episode steps: 20, steps per second: 13, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.539, 0.913], mean_best_reward: --\n  8189/100000: episode: 457, duration: 1.521s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.118 [-0.744, 1.334], mean_best_reward: --\n  8263/100000: episode: 458, duration: 6.095s, episode steps: 74, steps per second: 12, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.405 [0.000, 1.000], mean observation: -0.071 [-2.657, 2.810], mean_best_reward: --\n  8289/100000: episode: 459, duration: 2.490s, episode steps: 26, steps per second: 10, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.093 [-0.373, 1.011], mean_best_reward: --\n  8313/100000: episode: 460, duration: 2.009s, episode steps: 24, steps per second: 12, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.051 [-3.367, 2.316], mean_best_reward: --\n  8324/100000: episode: 461, duration: 1.001s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.109 [-2.438, 1.578], mean_best_reward: --\n  8340/100000: episode: 462, duration: 1.400s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.076 [-1.523, 0.974], mean_best_reward: --\n  8378/100000: episode: 463, duration: 2.886s, episode steps: 38, steps per second: 13, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.142 [-0.864, 0.629], mean_best_reward: --\n  8391/100000: episode: 464, duration: 0.892s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.091 [-2.229, 1.394], mean_best_reward: --\n  8433/100000: episode: 465, duration: 2.703s, episode steps: 42, steps per second: 16, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.135 [-0.396, 1.403], mean_best_reward: --\n  8445/100000: episode: 466, duration: 0.895s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-0.965, 1.728], mean_best_reward: --\n  8458/100000: episode: 467, duration: 1.112s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.097 [-1.377, 2.326], mean_best_reward: --\n  8483/100000: episode: 468, duration: 2.600s, episode steps: 25, steps per second: 10, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: -0.112 [-1.618, 0.607], mean_best_reward: --\n  8494/100000: episode: 469, duration: 1.201s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.134 [-1.799, 2.845], mean_best_reward: --\n  8507/100000: episode: 470, duration: 1.283s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.097 [-1.197, 2.008], mean_best_reward: --\n  8516/100000: episode: 471, duration: 1.008s, episode steps: 9, steps per second: 9, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.151 [-1.724, 2.821], mean_best_reward: --\n  8527/100000: episode: 472, duration: 0.783s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.142 [-1.770, 2.860], mean_best_reward: --\n  8543/100000: episode: 473, duration: 0.912s, episode steps: 16, steps per second: 18, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.760, 1.304], mean_best_reward: --\n  8556/100000: episode: 474, duration: 1.187s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.087 [-1.393, 2.227], mean_best_reward: --\n  8568/100000: episode: 475, duration: 1.015s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.090 [-1.950, 2.985], mean_best_reward: --\n  8600/100000: episode: 476, duration: 2.105s, episode steps: 32, steps per second: 15, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.074 [-0.785, 1.086], mean_best_reward: --\n  8674/100000: episode: 477, duration: 5.386s, episode steps: 74, steps per second: 14, episode reward: 74.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.486 [0.000, 1.000], mean observation: -0.117 [-0.996, 1.169], mean_best_reward: --\n  8686/100000: episode: 478, duration: 0.803s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.123 [-1.534, 2.465], mean_best_reward: --\n  8698/100000: episode: 479, duration: 1.291s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.115 [-1.552, 2.529], mean_best_reward: --\n  8740/100000: episode: 480, duration: 3.996s, episode steps: 42, steps per second: 11, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: 0.112 [-0.299, 0.865], mean_best_reward: --\n  8774/100000: episode: 481, duration: 3.399s, episode steps: 34, steps per second: 10, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: -0.026 [-1.624, 0.832], mean_best_reward: --\n  8787/100000: episode: 482, duration: 1.013s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.104 [-1.806, 1.149], mean_best_reward: --\n  8800/100000: episode: 483, duration: 1.190s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.089 [-1.423, 2.292], mean_best_reward: --\n  8810/100000: episode: 484, duration: 0.709s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.133 [-2.182, 1.332], mean_best_reward: --\n  8820/100000: episode: 485, duration: 0.804s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.120 [-1.599, 2.538], mean_best_reward: --\n  8833/100000: episode: 486, duration: 1.303s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.133 [-1.350, 2.387], mean_best_reward: --\n  8854/100000: episode: 487, duration: 1.701s, episode steps: 21, steps per second: 12, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.238 [0.000, 1.000], mean observation: 0.037 [-2.105, 3.021], mean_best_reward: --\n  8865/100000: episode: 488, duration: 1.194s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.321, 1.424], mean_best_reward: --\n  8888/100000: episode: 489, duration: 1.989s, episode steps: 23, steps per second: 12, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.052 [-1.422, 2.373], mean_best_reward: --\n  8908/100000: episode: 490, duration: 1.797s, episode steps: 20, steps per second: 11, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: 0.023 [-1.198, 1.598], mean_best_reward: --\n  8919/100000: episode: 491, duration: 1.303s, episode steps: 11, steps per second: 8, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.117 [-0.947, 1.624], mean_best_reward: --\n  8930/100000: episode: 492, duration: 0.811s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.091 [-1.196, 1.900], mean_best_reward: --\n  8958/100000: episode: 493, duration: 2.385s, episode steps: 28, steps per second: 12, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.105 [-0.581, 0.943], mean_best_reward: --\n  8974/100000: episode: 494, duration: 1.107s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.117 [-0.543, 1.160], mean_best_reward: --\n  8988/100000: episode: 495, duration: 1.293s, episode steps: 14, steps per second: 11, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.082 [-1.390, 2.165], mean_best_reward: --\n  9006/100000: episode: 496, duration: 1.388s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.053 [-1.407, 2.193], mean_best_reward: --\n  9040/100000: episode: 497, duration: 2.308s, episode steps: 34, steps per second: 15, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.024 [-1.354, 1.935], mean_best_reward: --\n  9069/100000: episode: 498, duration: 1.910s, episode steps: 29, steps per second: 15, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: 0.072 [-1.345, 2.416], mean_best_reward: --\n  9081/100000: episode: 499, duration: 1.092s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.111 [-0.967, 1.504], mean_best_reward: --\n  9096/100000: episode: 500, duration: 0.902s, episode steps: 15, steps per second: 17, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.110 [-0.925, 1.451], mean_best_reward: --\n  9107/100000: episode: 501, duration: 1.086s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.119 [-1.009, 1.752], mean_best_reward: 133.000000\n  9119/100000: episode: 502, duration: 1.096s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.135 [-1.556, 2.563], mean_best_reward: --\n  9127/100000: episode: 503, duration: 0.516s, episode steps: 8, steps per second: 15, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.160 [-2.232, 1.365], mean_best_reward: --\n  9150/100000: episode: 504, duration: 2.193s, episode steps: 23, steps per second: 10, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.304 [0.000, 1.000], mean observation: 0.016 [-1.746, 2.486], mean_best_reward: --\n  9180/100000: episode: 505, duration: 2.595s, episode steps: 30, steps per second: 12, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.065 [-0.978, 1.853], mean_best_reward: --\n  9191/100000: episode: 506, duration: 0.615s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.110 [-1.761, 2.682], mean_best_reward: --\n  9203/100000: episode: 507, duration: 0.779s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.097 [-1.191, 2.017], mean_best_reward: --\n  9234/100000: episode: 508, duration: 2.613s, episode steps: 31, steps per second: 12, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.387 [0.000, 1.000], mean observation: 0.045 [-1.419, 2.421], mean_best_reward: --\n  9245/100000: episode: 509, duration: 0.794s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.145 [-1.768, 2.857], mean_best_reward: --\n  9271/100000: episode: 510, duration: 2.706s, episode steps: 26, steps per second: 10, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.062 [-1.153, 1.704], mean_best_reward: --\n  9288/100000: episode: 511, duration: 1.880s, episode steps: 17, steps per second: 9, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.061 [-0.843, 1.492], mean_best_reward: --\n  9299/100000: episode: 512, duration: 1.114s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.110 [-1.167, 1.734], mean_best_reward: --\n  9309/100000: episode: 513, duration: 1.183s, episode steps: 10, steps per second: 8, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.136 [-2.040, 1.183], mean_best_reward: --\n  9317/100000: episode: 514, duration: 0.909s, episode steps: 8, steps per second: 9, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.152 [-1.566, 2.525], mean_best_reward: --\n  9344/100000: episode: 515, duration: 2.008s, episode steps: 27, steps per second: 13, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.115 [-0.356, 0.784], mean_best_reward: --\n  9353/100000: episode: 516, duration: 0.784s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.136 [-2.241, 1.398], mean_best_reward: --\n  9376/100000: episode: 517, duration: 1.502s, episode steps: 23, steps per second: 15, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.043 [-1.032, 1.800], mean_best_reward: --\n  9387/100000: episode: 518, duration: 0.707s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.134 [-1.360, 2.275], mean_best_reward: --\n  9400/100000: episode: 519, duration: 1.098s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.105 [-2.871, 1.757], mean_best_reward: --\n  9415/100000: episode: 520, duration: 1.289s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.123 [-2.402, 1.349], mean_best_reward: --\n  9427/100000: episode: 521, duration: 0.991s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.099 [-1.389, 2.206], mean_best_reward: --\n  9442/100000: episode: 522, duration: 1.203s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.089 [-1.604, 2.526], mean_best_reward: --\n  9478/100000: episode: 523, duration: 2.498s, episode steps: 36, steps per second: 14, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.034 [-1.293, 0.764], mean_best_reward: --\n  9489/100000: episode: 524, duration: 1.095s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-1.362, 2.120], mean_best_reward: --\n  9510/100000: episode: 525, duration: 1.803s, episode steps: 21, steps per second: 12, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.045 [-1.725, 2.650], mean_best_reward: --\n  9519/100000: episode: 526, duration: 1.105s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.126 [-1.153, 1.929], mean_best_reward: --\n  9532/100000: episode: 527, duration: 1.508s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.091 [-1.221, 1.873], mean_best_reward: --\n  9546/100000: episode: 528, duration: 0.990s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.105 [-1.695, 0.935], mean_best_reward: --\n  9557/100000: episode: 529, duration: 0.796s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.139 [-1.332, 2.341], mean_best_reward: --\n  9567/100000: episode: 530, duration: 0.985s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.149 [-1.331, 2.094], mean_best_reward: --\n  9580/100000: episode: 531, duration: 1.119s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.119 [-0.992, 1.822], mean_best_reward: --\n  9649/100000: episode: 532, duration: 4.499s, episode steps: 69, steps per second: 15, episode reward: 69.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.061 [-0.923, 0.740], mean_best_reward: --\n  9659/100000: episode: 533, duration: 0.890s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.129 [-2.521, 1.601], mean_best_reward: --\n  9673/100000: episode: 534, duration: 1.201s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.086 [-1.417, 2.249], mean_best_reward: --\n  9688/100000: episode: 535, duration: 1.292s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.080 [-0.998, 1.663], mean_best_reward: --\n  9704/100000: episode: 536, duration: 1.091s, episode steps: 16, steps per second: 15, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.061 [-1.335, 2.003], mean_best_reward: --\n  9756/100000: episode: 537, duration: 3.798s, episode steps: 52, steps per second: 14, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.235 [-1.310, 0.721], mean_best_reward: --\n  9770/100000: episode: 538, duration: 1.510s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.093 [-1.132, 1.861], mean_best_reward: --\n  9786/100000: episode: 539, duration: 1.101s, episode steps: 16, steps per second: 15, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.105 [-0.590, 1.291], mean_best_reward: --\n  9797/100000: episode: 540, duration: 0.599s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.119 [-1.538, 2.422], mean_best_reward: --\n  9813/100000: episode: 541, duration: 1.608s, episode steps: 16, steps per second: 10, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.117 [-2.112, 1.153], mean_best_reward: --\n  9826/100000: episode: 542, duration: 1.294s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.110 [-1.793, 2.877], mean_best_reward: --\n  9840/100000: episode: 543, duration: 1.494s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.094 [-1.154, 1.780], mean_best_reward: --\n  9853/100000: episode: 544, duration: 1.095s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.096 [-1.420, 2.323], mean_best_reward: --\n  9882/100000: episode: 545, duration: 2.212s, episode steps: 29, steps per second: 13, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: 0.047 [-1.316, 2.074], mean_best_reward: --\n  9901/100000: episode: 546, duration: 1.783s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.368 [0.000, 1.000], mean observation: 0.077 [-1.174, 1.953], mean_best_reward: --\n  9915/100000: episode: 547, duration: 1.202s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.095 [-1.336, 2.179], mean_best_reward: --\n  9928/100000: episode: 548, duration: 1.302s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.098 [-1.728, 2.727], mean_best_reward: --\n  9941/100000: episode: 549, duration: 1.508s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.092 [-1.421, 2.284], mean_best_reward: --\n  9969/100000: episode: 550, duration: 2.418s, episode steps: 28, steps per second: 12, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.043 [-1.127, 0.740], mean_best_reward: --\n  9979/100000: episode: 551, duration: 0.998s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.126 [-1.580, 2.457], mean_best_reward: 78.000000\n 10002/100000: episode: 552, duration: 1.798s, episode steps: 23, steps per second: 13, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.522 [0.000, 1.000], mean observation: 0.076 [-0.775, 1.155], mean_best_reward: --\n 10014/100000: episode: 553, duration: 1.098s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.105 [-0.951, 1.542], mean_best_reward: --\n 10028/100000: episode: 554, duration: 1.089s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.096 [-2.072, 1.215], mean_best_reward: --\n 10036/100000: episode: 555, duration: 1.004s, episode steps: 8, steps per second: 8, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.131 [-1.577, 2.521], mean_best_reward: --\n 10045/100000: episode: 556, duration: 0.901s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.168 [-1.713, 2.817], mean_best_reward: --\n 10095/100000: episode: 557, duration: 4.906s, episode steps: 50, steps per second: 10, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.054 [-2.313, 1.903], mean_best_reward: --\n 10128/100000: episode: 558, duration: 2.888s, episode steps: 33, steps per second: 11, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.515 [0.000, 1.000], mean observation: 0.096 [-0.767, 1.148], mean_best_reward: --\n 10150/100000: episode: 559, duration: 1.512s, episode steps: 22, steps per second: 15, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.111 [-0.440, 0.726], mean_best_reward: --\n 10162/100000: episode: 560, duration: 0.790s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.130 [-0.952, 1.560], mean_best_reward: --\n 10174/100000: episode: 561, duration: 0.707s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.106 [-1.603, 2.552], mean_best_reward: --\n 10199/100000: episode: 562, duration: 1.787s, episode steps: 25, steps per second: 14, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.073 [-0.963, 1.892], mean_best_reward: --\n 10215/100000: episode: 563, duration: 1.798s, episode steps: 16, steps per second: 9, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.096 [-2.689, 1.728], mean_best_reward: --\n 10246/100000: episode: 564, duration: 2.604s, episode steps: 31, steps per second: 12, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.108 [-0.406, 1.237], mean_best_reward: --\n 10257/100000: episode: 565, duration: 1.096s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.116 [-1.138, 1.816], mean_best_reward: --\n 10272/100000: episode: 566, duration: 1.093s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.093 [-1.432, 0.802], mean_best_reward: --\n 10324/100000: episode: 567, duration: 4.392s, episode steps: 52, steps per second: 12, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: 0.062 [-0.859, 1.489], mean_best_reward: --\n 10335/100000: episode: 568, duration: 0.720s, episode steps: 11, steps per second: 15, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.119 [-1.702, 1.015], mean_best_reward: --\n 10384/100000: episode: 569, duration: 4.283s, episode steps: 49, steps per second: 11, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.006 [-0.647, 1.349], mean_best_reward: --\n 10394/100000: episode: 570, duration: 0.915s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.118 [-1.988, 3.010], mean_best_reward: --\n 10425/100000: episode: 571, duration: 1.984s, episode steps: 31, steps per second: 16, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: -0.092 [-1.175, 0.429], mean_best_reward: --\n 10451/100000: episode: 572, duration: 2.006s, episode steps: 26, steps per second: 13, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.108 [-0.796, 1.215], mean_best_reward: --\n 10459/100000: episode: 573, duration: 0.805s, episode steps: 8, steps per second: 10, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.161 [-1.553, 2.564], mean_best_reward: --\n 10486/100000: episode: 574, duration: 2.501s, episode steps: 27, steps per second: 11, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.481 [0.000, 1.000], mean observation: -0.100 [-0.908, 0.359], mean_best_reward: --\n 10499/100000: episode: 575, duration: 1.184s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.099 [-2.029, 1.376], mean_best_reward: --\n 10513/100000: episode: 576, duration: 0.912s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.097 [-1.202, 2.045], mean_best_reward: --\n 10537/100000: episode: 577, duration: 2.394s, episode steps: 24, steps per second: 10, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.083 [-1.534, 2.621], mean_best_reward: --\n 10548/100000: episode: 578, duration: 0.799s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.142 [-0.950, 1.850], mean_best_reward: --\n 10561/100000: episode: 579, duration: 1.202s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.100 [-1.816, 1.000], mean_best_reward: --\n 10576/100000: episode: 580, duration: 1.290s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.083 [-1.363, 2.174], mean_best_reward: --\n 10591/100000: episode: 581, duration: 1.798s, episode steps: 15, steps per second: 8, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.090 [-1.918, 1.186], mean_best_reward: --\n 10604/100000: episode: 582, duration: 1.395s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.099 [-1.169, 1.823], mean_best_reward: --\n 10641/100000: episode: 583, duration: 2.810s, episode steps: 37, steps per second: 13, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.092 [-0.443, 1.053], mean_best_reward: --\n 10688/100000: episode: 584, duration: 3.595s, episode steps: 47, steps per second: 13, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.532 [0.000, 1.000], mean observation: -0.097 [-1.619, 0.639], mean_best_reward: --\n 10699/100000: episode: 585, duration: 0.707s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.123 [-1.720, 2.738], mean_best_reward: --\n 10723/100000: episode: 586, duration: 2.000s, episode steps: 24, steps per second: 12, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-0.944, 0.601], mean_best_reward: --\n 10734/100000: episode: 587, duration: 1.086s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.115 [-2.466, 1.566], mean_best_reward: --\n 10754/100000: episode: 588, duration: 1.799s, episode steps: 20, steps per second: 11, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.066 [-2.597, 1.722], mean_best_reward: --\n 10769/100000: episode: 589, duration: 1.394s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.096 [-0.582, 1.048], mean_best_reward: --\n 10784/100000: episode: 590, duration: 1.304s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.090 [-2.158, 1.332], mean_best_reward: --\n 10796/100000: episode: 591, duration: 0.802s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.100 [-1.206, 2.058], mean_best_reward: --\n 10824/100000: episode: 592, duration: 2.783s, episode steps: 28, steps per second: 10, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.099 [-1.439, 0.593], mean_best_reward: --\n 10834/100000: episode: 593, duration: 0.608s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.125 [-1.214, 1.825], mean_best_reward: --\n 10857/100000: episode: 594, duration: 2.000s, episode steps: 23, steps per second: 11, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.116 [-0.410, 1.263], mean_best_reward: --\n 10889/100000: episode: 595, duration: 2.191s, episode steps: 32, steps per second: 15, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: -0.040 [-1.470, 0.657], mean_best_reward: --\n 10898/100000: episode: 596, duration: 1.010s, episode steps: 9, steps per second: 9, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.133 [-1.590, 2.471], mean_best_reward: --\n 10911/100000: episode: 597, duration: 0.800s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.098 [-2.743, 1.765], mean_best_reward: --\n 10933/100000: episode: 598, duration: 1.681s, episode steps: 22, steps per second: 13, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.049 [-1.562, 0.951], mean_best_reward: --\n 10946/100000: episode: 599, duration: 1.012s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.125 [-1.416, 2.420], mean_best_reward: --\n 10963/100000: episode: 600, duration: 1.609s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.072 [-1.020, 0.630], mean_best_reward: --\n 11001/100000: episode: 601, duration: 3.679s, episode steps: 38, steps per second: 10, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: -0.069 [-0.966, 0.611], mean_best_reward: 39.500000\n 11026/100000: episode: 602, duration: 2.304s, episode steps: 25, steps per second: 11, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.060 [-0.790, 1.226], mean_best_reward: --\n 11051/100000: episode: 603, duration: 2.295s, episode steps: 25, steps per second: 11, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: -0.097 [-1.133, 0.563], mean_best_reward: --\n 11093/100000: episode: 604, duration: 3.798s, episode steps: 42, steps per second: 11, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: 0.085 [-2.331, 1.966], mean_best_reward: --\n 11137/100000: episode: 605, duration: 3.813s, episode steps: 44, steps per second: 12, episode reward: 44.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.477 [0.000, 1.000], mean observation: -0.086 [-0.786, 0.579], mean_best_reward: --\n 11159/100000: episode: 606, duration: 1.683s, episode steps: 22, steps per second: 13, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.086 [-2.126, 1.166], mean_best_reward: --\n 11194/100000: episode: 607, duration: 2.998s, episode steps: 35, steps per second: 12, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.042 [-1.043, 0.565], mean_best_reward: --\n 11243/100000: episode: 608, duration: 4.216s, episode steps: 49, steps per second: 12, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.449 [0.000, 1.000], mean observation: -0.145 [-0.942, 0.555], mean_best_reward: --\n 11288/100000: episode: 609, duration: 2.991s, episode steps: 45, steps per second: 15, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.060 [-0.805, 1.435], mean_best_reward: --\n 11312/100000: episode: 610, duration: 1.690s, episode steps: 24, steps per second: 14, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.047 [-0.832, 1.589], mean_best_reward: --\n 11350/100000: episode: 611, duration: 3.299s, episode steps: 38, steps per second: 12, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.112 [-1.448, 0.622], mean_best_reward: --\n 11456/100000: episode: 612, duration: 9.000s, episode steps: 106, steps per second: 12, episode reward: 106.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: -0.041 [-0.826, 1.291], mean_best_reward: --\n 11468/100000: episode: 613, duration: 0.822s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.080 [-1.210, 1.939], mean_best_reward: --\n 11536/100000: episode: 614, duration: 4.779s, episode steps: 68, steps per second: 14, episode reward: 68.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.559 [0.000, 1.000], mean observation: 0.121 [-2.038, 1.758], mean_best_reward: --\n 11551/100000: episode: 615, duration: 1.403s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.127 [-0.735, 1.209], mean_best_reward: --\n 11573/100000: episode: 616, duration: 1.810s, episode steps: 22, steps per second: 12, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.068 [-1.609, 0.811], mean_best_reward: --\n 11606/100000: episode: 617, duration: 2.979s, episode steps: 33, steps per second: 11, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.485 [0.000, 1.000], mean observation: -0.115 [-0.972, 0.419], mean_best_reward: --\n 11625/100000: episode: 618, duration: 2.022s, episode steps: 19, steps per second: 9, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.105 [-1.533, 0.751], mean_best_reward: --\n 11739/100000: episode: 619, duration: 8.615s, episode steps: 114, steps per second: 13, episode reward: 114.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.081 [-1.025, 1.275], mean_best_reward: --\n 11760/100000: episode: 620, duration: 1.582s, episode steps: 21, steps per second: 13, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.094 [-0.790, 1.185], mean_best_reward: --\n 11814/100000: episode: 621, duration: 4.601s, episode steps: 54, steps per second: 12, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.537 [0.000, 1.000], mean observation: -0.044 [-1.802, 0.776], mean_best_reward: --\n 11840/100000: episode: 622, duration: 2.501s, episode steps: 26, steps per second: 10, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: 0.058 [-0.990, 1.503], mean_best_reward: --\n 11858/100000: episode: 623, duration: 1.715s, episode steps: 18, steps per second: 10, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.076 [-2.144, 1.348], mean_best_reward: --\n 11876/100000: episode: 624, duration: 1.413s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.088 [-0.598, 0.961], mean_best_reward: --\n 11894/100000: episode: 625, duration: 1.395s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.109 [-1.007, 0.548], mean_best_reward: --\n 11914/100000: episode: 626, duration: 1.783s, episode steps: 20, steps per second: 11, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.066 [-0.785, 1.535], mean_best_reward: --\n 11932/100000: episode: 627, duration: 1.215s, episode steps: 18, steps per second: 15, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.083 [-0.820, 1.254], mean_best_reward: --\n 11948/100000: episode: 628, duration: 1.406s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.128 [-1.720, 0.771], mean_best_reward: --\n 11963/100000: episode: 629, duration: 0.998s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.076 [-1.392, 0.836], mean_best_reward: --\n 12012/100000: episode: 630, duration: 3.691s, episode steps: 49, steps per second: 13, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.096 [-0.477, 1.321], mean_best_reward: --\n 12024/100000: episode: 631, duration: 1.091s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.148 [-1.339, 2.293], mean_best_reward: --\n 12067/100000: episode: 632, duration: 3.597s, episode steps: 43, steps per second: 12, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.535 [0.000, 1.000], mean observation: 0.095 [-0.409, 0.881], mean_best_reward: --\n 12091/100000: episode: 633, duration: 1.512s, episode steps: 24, steps per second: 16, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.045 [-1.010, 1.778], mean_best_reward: --\n 12154/100000: episode: 634, duration: 5.783s, episode steps: 63, steps per second: 11, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.141 [-0.569, 0.905], mean_best_reward: --\n 12168/100000: episode: 635, duration: 1.209s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.075 [-0.839, 1.461], mean_best_reward: --\n 12187/100000: episode: 636, duration: 1.989s, episode steps: 19, steps per second: 10, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.105 [-0.553, 1.019], mean_best_reward: --\n 12211/100000: episode: 637, duration: 2.015s, episode steps: 24, steps per second: 12, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.098 [-0.569, 1.071], mean_best_reward: --\n 12231/100000: episode: 638, duration: 1.506s, episode steps: 20, steps per second: 13, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-1.159, 2.013], mean_best_reward: --\n 12243/100000: episode: 639, duration: 1.391s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-0.799, 1.517], mean_best_reward: --\n 12275/100000: episode: 640, duration: 2.602s, episode steps: 32, steps per second: 12, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.094 [-0.653, 1.475], mean_best_reward: --\n 12289/100000: episode: 641, duration: 1.679s, episode steps: 14, steps per second: 8, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.093 [-1.160, 1.909], mean_best_reward: --\n 12328/100000: episode: 642, duration: 3.401s, episode steps: 39, steps per second: 11, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.462 [0.000, 1.000], mean observation: -0.082 [-1.003, 0.591], mean_best_reward: --\n 12352/100000: episode: 643, duration: 2.199s, episode steps: 24, steps per second: 11, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.006 [-1.559, 2.152], mean_best_reward: --\n 12394/100000: episode: 644, duration: 3.115s, episode steps: 42, steps per second: 13, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.452 [0.000, 1.000], mean observation: -0.064 [-0.863, 0.606], mean_best_reward: --\n 12449/100000: episode: 645, duration: 4.381s, episode steps: 55, steps per second: 13, episode reward: 55.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.509 [0.000, 1.000], mean observation: 0.061 [-0.626, 1.149], mean_best_reward: --\n 12466/100000: episode: 646, duration: 1.400s, episode steps: 17, steps per second: 12, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: -0.089 [-1.108, 0.597], mean_best_reward: --\n 12483/100000: episode: 647, duration: 1.413s, episode steps: 17, steps per second: 12, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.076 [-1.132, 1.639], mean_best_reward: --\n 12501/100000: episode: 648, duration: 1.793s, episode steps: 18, steps per second: 10, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.053 [-1.218, 1.971], mean_best_reward: --\n 12561/100000: episode: 649, duration: 4.709s, episode steps: 60, steps per second: 13, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.040 [-0.591, 1.205], mean_best_reward: --\n 12576/100000: episode: 650, duration: 1.282s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.093 [-1.116, 0.578], mean_best_reward: --\n 12612/100000: episode: 651, duration: 2.700s, episode steps: 36, steps per second: 13, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.049 [-1.124, 0.563], mean_best_reward: 133.000000\n 12626/100000: episode: 652, duration: 1.013s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.085 [-1.963, 1.227], mean_best_reward: --\n 12638/100000: episode: 653, duration: 0.983s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.109 [-1.167, 1.927], mean_best_reward: --\n 12651/100000: episode: 654, duration: 1.098s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.116 [-1.337, 2.268], mean_best_reward: --\n 12659/100000: episode: 655, duration: 0.816s, episode steps: 8, steps per second: 10, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.541, 2.511], mean_best_reward: --\n 12694/100000: episode: 656, duration: 2.893s, episode steps: 35, steps per second: 12, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.457 [0.000, 1.000], mean observation: -0.028 [-1.177, 1.538], mean_best_reward: --\n 12709/100000: episode: 657, duration: 0.903s, episode steps: 15, steps per second: 17, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.071 [-1.830, 1.203], mean_best_reward: --\n 12720/100000: episode: 658, duration: 1.087s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.152 [-1.339, 2.336], mean_best_reward: --\n 12728/100000: episode: 659, duration: 1.006s, episode steps: 8, steps per second: 8, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.137 [-2.550, 1.615], mean_best_reward: --\n 12748/100000: episode: 660, duration: 2.295s, episode steps: 20, steps per second: 9, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.053 [-1.172, 1.816], mean_best_reward: --\n 12762/100000: episode: 661, duration: 1.305s, episode steps: 14, steps per second: 11, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.102 [-1.133, 1.868], mean_best_reward: --\n 12775/100000: episode: 662, duration: 0.900s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.125 [-1.977, 1.179], mean_best_reward: --\n 12790/100000: episode: 663, duration: 1.294s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.105 [-2.750, 1.713], mean_best_reward: --\n 12802/100000: episode: 664, duration: 0.985s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.107 [-1.189, 2.116], mean_best_reward: --\n 12814/100000: episode: 665, duration: 0.914s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.100 [-1.172, 1.933], mean_best_reward: --\n 12831/100000: episode: 666, duration: 1.208s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.235 [0.000, 1.000], mean observation: 0.067 [-1.775, 2.676], mean_best_reward: --\n 12846/100000: episode: 667, duration: 1.181s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.123 [-0.956, 1.859], mean_best_reward: --\n 12861/100000: episode: 668, duration: 0.914s, episode steps: 15, steps per second: 16, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.081 [-1.217, 1.965], mean_best_reward: --\n 12949/100000: episode: 669, duration: 6.882s, episode steps: 88, steps per second: 13, episode reward: 88.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.122 [-0.771, 1.515], mean_best_reward: --\n 12963/100000: episode: 670, duration: 1.998s, episode steps: 14, steps per second: 7, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.089 [-1.167, 1.911], mean_best_reward: --\n 12984/100000: episode: 671, duration: 1.213s, episode steps: 21, steps per second: 17, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.066 [-1.213, 2.053], mean_best_reward: --\n 13005/100000: episode: 672, duration: 1.282s, episode steps: 21, steps per second: 16, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.036 [-1.640, 0.979], mean_best_reward: --\n 13020/100000: episode: 673, duration: 1.017s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.103 [-1.331, 2.276], mean_best_reward: --\n 13036/100000: episode: 674, duration: 1.000s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.088 [-1.570, 2.608], mean_best_reward: --\n 13050/100000: episode: 675, duration: 1.202s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.099 [-1.589, 2.629], mean_best_reward: --\n 13068/100000: episode: 676, duration: 1.481s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.072 [-1.590, 2.512], mean_best_reward: --\n 13081/100000: episode: 677, duration: 1.101s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.154 [0.000, 1.000], mean observation: 0.128 [-1.757, 2.830], mean_best_reward: --\n 13100/100000: episode: 678, duration: 1.089s, episode steps: 19, steps per second: 17, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.049 [-1.739, 1.026], mean_best_reward: --\n 13109/100000: episode: 679, duration: 0.791s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.153 [-1.330, 2.258], mean_best_reward: --\n 13120/100000: episode: 680, duration: 1.015s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.126 [-2.385, 1.560], mean_best_reward: --\n 13131/100000: episode: 681, duration: 0.799s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.114 [-0.945, 1.636], mean_best_reward: --\n 13149/100000: episode: 682, duration: 1.401s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.085 [-1.574, 2.687], mean_best_reward: --\n 13164/100000: episode: 683, duration: 1.090s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.071 [-1.031, 1.795], mean_best_reward: --\n 13184/100000: episode: 684, duration: 1.091s, episode steps: 20, steps per second: 18, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.063 [-1.949, 3.077], mean_best_reward: --\n 13197/100000: episode: 685, duration: 0.911s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.104 [-1.338, 2.269], mean_best_reward: --\n 13211/100000: episode: 686, duration: 1.204s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.084 [-1.981, 3.055], mean_best_reward: --\n 13222/100000: episode: 687, duration: 0.595s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.134 [-2.782, 1.732], mean_best_reward: --\n 13232/100000: episode: 688, duration: 0.608s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.121 [-1.581, 0.992], mean_best_reward: --\n 13250/100000: episode: 689, duration: 1.190s, episode steps: 18, steps per second: 15, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.056 [-1.409, 2.186], mean_best_reward: --\n 13263/100000: episode: 690, duration: 1.289s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.103 [-1.001, 1.791], mean_best_reward: --\n 13278/100000: episode: 691, duration: 0.919s, episode steps: 15, steps per second: 16, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.108 [-1.427, 2.471], mean_best_reward: --\n 13288/100000: episode: 692, duration: 0.597s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.169 [-1.713, 2.743], mean_best_reward: --\n 13298/100000: episode: 693, duration: 1.197s, episode steps: 10, steps per second: 8, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.137 [-1.920, 2.981], mean_best_reward: --\n 13312/100000: episode: 694, duration: 0.994s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.087 [-1.219, 2.033], mean_best_reward: --\n 13321/100000: episode: 695, duration: 0.706s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.135 [-2.495, 1.593], mean_best_reward: --\n 13335/100000: episode: 696, duration: 0.982s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.099 [-1.947, 3.044], mean_best_reward: --\n 13344/100000: episode: 697, duration: 0.609s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.333, 2.221], mean_best_reward: --\n 13355/100000: episode: 698, duration: 0.603s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.100 [-1.413, 2.250], mean_best_reward: --\n 13369/100000: episode: 699, duration: 1.087s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.083 [-1.999, 2.992], mean_best_reward: --\n 13382/100000: episode: 700, duration: 0.814s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.111 [-0.807, 1.485], mean_best_reward: --\n 13392/100000: episode: 701, duration: 0.589s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.133 [-1.593, 2.600], mean_best_reward: 68.500000\n 13402/100000: episode: 702, duration: 0.611s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.150 [-1.558, 2.527], mean_best_reward: --\n 13416/100000: episode: 703, duration: 1.083s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.081 [-0.795, 1.318], mean_best_reward: --\n 13426/100000: episode: 704, duration: 0.512s, episode steps: 10, steps per second: 20, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.124 [-1.400, 2.280], mean_best_reward: --\n 13450/100000: episode: 705, duration: 1.099s, episode steps: 24, steps per second: 22, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.044 [-1.016, 1.352], mean_best_reward: --\n 13461/100000: episode: 706, duration: 0.703s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.126 [-2.378, 1.525], mean_best_reward: --\n 13513/100000: episode: 707, duration: 2.992s, episode steps: 52, steps per second: 17, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.059 [-0.748, 0.921], mean_best_reward: --\n 13544/100000: episode: 708, duration: 1.998s, episode steps: 31, steps per second: 16, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.090 [-0.425, 1.314], mean_best_reward: --\n 13573/100000: episode: 709, duration: 1.802s, episode steps: 29, steps per second: 16, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.084 [-1.210, 2.277], mean_best_reward: --\n 13587/100000: episode: 710, duration: 1.008s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.134 [-1.645, 0.783], mean_best_reward: --\n 13603/100000: episode: 711, duration: 1.089s, episode steps: 16, steps per second: 15, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.069 [-1.008, 1.654], mean_best_reward: --\n 13627/100000: episode: 712, duration: 1.594s, episode steps: 24, steps per second: 15, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.107 [-0.553, 0.954], mean_best_reward: --\n 13639/100000: episode: 713, duration: 1.190s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.107 [-2.446, 1.528], mean_best_reward: --\n 13648/100000: episode: 714, duration: 0.801s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.157 [-1.781, 2.864], mean_best_reward: --\n 13665/100000: episode: 715, duration: 1.196s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.101 [-0.984, 1.884], mean_best_reward: --\n 13678/100000: episode: 716, duration: 0.904s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.066 [-1.822, 1.206], mean_best_reward: --\n 13724/100000: episode: 717, duration: 2.415s, episode steps: 46, steps per second: 19, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: -0.040 [-0.795, 1.032], mean_best_reward: --\n 13734/100000: episode: 718, duration: 0.688s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.149 [-0.946, 1.723], mean_best_reward: --\n 13757/100000: episode: 719, duration: 1.404s, episode steps: 23, steps per second: 16, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.084 [-2.061, 1.139], mean_best_reward: --\n 13769/100000: episode: 720, duration: 0.599s, episode steps: 12, steps per second: 20, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.107 [-2.521, 1.523], mean_best_reward: --\n 13783/100000: episode: 721, duration: 0.903s, episode steps: 14, steps per second: 16, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.109 [-1.762, 1.008], mean_best_reward: --\n 13792/100000: episode: 722, duration: 0.591s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.171 [-1.739, 2.879], mean_best_reward: --\n 13808/100000: episode: 723, duration: 1.104s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.076 [-1.159, 1.857], mean_best_reward: --\n 13821/100000: episode: 724, duration: 1.098s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.124 [-1.795, 1.136], mean_best_reward: --\n 13833/100000: episode: 725, duration: 0.893s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.090 [-1.162, 1.911], mean_best_reward: --\n 13850/100000: episode: 726, duration: 1.298s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.053 [-2.146, 1.401], mean_best_reward: --\n 13864/100000: episode: 727, duration: 0.810s, episode steps: 14, steps per second: 17, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.120 [-1.143, 2.122], mean_best_reward: --\n 13881/100000: episode: 728, duration: 1.191s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.107 [-0.556, 1.072], mean_best_reward: --\n 13896/100000: episode: 729, duration: 0.904s, episode steps: 15, steps per second: 17, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.267 [0.000, 1.000], mean observation: 0.072 [-1.415, 2.261], mean_best_reward: --\n 13912/100000: episode: 730, duration: 1.196s, episode steps: 16, steps per second: 13, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.105 [-1.529, 2.562], mean_best_reward: --\n 13924/100000: episode: 731, duration: 0.785s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.114 [-1.347, 2.218], mean_best_reward: --\n 13955/100000: episode: 732, duration: 1.817s, episode steps: 31, steps per second: 17, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: -0.071 [-1.738, 0.963], mean_best_reward: --\n 13968/100000: episode: 733, duration: 0.791s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.105 [-1.196, 1.793], mean_best_reward: --\n 13978/100000: episode: 734, duration: 0.691s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.139 [-1.583, 2.546], mean_best_reward: --\n 13996/100000: episode: 735, duration: 1.098s, episode steps: 18, steps per second: 16, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.070 [-1.341, 2.157], mean_best_reward: --\n 14006/100000: episode: 736, duration: 0.512s, episode steps: 10, steps per second: 20, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.121 [-0.957, 1.570], mean_best_reward: --\n 14028/100000: episode: 737, duration: 1.102s, episode steps: 22, steps per second: 20, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.066 [-1.932, 3.036], mean_best_reward: --\n 14039/100000: episode: 738, duration: 0.684s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.114 [-1.415, 2.316], mean_best_reward: --\n 14056/100000: episode: 739, duration: 0.899s, episode steps: 17, steps per second: 19, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.061 [-0.993, 1.681], mean_best_reward: --\n 14074/100000: episode: 740, duration: 0.913s, episode steps: 18, steps per second: 20, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.098 [-0.769, 1.618], mean_best_reward: --\n 14084/100000: episode: 741, duration: 0.690s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.114 [-1.165, 1.994], mean_best_reward: --\n 14202/100000: episode: 742, duration: 6.712s, episode steps: 118, steps per second: 18, episode reward: 118.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.071 [-1.552, 0.932], mean_best_reward: --\n 14215/100000: episode: 743, duration: 0.786s, episode steps: 13, steps per second: 17, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.107 [-2.865, 1.810], mean_best_reward: --\n 14238/100000: episode: 744, duration: 1.095s, episode steps: 23, steps per second: 21, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.609 [0.000, 1.000], mean observation: -0.049 [-2.017, 1.222], mean_best_reward: --\n 14252/100000: episode: 745, duration: 0.709s, episode steps: 14, steps per second: 20, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.107 [-2.082, 1.148], mean_best_reward: --\n 14315/100000: episode: 746, duration: 4.308s, episode steps: 63, steps per second: 15, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: 0.122 [-0.749, 0.996], mean_best_reward: --\n 14325/100000: episode: 747, duration: 0.880s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.106 [-1.584, 2.498], mean_best_reward: --\n 14370/100000: episode: 748, duration: 3.204s, episode steps: 45, steps per second: 14, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.026 [-0.607, 1.118], mean_best_reward: --\n 14383/100000: episode: 749, duration: 0.908s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.117 [-1.792, 0.975], mean_best_reward: --\n 14393/100000: episode: 750, duration: 0.884s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.155 [-1.540, 2.548], mean_best_reward: --\n 14410/100000: episode: 751, duration: 1.802s, episode steps: 17, steps per second: 9, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: -0.105 [-0.994, 0.549], mean_best_reward: 57.500000\n 14421/100000: episode: 752, duration: 0.803s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.126 [-2.774, 1.759], mean_best_reward: --\n 14461/100000: episode: 753, duration: 2.887s, episode steps: 40, steps per second: 14, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.067 [-1.675, 0.946], mean_best_reward: --\n 14476/100000: episode: 754, duration: 1.106s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.087 [-2.676, 1.726], mean_best_reward: --\n 14492/100000: episode: 755, duration: 1.186s, episode steps: 16, steps per second: 13, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.052 [-2.640, 1.799], mean_best_reward: --\n 14503/100000: episode: 756, duration: 0.614s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.112 [-1.420, 2.307], mean_best_reward: --\n 14514/100000: episode: 757, duration: 0.703s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.114 [-1.355, 2.310], mean_best_reward: --\n 14524/100000: episode: 758, duration: 0.992s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.121 [-1.600, 2.551], mean_best_reward: --\n 14537/100000: episode: 759, duration: 1.104s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.094 [-1.656, 0.953], mean_best_reward: --\n 14547/100000: episode: 760, duration: 0.592s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.160 [-1.325, 2.146], mean_best_reward: --\n 14561/100000: episode: 761, duration: 1.018s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.114 [-1.264, 0.563], mean_best_reward: --\n 14573/100000: episode: 762, duration: 1.177s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.111 [-2.085, 1.358], mean_best_reward: --\n 14588/100000: episode: 763, duration: 1.396s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.104 [-1.933, 1.148], mean_best_reward: --\n 14605/100000: episode: 764, duration: 1.316s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.072 [-1.576, 2.503], mean_best_reward: --\n 14623/100000: episode: 765, duration: 1.501s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.061 [-2.385, 1.538], mean_best_reward: --\n 14637/100000: episode: 766, duration: 0.903s, episode steps: 14, steps per second: 16, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.143 [0.000, 1.000], mean observation: 0.089 [-1.974, 3.043], mean_best_reward: --\n 14648/100000: episode: 767, duration: 0.904s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.131 [-1.225, 1.973], mean_best_reward: --\n 14670/100000: episode: 768, duration: 1.897s, episode steps: 22, steps per second: 12, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.058 [-1.339, 0.774], mean_best_reward: --\n 14685/100000: episode: 769, duration: 0.995s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.079 [-2.174, 1.361], mean_best_reward: --\n 14694/100000: episode: 770, duration: 1.089s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.139 [-1.167, 1.950], mean_best_reward: --\n 14714/100000: episode: 771, duration: 1.894s, episode steps: 20, steps per second: 11, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.650 [0.000, 1.000], mean observation: -0.067 [-2.048, 1.153], mean_best_reward: --\n 14729/100000: episode: 772, duration: 1.107s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.102 [-1.461, 0.751], mean_best_reward: --\n 14741/100000: episode: 773, duration: 0.797s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.132 [-3.075, 1.933], mean_best_reward: --\n 14757/100000: episode: 774, duration: 1.115s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.064 [-1.203, 1.816], mean_best_reward: --\n 14775/100000: episode: 775, duration: 1.597s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.063 [-2.544, 1.585], mean_best_reward: --\n 14801/100000: episode: 776, duration: 1.887s, episode steps: 26, steps per second: 14, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.654 [0.000, 1.000], mean observation: -0.000 [-2.173, 1.575], mean_best_reward: --\n 14822/100000: episode: 777, duration: 1.495s, episode steps: 21, steps per second: 14, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.024 [-2.257, 1.594], mean_best_reward: --\n 14833/100000: episode: 778, duration: 0.697s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.151 [-2.799, 1.713], mean_best_reward: --\n 14851/100000: episode: 779, duration: 1.804s, episode steps: 18, steps per second: 10, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.083 [-1.042, 0.618], mean_best_reward: --\n 14871/100000: episode: 780, duration: 1.309s, episode steps: 20, steps per second: 15, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.049 [-0.843, 1.585], mean_best_reward: --\n 14883/100000: episode: 781, duration: 0.995s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.126 [-2.572, 1.535], mean_best_reward: --\n 14934/100000: episode: 782, duration: 4.089s, episode steps: 51, steps per second: 12, episode reward: 51.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.490 [0.000, 1.000], mean observation: 0.087 [-0.712, 1.503], mean_best_reward: --\n 14950/100000: episode: 783, duration: 1.217s, episode steps: 16, steps per second: 13, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.083 [-2.048, 1.193], mean_best_reward: --\n 14961/100000: episode: 784, duration: 1.016s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.207, 1.357], mean_best_reward: --\n 14993/100000: episode: 785, duration: 2.500s, episode steps: 32, steps per second: 13, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: 0.017 [-3.068, 2.340], mean_best_reward: --\n 15010/100000: episode: 786, duration: 1.283s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.032 [-2.576, 1.797], mean_best_reward: --\n 15025/100000: episode: 787, duration: 1.717s, episode steps: 15, steps per second: 9, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.091 [-2.328, 1.401], mean_best_reward: --\n 15034/100000: episode: 788, duration: 0.796s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.155 [-2.796, 1.741], mean_best_reward: --\n 15044/100000: episode: 789, duration: 0.799s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.129 [-1.556, 2.458], mean_best_reward: --\n 15055/100000: episode: 790, duration: 0.702s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.104 [-1.793, 1.215], mean_best_reward: --\n 15066/100000: episode: 791, duration: 1.179s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.116 [-2.367, 1.427], mean_best_reward: --\n 15077/100000: episode: 792, duration: 1.209s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.133 [-1.421, 2.370], mean_best_reward: --\n 15087/100000: episode: 793, duration: 0.605s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.094 [-2.183, 1.416], mean_best_reward: --\n 15101/100000: episode: 794, duration: 1.088s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.116 [-2.051, 1.154], mean_best_reward: --\n 15112/100000: episode: 795, duration: 0.795s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.149 [-2.819, 1.740], mean_best_reward: --\n 15122/100000: episode: 796, duration: 0.703s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-3.000, 1.936], mean_best_reward: --\n 15139/100000: episode: 797, duration: 1.306s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.077 [-2.448, 1.551], mean_best_reward: --\n 15154/100000: episode: 798, duration: 1.016s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.098 [-2.288, 1.349], mean_best_reward: --\n 15163/100000: episode: 799, duration: 0.903s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.145 [-2.829, 1.766], mean_best_reward: --\n 15173/100000: episode: 800, duration: 0.899s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.124 [-1.995, 1.149], mean_best_reward: --\n 15183/100000: episode: 801, duration: 0.796s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.154 [-2.608, 1.522], mean_best_reward: 89.000000\n 15203/100000: episode: 802, duration: 1.303s, episode steps: 20, steps per second: 15, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.082 [-1.189, 0.777], mean_best_reward: --\n 15224/100000: episode: 803, duration: 2.000s, episode steps: 21, steps per second: 10, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.619 [0.000, 1.000], mean observation: -0.029 [-1.682, 1.183], mean_best_reward: --\n 15234/100000: episode: 804, duration: 0.615s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.141 [-2.086, 1.356], mean_best_reward: --\n 15253/100000: episode: 805, duration: 1.383s, episode steps: 19, steps per second: 14, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.078 [-2.227, 1.340], mean_best_reward: --\n 15267/100000: episode: 806, duration: 0.902s, episode steps: 14, steps per second: 16, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.084 [-2.386, 1.552], mean_best_reward: --\n 15278/100000: episode: 807, duration: 0.800s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.123 [-2.211, 1.366], mean_best_reward: --\n 15290/100000: episode: 808, duration: 0.913s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.118 [-0.951, 1.524], mean_best_reward: --\n 15308/100000: episode: 809, duration: 1.885s, episode steps: 18, steps per second: 10, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.067 [-0.845, 1.574], mean_best_reward: --\n 15325/100000: episode: 810, duration: 1.295s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.093 [-2.324, 1.376], mean_best_reward: --\n 15335/100000: episode: 811, duration: 0.899s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.138 [-2.528, 1.530], mean_best_reward: --\n 15346/100000: episode: 812, duration: 1.005s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.138 [-1.783, 0.971], mean_best_reward: --\n 15365/100000: episode: 813, duration: 1.793s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.091 [-1.793, 0.972], mean_best_reward: --\n 15386/100000: episode: 814, duration: 2.011s, episode steps: 21, steps per second: 10, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.038 [-0.805, 1.081], mean_best_reward: --\n 15403/100000: episode: 815, duration: 1.608s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.057 [-1.509, 2.210], mean_best_reward: --\n 15418/100000: episode: 816, duration: 1.881s, episode steps: 15, steps per second: 8, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.084 [-0.972, 1.401], mean_best_reward: --\n 15467/100000: episode: 817, duration: 5.312s, episode steps: 49, steps per second: 9, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.612 [0.000, 1.000], mean observation: 0.057 [-2.604, 2.252], mean_best_reward: --\n 15477/100000: episode: 818, duration: 0.594s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.126 [-0.937, 1.683], mean_best_reward: --\n 15488/100000: episode: 819, duration: 1.204s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.112 [-1.809, 1.037], mean_best_reward: --\n 15506/100000: episode: 820, duration: 1.581s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.091 [-0.757, 1.208], mean_best_reward: --\n 15516/100000: episode: 821, duration: 0.999s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.138 [-1.632, 0.970], mean_best_reward: --\n 15579/100000: episode: 822, duration: 6.000s, episode steps: 63, steps per second: 10, episode reward: 63.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.118 [-1.276, 1.348], mean_best_reward: --\n 15591/100000: episode: 823, duration: 1.002s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.118 [-1.140, 2.048], mean_best_reward: --\n 15611/100000: episode: 824, duration: 1.697s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: 0.088 [-0.798, 1.367], mean_best_reward: --\n 15625/100000: episode: 825, duration: 0.897s, episode steps: 14, steps per second: 16, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.098 [-0.800, 1.603], mean_best_reward: --\n 15649/100000: episode: 826, duration: 2.218s, episode steps: 24, steps per second: 11, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.562, 1.094], mean_best_reward: --\n 15664/100000: episode: 827, duration: 1.398s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: 0.079 [-0.794, 1.321], mean_best_reward: --\n 15676/100000: episode: 828, duration: 1.877s, episode steps: 12, steps per second: 6, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.108 [-2.573, 1.617], mean_best_reward: --\n 15687/100000: episode: 829, duration: 0.914s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.133 [-2.906, 1.776], mean_best_reward: --\n 15699/100000: episode: 830, duration: 1.090s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.109 [-1.209, 2.056], mean_best_reward: --\n 15711/100000: episode: 831, duration: 0.997s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.116 [-3.025, 1.975], mean_best_reward: --\n 15723/100000: episode: 832, duration: 1.303s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: 0.121 [-0.749, 1.289], mean_best_reward: --\n 15732/100000: episode: 833, duration: 0.799s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.152 [-2.249, 1.366], mean_best_reward: --\n 15749/100000: episode: 834, duration: 1.211s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.050 [-1.001, 1.642], mean_best_reward: --\n 15772/100000: episode: 835, duration: 1.601s, episode steps: 23, steps per second: 14, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.066 [-1.164, 2.091], mean_best_reward: --\n 15800/100000: episode: 836, duration: 2.099s, episode steps: 28, steps per second: 13, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.679 [0.000, 1.000], mean observation: 0.010 [-2.608, 1.929], mean_best_reward: --\n 15814/100000: episode: 837, duration: 1.386s, episode steps: 14, steps per second: 10, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.109 [-2.180, 1.358], mean_best_reward: --\n 15828/100000: episode: 838, duration: 1.403s, episode steps: 14, steps per second: 10, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.110 [-0.620, 1.218], mean_best_reward: --\n 15841/100000: episode: 839, duration: 1.488s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.129 [-2.840, 1.717], mean_best_reward: --\n 15851/100000: episode: 840, duration: 1.016s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.142 [-1.359, 2.265], mean_best_reward: --\n 15884/100000: episode: 841, duration: 2.803s, episode steps: 33, steps per second: 12, episode reward: 33.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.394 [0.000, 1.000], mean observation: -0.005 [-1.324, 1.735], mean_best_reward: --\n 15896/100000: episode: 842, duration: 1.095s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.102 [-3.031, 1.972], mean_best_reward: --\n 15907/100000: episode: 843, duration: 1.095s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.126 [-1.746, 0.946], mean_best_reward: --\n 15953/100000: episode: 844, duration: 3.291s, episode steps: 46, steps per second: 14, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.630 [0.000, 1.000], mean observation: 0.038 [-2.770, 2.273], mean_best_reward: --\n 15972/100000: episode: 845, duration: 1.412s, episode steps: 19, steps per second: 13, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.098 [-0.949, 0.437], mean_best_reward: --\n 15980/100000: episode: 846, duration: 0.583s, episode steps: 8, steps per second: 14, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.148 [-1.591, 2.561], mean_best_reward: --\n 16014/100000: episode: 847, duration: 3.095s, episode steps: 34, steps per second: 11, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.529 [0.000, 1.000], mean observation: 0.092 [-0.589, 0.933], mean_best_reward: --\n 16029/100000: episode: 848, duration: 1.409s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.108 [-1.873, 1.135], mean_best_reward: --\n 16048/100000: episode: 849, duration: 1.587s, episode steps: 19, steps per second: 12, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.065 [-1.577, 2.360], mean_best_reward: --\n 16058/100000: episode: 850, duration: 0.419s, episode steps: 10, steps per second: 24, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.138 [-0.750, 1.394], mean_best_reward: --\n 16084/100000: episode: 851, duration: 2.099s, episode steps: 26, steps per second: 12, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.058 [-2.970, 1.912], mean_best_reward: 42.000000\n 16144/100000: episode: 852, duration: 4.898s, episode steps: 60, steps per second: 12, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: -0.022 [-1.917, 1.688], mean_best_reward: --\n 16183/100000: episode: 853, duration: 3.096s, episode steps: 39, steps per second: 13, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.081 [-0.407, 0.885], mean_best_reward: --\n 16195/100000: episode: 854, duration: 0.895s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.089 [-2.243, 1.401], mean_best_reward: --\n 16204/100000: episode: 855, duration: 1.081s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.139 [-2.348, 1.386], mean_best_reward: --\n 16216/100000: episode: 856, duration: 0.899s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.143 [-2.591, 1.555], mean_best_reward: --\n 16226/100000: episode: 857, duration: 0.917s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-3.101, 1.908], mean_best_reward: --\n 16237/100000: episode: 858, duration: 0.777s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.144 [-1.709, 0.941], mean_best_reward: --\n 16253/100000: episode: 859, duration: 1.205s, episode steps: 16, steps per second: 13, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.094 [-2.624, 1.595], mean_best_reward: --\n 16262/100000: episode: 860, duration: 0.805s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.167 [-2.279, 1.329], mean_best_reward: --\n 16273/100000: episode: 861, duration: 1.286s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.111 [-1.520, 1.012], mean_best_reward: --\n 16283/100000: episode: 862, duration: 0.803s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-0.752, 1.531], mean_best_reward: --\n 16293/100000: episode: 863, duration: 0.799s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.121 [-2.627, 1.612], mean_best_reward: --\n 16312/100000: episode: 864, duration: 1.497s, episode steps: 19, steps per second: 13, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.076 [-1.430, 0.795], mean_best_reward: --\n 16326/100000: episode: 865, duration: 1.189s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.103 [-1.592, 0.790], mean_best_reward: --\n 16351/100000: episode: 866, duration: 1.699s, episode steps: 25, steps per second: 15, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.560 [0.000, 1.000], mean observation: 0.115 [-0.604, 0.929], mean_best_reward: --\n 16363/100000: episode: 867, duration: 1.292s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.108 [-1.449, 0.745], mean_best_reward: --\n 16374/100000: episode: 868, duration: 0.702s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.111 [-2.260, 1.395], mean_best_reward: --\n 16405/100000: episode: 869, duration: 2.518s, episode steps: 31, steps per second: 12, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.548 [0.000, 1.000], mean observation: 0.160 [-0.571, 0.969], mean_best_reward: --\n 16422/100000: episode: 870, duration: 1.376s, episode steps: 17, steps per second: 12, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.062 [-1.571, 1.029], mean_best_reward: --\n 16433/100000: episode: 871, duration: 0.700s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.127 [-1.767, 2.866], mean_best_reward: --\n 16453/100000: episode: 872, duration: 1.614s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.078 [-2.692, 1.613], mean_best_reward: --\n 16462/100000: episode: 873, duration: 0.897s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.127 [-2.298, 1.381], mean_best_reward: --\n 16478/100000: episode: 874, duration: 1.489s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.077 [-2.691, 1.737], mean_best_reward: --\n 16491/100000: episode: 875, duration: 1.406s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.087 [-2.193, 1.389], mean_best_reward: --\n 16507/100000: episode: 876, duration: 1.885s, episode steps: 16, steps per second: 8, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.079 [-0.974, 1.583], mean_best_reward: --\n 16517/100000: episode: 877, duration: 0.921s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.099 [-1.199, 1.851], mean_best_reward: --\n 16528/100000: episode: 878, duration: 1.197s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.117 [-2.257, 1.414], mean_best_reward: --\n 16559/100000: episode: 879, duration: 3.117s, episode steps: 31, steps per second: 10, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: 0.111 [-0.612, 0.995], mean_best_reward: --\n 16573/100000: episode: 880, duration: 0.893s, episode steps: 14, steps per second: 16, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.264, 0.740], mean_best_reward: --\n 16583/100000: episode: 881, duration: 0.800s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.138 [-2.010, 1.147], mean_best_reward: --\n 16604/100000: episode: 882, duration: 2.490s, episode steps: 21, steps per second: 8, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.068 [-2.321, 1.374], mean_best_reward: --\n 16620/100000: episode: 883, duration: 1.599s, episode steps: 16, steps per second: 10, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.124 [-1.658, 0.752], mean_best_reward: --\n 16632/100000: episode: 884, duration: 1.111s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.115 [-2.190, 1.402], mean_best_reward: --\n 16644/100000: episode: 885, duration: 0.603s, episode steps: 12, steps per second: 20, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.127 [-2.120, 1.194], mean_best_reward: --\n 16663/100000: episode: 886, duration: 1.880s, episode steps: 19, steps per second: 10, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.063 [-2.269, 1.422], mean_best_reward: --\n 16673/100000: episode: 887, duration: 0.917s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.123 [-1.360, 2.108], mean_best_reward: --\n 16684/100000: episode: 888, duration: 0.795s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: 0.127 [-0.750, 1.308], mean_best_reward: --\n 16695/100000: episode: 889, duration: 0.884s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.141 [-2.828, 1.731], mean_best_reward: --\n 16709/100000: episode: 890, duration: 1.500s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.091 [-1.910, 1.194], mean_best_reward: --\n 16732/100000: episode: 891, duration: 1.812s, episode steps: 23, steps per second: 13, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.435 [0.000, 1.000], mean observation: 0.048 [-0.946, 1.294], mean_best_reward: --\n 16813/100000: episode: 892, duration: 7.593s, episode steps: 81, steps per second: 11, episode reward: 81.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.494 [0.000, 1.000], mean observation: -0.065 [-1.326, 0.799], mean_best_reward: --\n 16827/100000: episode: 893, duration: 1.506s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.109 [-2.577, 1.586], mean_best_reward: --\n 16863/100000: episode: 894, duration: 3.089s, episode steps: 36, steps per second: 12, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.417 [0.000, 1.000], mean observation: -0.046 [-1.370, 1.528], mean_best_reward: --\n 16875/100000: episode: 895, duration: 1.302s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.088 [-2.476, 1.595], mean_best_reward: --\n 16888/100000: episode: 896, duration: 1.795s, episode steps: 13, steps per second: 7, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.081 [-1.215, 1.930], mean_best_reward: --\n 16899/100000: episode: 897, duration: 1.407s, episode steps: 11, steps per second: 8, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.106 [-1.857, 1.147], mean_best_reward: --\n 16915/100000: episode: 898, duration: 1.783s, episode steps: 16, steps per second: 9, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.064 [-2.201, 1.417], mean_best_reward: --\n 16939/100000: episode: 899, duration: 1.602s, episode steps: 24, steps per second: 15, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.060 [-0.952, 1.509], mean_best_reward: --\n 16956/100000: episode: 900, duration: 0.907s, episode steps: 17, steps per second: 19, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.096 [-2.444, 1.421], mean_best_reward: --\n 16965/100000: episode: 901, duration: 0.889s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.141 [-2.348, 1.411], mean_best_reward: 93.000000\n 16975/100000: episode: 902, duration: 1.117s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.164 [-2.784, 1.722], mean_best_reward: --\n 16985/100000: episode: 903, duration: 0.791s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.117 [-1.911, 1.218], mean_best_reward: --\n 16998/100000: episode: 904, duration: 1.799s, episode steps: 13, steps per second: 7, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.086 [-1.725, 1.022], mean_best_reward: --\n 17014/100000: episode: 905, duration: 1.500s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.094 [-2.016, 1.162], mean_best_reward: --\n 17025/100000: episode: 906, duration: 1.284s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.120 [-0.941, 1.726], mean_best_reward: --\n 17041/100000: episode: 907, duration: 1.522s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.114 [-0.808, 1.643], mean_best_reward: --\n 17057/100000: episode: 908, duration: 1.177s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.111 [-0.786, 1.649], mean_best_reward: --\n 17066/100000: episode: 909, duration: 0.703s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.140 [-2.445, 1.518], mean_best_reward: --\n 17094/100000: episode: 910, duration: 2.297s, episode steps: 28, steps per second: 12, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.056 [-1.592, 0.800], mean_best_reward: --\n 17120/100000: episode: 911, duration: 2.114s, episode steps: 26, steps per second: 12, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.121 [-1.119, 0.372], mean_best_reward: --\n 17131/100000: episode: 912, duration: 0.889s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.148 [-1.551, 2.464], mean_best_reward: --\n 17154/100000: episode: 913, duration: 1.706s, episode steps: 23, steps per second: 13, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.092 [-1.346, 2.447], mean_best_reward: --\n 17185/100000: episode: 914, duration: 3.000s, episode steps: 31, steps per second: 10, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.484 [0.000, 1.000], mean observation: 0.073 [-0.837, 1.615], mean_best_reward: --\n 17202/100000: episode: 915, duration: 1.097s, episode steps: 17, steps per second: 15, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.084 [-1.892, 1.172], mean_best_reward: --\n 17212/100000: episode: 916, duration: 1.000s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.121 [-2.500, 1.570], mean_best_reward: --\n 17224/100000: episode: 917, duration: 1.599s, episode steps: 12, steps per second: 8, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.115 [-1.581, 2.561], mean_best_reward: --\n 17285/100000: episode: 918, duration: 4.403s, episode steps: 61, steps per second: 14, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.492 [0.000, 1.000], mean observation: -0.020 [-1.204, 1.614], mean_best_reward: --\n 17306/100000: episode: 919, duration: 1.890s, episode steps: 21, steps per second: 11, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.069 [-1.285, 0.816], mean_best_reward: --\n 17317/100000: episode: 920, duration: 1.003s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.124 [-1.415, 2.273], mean_best_reward: --\n 17330/100000: episode: 921, duration: 0.897s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.103 [-1.351, 2.153], mean_best_reward: --\n 17372/100000: episode: 922, duration: 3.102s, episode steps: 42, steps per second: 14, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.118 [-0.566, 1.167], mean_best_reward: --\n 17388/100000: episode: 923, duration: 0.991s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.095 [-1.816, 1.016], mean_best_reward: --\n 17399/100000: episode: 924, duration: 0.898s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.115 [-2.748, 1.805], mean_best_reward: --\n 17414/100000: episode: 925, duration: 0.906s, episode steps: 15, steps per second: 17, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.093 [-2.262, 1.361], mean_best_reward: --\n 17427/100000: episode: 926, duration: 0.995s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.088 [-0.832, 1.345], mean_best_reward: --\n 17436/100000: episode: 927, duration: 0.689s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.159 [-2.866, 1.727], mean_best_reward: --\n 17449/100000: episode: 928, duration: 1.299s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.120 [-1.363, 2.300], mean_best_reward: --\n 17467/100000: episode: 929, duration: 1.416s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.046 [-1.995, 2.998], mean_best_reward: --\n 17495/100000: episode: 930, duration: 2.592s, episode steps: 28, steps per second: 11, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.074 [-0.609, 1.014], mean_best_reward: --\n 17521/100000: episode: 931, duration: 2.001s, episode steps: 26, steps per second: 13, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.423 [0.000, 1.000], mean observation: 0.073 [-0.751, 1.549], mean_best_reward: --\n 17533/100000: episode: 932, duration: 1.294s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.107 [-2.545, 1.583], mean_best_reward: --\n 17572/100000: episode: 933, duration: 4.000s, episode steps: 39, steps per second: 10, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: 0.104 [-0.825, 1.351], mean_best_reward: --\n 17582/100000: episode: 934, duration: 1.194s, episode steps: 10, steps per second: 8, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.165 [-3.104, 1.934], mean_best_reward: --\n 17617/100000: episode: 935, duration: 3.317s, episode steps: 35, steps per second: 11, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.088 [-0.763, 1.103], mean_best_reward: --\n 17627/100000: episode: 936, duration: 0.980s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.131 [-1.617, 2.622], mean_best_reward: --\n 17684/100000: episode: 937, duration: 5.503s, episode steps: 57, steps per second: 10, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.491 [0.000, 1.000], mean observation: 0.071 [-0.677, 1.311], mean_best_reward: --\n 17700/100000: episode: 938, duration: 1.410s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.098 [-2.109, 1.196], mean_best_reward: --\n 17710/100000: episode: 939, duration: 1.101s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.138 [-0.950, 1.687], mean_best_reward: --\n 17723/100000: episode: 940, duration: 1.017s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.095 [-1.747, 1.166], mean_best_reward: --\n 17735/100000: episode: 941, duration: 1.102s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.135 [-0.754, 1.565], mean_best_reward: --\n 17754/100000: episode: 942, duration: 1.689s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.104 [-1.960, 0.993], mean_best_reward: --\n 17765/100000: episode: 943, duration: 0.806s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.140 [-2.334, 1.366], mean_best_reward: --\n 17782/100000: episode: 944, duration: 0.895s, episode steps: 17, steps per second: 19, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.064 [-1.622, 1.004], mean_best_reward: --\n 17806/100000: episode: 945, duration: 1.295s, episode steps: 24, steps per second: 19, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.103 [-1.869, 0.792], mean_best_reward: --\n 17817/100000: episode: 946, duration: 1.012s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.111 [-2.780, 1.747], mean_best_reward: --\n 17829/100000: episode: 947, duration: 0.883s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.123 [-2.576, 1.538], mean_best_reward: --\n 17846/100000: episode: 948, duration: 1.013s, episode steps: 17, steps per second: 17, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.084 [-1.423, 2.341], mean_best_reward: --\n 17865/100000: episode: 949, duration: 1.491s, episode steps: 19, steps per second: 13, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.632 [0.000, 1.000], mean observation: -0.077 [-2.051, 1.181], mean_best_reward: --\n 17877/100000: episode: 950, duration: 1.288s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.120 [-1.741, 0.981], mean_best_reward: --\n 17899/100000: episode: 951, duration: 1.799s, episode steps: 22, steps per second: 12, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.093 [-1.033, 0.599], mean_best_reward: 55.000000\n 17917/100000: episode: 952, duration: 1.108s, episode steps: 18, steps per second: 16, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.066 [-1.476, 0.964], mean_best_reward: --\n 17927/100000: episode: 953, duration: 1.001s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.130 [-2.618, 1.616], mean_best_reward: --\n 17952/100000: episode: 954, duration: 2.602s, episode steps: 25, steps per second: 10, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.720 [0.000, 1.000], mean observation: -0.017 [-2.989, 2.147], mean_best_reward: --\n 17975/100000: episode: 955, duration: 1.884s, episode steps: 23, steps per second: 12, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: -0.009 [-1.398, 1.767], mean_best_reward: --\n 18014/100000: episode: 956, duration: 2.802s, episode steps: 39, steps per second: 14, episode reward: 39.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.487 [0.000, 1.000], mean observation: -0.144 [-1.149, 0.442], mean_best_reward: --\n 18024/100000: episode: 957, duration: 0.904s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.122 [-2.220, 1.384], mean_best_reward: --\n 18034/100000: episode: 958, duration: 1.108s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.126 [-1.675, 1.008], mean_best_reward: --\n 18051/100000: episode: 959, duration: 1.181s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.061 [-1.221, 0.811], mean_best_reward: --\n 18067/100000: episode: 960, duration: 1.114s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.093 [-2.102, 1.347], mean_best_reward: --\n 18088/100000: episode: 961, duration: 1.486s, episode steps: 21, steps per second: 14, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.090 [-2.366, 1.345], mean_best_reward: --\n 18099/100000: episode: 962, duration: 0.897s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.152 [-2.862, 1.721], mean_best_reward: --\n 18109/100000: episode: 963, duration: 1.007s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.159 [-2.036, 1.157], mean_best_reward: --\n 18121/100000: episode: 964, duration: 1.395s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.116 [-2.603, 1.592], mean_best_reward: --\n 18138/100000: episode: 965, duration: 1.502s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.077 [-1.760, 1.036], mean_best_reward: --\n 18154/100000: episode: 966, duration: 1.307s, episode steps: 16, steps per second: 12, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.089 [-1.347, 0.747], mean_best_reward: --\n 18169/100000: episode: 967, duration: 1.108s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.094 [-2.315, 1.402], mean_best_reward: --\n 18180/100000: episode: 968, duration: 0.981s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.116 [-2.449, 1.556], mean_best_reward: --\n 18222/100000: episode: 969, duration: 3.096s, episode steps: 42, steps per second: 14, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.054 [-0.868, 0.398], mean_best_reward: --\n 18241/100000: episode: 970, duration: 2.202s, episode steps: 19, steps per second: 9, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.107 [-2.483, 1.361], mean_best_reward: --\n 18251/100000: episode: 971, duration: 0.809s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.130 [-2.971, 1.945], mean_best_reward: --\n 18282/100000: episode: 972, duration: 1.610s, episode steps: 31, steps per second: 19, episode reward: 31.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.516 [0.000, 1.000], mean observation: -0.059 [-1.113, 0.738], mean_best_reward: --\n 18296/100000: episode: 973, duration: 0.995s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.113 [-2.041, 1.162], mean_best_reward: --\n 18310/100000: episode: 974, duration: 1.277s, episode steps: 14, steps per second: 11, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.100 [-2.556, 1.594], mean_best_reward: --\n 18322/100000: episode: 975, duration: 1.013s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.116 [-2.160, 1.389], mean_best_reward: --\n 18345/100000: episode: 976, duration: 1.800s, episode steps: 23, steps per second: 13, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.391 [0.000, 1.000], mean observation: 0.018 [-1.569, 2.088], mean_best_reward: --\n 18356/100000: episode: 977, duration: 0.900s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.149 [-2.277, 1.328], mean_best_reward: --\n 18367/100000: episode: 978, duration: 1.283s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.112 [-2.074, 1.396], mean_best_reward: --\n 18389/100000: episode: 979, duration: 1.419s, episode steps: 22, steps per second: 16, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.096 [-1.454, 0.587], mean_best_reward: --\n 18399/100000: episode: 980, duration: 0.683s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.149 [-3.082, 1.977], mean_best_reward: --\n 18409/100000: episode: 981, duration: 0.614s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.149 [-2.562, 1.528], mean_best_reward: --\n 18425/100000: episode: 982, duration: 1.382s, episode steps: 16, steps per second: 12, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-2.573, 1.524], mean_best_reward: --\n 18447/100000: episode: 983, duration: 1.507s, episode steps: 22, steps per second: 15, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.067 [-1.159, 0.812], mean_best_reward: --\n 18473/100000: episode: 984, duration: 2.611s, episode steps: 26, steps per second: 10, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.058 [-1.498, 0.839], mean_best_reward: --\n 18491/100000: episode: 985, duration: 1.179s, episode steps: 18, steps per second: 15, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.089 [-2.019, 1.130], mean_best_reward: --\n 18503/100000: episode: 986, duration: 0.919s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.112 [-1.820, 1.152], mean_best_reward: --\n 18515/100000: episode: 987, duration: 1.108s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-2.009, 1.199], mean_best_reward: --\n 18528/100000: episode: 988, duration: 1.092s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.110 [-2.381, 1.397], mean_best_reward: --\n 18549/100000: episode: 989, duration: 2.005s, episode steps: 21, steps per second: 10, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.109 [-0.987, 0.569], mean_best_reward: --\n 18567/100000: episode: 990, duration: 1.415s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.061 [-1.186, 1.999], mean_best_reward: --\n 18582/100000: episode: 991, duration: 1.378s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.052 [-2.170, 1.404], mean_best_reward: --\n 18608/100000: episode: 992, duration: 1.918s, episode steps: 26, steps per second: 14, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.069 [-1.687, 0.772], mean_best_reward: --\n 18627/100000: episode: 993, duration: 1.599s, episode steps: 19, steps per second: 12, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.076 [-2.436, 1.544], mean_best_reward: --\n 18638/100000: episode: 994, duration: 0.991s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.125 [-1.956, 1.144], mean_best_reward: --\n 18653/100000: episode: 995, duration: 1.892s, episode steps: 15, steps per second: 8, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.079 [-1.715, 0.961], mean_best_reward: --\n 18690/100000: episode: 996, duration: 2.912s, episode steps: 37, steps per second: 13, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.649 [0.000, 1.000], mean observation: -0.064 [-3.283, 2.106], mean_best_reward: --\n 18703/100000: episode: 997, duration: 0.882s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.101 [-2.785, 1.758], mean_best_reward: --\n 18715/100000: episode: 998, duration: 1.215s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.118 [-2.606, 1.581], mean_best_reward: --\n 18727/100000: episode: 999, duration: 1.082s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.123 [-1.540, 2.558], mean_best_reward: --\n 18746/100000: episode: 1000, duration: 1.819s, episode steps: 19, steps per second: 10, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.087 [-0.623, 1.323], mean_best_reward: --\n 18758/100000: episode: 1001, duration: 1.389s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.127 [-2.566, 1.571], mean_best_reward: 74.500000\n 18769/100000: episode: 1002, duration: 0.993s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.107 [-1.807, 2.794], mean_best_reward: --\n 18787/100000: episode: 1003, duration: 1.517s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.095 [-0.957, 1.746], mean_best_reward: --\n 18803/100000: episode: 1004, duration: 1.479s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.114 [-1.515, 0.771], mean_best_reward: --\n 18821/100000: episode: 1005, duration: 1.607s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.087 [-2.116, 1.200], mean_best_reward: --\n 18850/100000: episode: 1006, duration: 2.904s, episode steps: 29, steps per second: 10, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: 0.004 [-1.517, 2.034], mean_best_reward: --\n 18871/100000: episode: 1007, duration: 2.087s, episode steps: 21, steps per second: 10, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.069 [-1.329, 2.175], mean_best_reward: --\n 18881/100000: episode: 1008, duration: 0.817s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.102 [-1.615, 2.481], mean_best_reward: --\n 18895/100000: episode: 1009, duration: 0.784s, episode steps: 14, steps per second: 18, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.121 [-1.158, 0.558], mean_best_reward: --\n 18904/100000: episode: 1010, duration: 0.993s, episode steps: 9, steps per second: 9, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.159 [-1.329, 2.305], mean_best_reward: --\n 18922/100000: episode: 1011, duration: 1.899s, episode steps: 18, steps per second: 9, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.075 [-0.775, 1.517], mean_best_reward: --\n 18933/100000: episode: 1012, duration: 1.112s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.118 [-1.339, 2.187], mean_best_reward: --\n 18949/100000: episode: 1013, duration: 1.887s, episode steps: 16, steps per second: 8, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.074 [-1.707, 1.030], mean_best_reward: --\n 18958/100000: episode: 1014, duration: 0.815s, episode steps: 9, steps per second: 11, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.160 [-2.500, 1.562], mean_best_reward: --\n 18972/100000: episode: 1015, duration: 1.497s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.064 [-2.330, 1.556], mean_best_reward: --\n 18986/100000: episode: 1016, duration: 1.101s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.081 [-1.933, 1.170], mean_best_reward: --\n 18997/100000: episode: 1017, duration: 1.078s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.100 [-1.415, 2.271], mean_best_reward: --\n 19049/100000: episode: 1018, duration: 4.116s, episode steps: 52, steps per second: 13, episode reward: 52.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: 0.137 [-0.309, 0.973], mean_best_reward: --\n 19057/100000: episode: 1019, duration: 0.797s, episode steps: 8, steps per second: 10, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.144 [-1.590, 2.572], mean_best_reward: --\n 19069/100000: episode: 1020, duration: 1.086s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.085 [-1.219, 1.731], mean_best_reward: --\n 19081/100000: episode: 1021, duration: 0.713s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.118 [-2.354, 1.531], mean_best_reward: --\n 19092/100000: episode: 1022, duration: 0.991s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.120 [-1.410, 2.296], mean_best_reward: --\n 19100/100000: episode: 1023, duration: 0.615s, episode steps: 8, steps per second: 13, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.161 [-2.579, 1.595], mean_best_reward: --\n 19111/100000: episode: 1024, duration: 1.206s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.135 [-2.852, 1.736], mean_best_reward: --\n 19121/100000: episode: 1025, duration: 1.095s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.146 [-1.776, 2.738], mean_best_reward: --\n 19134/100000: episode: 1026, duration: 1.220s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.104 [-2.694, 1.720], mean_best_reward: --\n 19152/100000: episode: 1027, duration: 1.595s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.064 [-1.196, 1.994], mean_best_reward: --\n 19164/100000: episode: 1028, duration: 0.894s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-2.124, 1.219], mean_best_reward: --\n 19185/100000: episode: 1029, duration: 1.807s, episode steps: 21, steps per second: 12, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.054 [-0.957, 1.699], mean_best_reward: --\n 19194/100000: episode: 1030, duration: 0.703s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.889 [0.000, 1.000], mean observation: -0.178 [-2.546, 1.527], mean_best_reward: --\n 19204/100000: episode: 1031, duration: 0.999s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.700 [0.000, 1.000], mean observation: -0.139 [-1.950, 1.168], mean_best_reward: --\n 19218/100000: episode: 1032, duration: 1.082s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.105 [-0.986, 1.570], mean_best_reward: --\n 19230/100000: episode: 1033, duration: 0.999s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.101 [-2.095, 1.212], mean_best_reward: --\n 19240/100000: episode: 1034, duration: 0.793s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.142 [-0.965, 1.752], mean_best_reward: --\n 19249/100000: episode: 1035, duration: 0.701s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.135 [-1.781, 2.778], mean_best_reward: --\n 19260/100000: episode: 1036, duration: 1.301s, episode steps: 11, steps per second: 8, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.154 [-2.846, 1.741], mean_best_reward: --\n 19272/100000: episode: 1037, duration: 0.997s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.118 [-3.037, 1.963], mean_best_reward: --\n 19300/100000: episode: 1038, duration: 2.512s, episode steps: 28, steps per second: 11, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: -0.057 [-1.183, 0.580], mean_best_reward: --\n 19312/100000: episode: 1039, duration: 1.405s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.126 [-2.605, 1.534], mean_best_reward: --\n 19340/100000: episode: 1040, duration: 2.188s, episode steps: 28, steps per second: 13, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.607 [0.000, 1.000], mean observation: -0.036 [-1.928, 1.174], mean_best_reward: --\n 19352/100000: episode: 1041, duration: 1.097s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.127 [-1.148, 1.992], mean_best_reward: --\n 19366/100000: episode: 1042, duration: 1.510s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.113 [-1.154, 2.049], mean_best_reward: --\n 19376/100000: episode: 1043, duration: 0.702s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.149 [-2.066, 1.159], mean_best_reward: --\n 19385/100000: episode: 1044, duration: 1.197s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.136 [-1.395, 2.323], mean_best_reward: --\n 19399/100000: episode: 1045, duration: 1.300s, episode steps: 14, steps per second: 11, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.067 [-1.190, 1.803], mean_best_reward: --\n 19408/100000: episode: 1046, duration: 0.894s, episode steps: 9, steps per second: 10, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.121 [-2.158, 1.392], mean_best_reward: --\n 19416/100000: episode: 1047, duration: 0.488s, episode steps: 8, steps per second: 16, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.875 [0.000, 1.000], mean observation: -0.146 [-2.227, 1.404], mean_best_reward: --\n 19427/100000: episode: 1048, duration: 0.898s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.115 [-0.609, 1.255], mean_best_reward: --\n 19474/100000: episode: 1049, duration: 3.598s, episode steps: 47, steps per second: 13, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.511 [0.000, 1.000], mean observation: 0.044 [-1.209, 0.879], mean_best_reward: --\n 19504/100000: episode: 1050, duration: 2.722s, episode steps: 30, steps per second: 11, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.027 [-0.959, 1.662], mean_best_reward: --\n 19520/100000: episode: 1051, duration: 1.594s, episode steps: 16, steps per second: 10, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.812 [0.000, 1.000], mean observation: -0.062 [-3.026, 1.987], mean_best_reward: 61.500000\n 19563/100000: episode: 1052, duration: 3.801s, episode steps: 43, steps per second: 11, episode reward: 43.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: -0.052 [-1.014, 0.542], mean_best_reward: --\n 19580/100000: episode: 1053, duration: 1.192s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.294 [0.000, 1.000], mean observation: 0.089 [-1.527, 2.410], mean_best_reward: --\n 19600/100000: episode: 1054, duration: 1.597s, episode steps: 20, steps per second: 13, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.102 [-1.638, 0.778], mean_best_reward: --\n 19610/100000: episode: 1055, duration: 1.194s, episode steps: 10, steps per second: 8, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.131 [-1.897, 1.130], mean_best_reward: --\n 19636/100000: episode: 1056, duration: 2.397s, episode steps: 26, steps per second: 11, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.577 [0.000, 1.000], mean observation: -0.025 [-1.614, 1.152], mean_best_reward: --\n 19646/100000: episode: 1057, duration: 0.814s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.132 [-2.684, 1.787], mean_best_reward: --\n 19663/100000: episode: 1058, duration: 1.589s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.471 [0.000, 1.000], mean observation: 0.068 [-1.026, 1.564], mean_best_reward: --\n 19673/100000: episode: 1059, duration: 1.181s, episode steps: 10, steps per second: 8, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.112 [-2.040, 1.192], mean_best_reward: --\n 19691/100000: episode: 1060, duration: 1.505s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.722 [0.000, 1.000], mean observation: -0.035 [-2.379, 1.595], mean_best_reward: --\n 19701/100000: episode: 1061, duration: 0.709s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.152 [-2.249, 1.370], mean_best_reward: --\n 19723/100000: episode: 1062, duration: 1.887s, episode steps: 22, steps per second: 12, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.636 [0.000, 1.000], mean observation: -0.060 [-2.032, 1.198], mean_best_reward: --\n 19746/100000: episode: 1063, duration: 2.198s, episode steps: 23, steps per second: 10, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.056 [-1.371, 0.656], mean_best_reward: --\n 19768/100000: episode: 1064, duration: 1.703s, episode steps: 22, steps per second: 13, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.097 [-1.002, 0.555], mean_best_reward: --\n 19786/100000: episode: 1065, duration: 1.291s, episode steps: 18, steps per second: 14, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.044 [-1.824, 1.193], mean_best_reward: --\n 19805/100000: episode: 1066, duration: 1.304s, episode steps: 19, steps per second: 15, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.069 [-1.472, 0.831], mean_best_reward: --\n 19813/100000: episode: 1067, duration: 0.401s, episode steps: 8, steps per second: 20, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.134 [-2.514, 1.589], mean_best_reward: --\n 19825/100000: episode: 1068, duration: 0.993s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.112 [-1.993, 1.185], mean_best_reward: --\n 19845/100000: episode: 1069, duration: 1.613s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.450 [0.000, 1.000], mean observation: -0.107 [-0.943, 0.614], mean_best_reward: --\n 19860/100000: episode: 1070, duration: 1.487s, episode steps: 15, steps per second: 10, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.081 [-2.315, 1.383], mean_best_reward: --\n 19869/100000: episode: 1071, duration: 1.100s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.152 [-2.836, 1.740], mean_best_reward: --\n 19898/100000: episode: 1072, duration: 2.611s, episode steps: 29, steps per second: 11, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.089 [-0.445, 1.079], mean_best_reward: --\n 19914/100000: episode: 1073, duration: 1.295s, episode steps: 16, steps per second: 12, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.096 [-0.743, 1.347], mean_best_reward: --\n 19964/100000: episode: 1074, duration: 3.099s, episode steps: 50, steps per second: 16, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.480 [0.000, 1.000], mean observation: -0.068 [-0.997, 0.374], mean_best_reward: --\n 19973/100000: episode: 1075, duration: 1.099s, episode steps: 9, steps per second: 8, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.154 [-1.883, 1.149], mean_best_reward: --\n 19989/100000: episode: 1076, duration: 1.816s, episode steps: 16, steps per second: 9, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.080 [-1.445, 0.948], mean_best_reward: --\n 20000/100000: episode: 1077, duration: 0.804s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.141 [-1.542, 2.516], mean_best_reward: --\n 20038/100000: episode: 1078, duration: 2.878s, episode steps: 38, steps per second: 13, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.042 [-0.752, 0.424], mean_best_reward: --\n 20060/100000: episode: 1079, duration: 1.518s, episode steps: 22, steps per second: 14, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.043 [-1.458, 1.019], mean_best_reward: --\n 20089/100000: episode: 1080, duration: 2.789s, episode steps: 29, steps per second: 10, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.074 [-0.808, 0.390], mean_best_reward: --\n 20106/100000: episode: 1081, duration: 1.206s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.107 [-1.910, 0.954], mean_best_reward: --\n 20131/100000: episode: 1082, duration: 1.677s, episode steps: 25, steps per second: 15, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.046 [-0.631, 1.359], mean_best_reward: --\n 20172/100000: episode: 1083, duration: 3.617s, episode steps: 41, steps per second: 11, episode reward: 41.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.488 [0.000, 1.000], mean observation: 0.080 [-0.396, 1.192], mean_best_reward: --\n 20190/100000: episode: 1084, duration: 1.400s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.075 [-1.343, 0.769], mean_best_reward: --\n 20205/100000: episode: 1085, duration: 0.711s, episode steps: 15, steps per second: 21, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.090 [-1.463, 0.800], mean_best_reward: --\n 20233/100000: episode: 1086, duration: 2.588s, episode steps: 28, steps per second: 11, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.679 [0.000, 1.000], mean observation: -0.005 [-2.574, 1.911], mean_best_reward: --\n 20245/100000: episode: 1087, duration: 1.205s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.102 [-2.494, 1.560], mean_best_reward: --\n 20274/100000: episode: 1088, duration: 2.313s, episode steps: 29, steps per second: 13, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.379 [0.000, 1.000], mean observation: 0.046 [-1.389, 2.434], mean_best_reward: --\n 20286/100000: episode: 1089, duration: 0.978s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.114 [-2.059, 1.146], mean_best_reward: --\n 20298/100000: episode: 1090, duration: 1.302s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.099 [-1.226, 0.824], mean_best_reward: --\n 20326/100000: episode: 1091, duration: 1.915s, episode steps: 28, steps per second: 15, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: -0.068 [-0.985, 0.614], mean_best_reward: --\n 20383/100000: episode: 1092, duration: 4.280s, episode steps: 57, steps per second: 13, episode reward: 57.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.117 [-0.674, 1.980], mean_best_reward: --\n 20409/100000: episode: 1093, duration: 2.200s, episode steps: 26, steps per second: 12, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-0.921, 0.432], mean_best_reward: --\n 20432/100000: episode: 1094, duration: 1.496s, episode steps: 23, steps per second: 15, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.097 [-1.721, 0.795], mean_best_reward: --\n 20470/100000: episode: 1095, duration: 2.601s, episode steps: 38, steps per second: 15, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.038 [-0.727, 1.122], mean_best_reward: --\n 20486/100000: episode: 1096, duration: 1.401s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.083 [-1.285, 0.627], mean_best_reward: --\n 20496/100000: episode: 1097, duration: 0.906s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.123 [-1.415, 2.139], mean_best_reward: --\n 20515/100000: episode: 1098, duration: 1.482s, episode steps: 19, steps per second: 13, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.789 [0.000, 1.000], mean observation: -0.026 [-3.075, 2.159], mean_best_reward: --\n 20540/100000: episode: 1099, duration: 1.998s, episode steps: 25, steps per second: 13, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.109 [-0.954, 2.019], mean_best_reward: --\n 20553/100000: episode: 1100, duration: 1.010s, episode steps: 13, steps per second: 13, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.100 [-2.281, 1.398], mean_best_reward: --\n 20563/100000: episode: 1101, duration: 0.786s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.111 [-1.963, 1.193], mean_best_reward: 67.000000\n 20575/100000: episode: 1102, duration: 1.005s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.135 [-2.134, 1.175], mean_best_reward: --\n 20592/100000: episode: 1103, duration: 1.188s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.098 [-1.552, 0.758], mean_best_reward: --\n 20629/100000: episode: 1104, duration: 2.511s, episode steps: 37, steps per second: 15, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: -0.043 [-1.153, 0.736], mean_best_reward: --\n 20679/100000: episode: 1105, duration: 2.807s, episode steps: 50, steps per second: 18, episode reward: 50.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.540 [0.000, 1.000], mean observation: 0.012 [-1.191, 0.922], mean_best_reward: --\n 20697/100000: episode: 1106, duration: 1.090s, episode steps: 18, steps per second: 17, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.106 [-0.591, 1.016], mean_best_reward: --\n 20709/100000: episode: 1107, duration: 1.092s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.124 [-1.914, 3.075], mean_best_reward: --\n 20745/100000: episode: 1108, duration: 2.298s, episode steps: 36, steps per second: 16, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.002 [-2.910, 2.265], mean_best_reward: --\n 20758/100000: episode: 1109, duration: 1.204s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.115 [-1.801, 0.940], mean_best_reward: --\n 20819/100000: episode: 1110, duration: 4.515s, episode steps: 61, steps per second: 14, episode reward: 61.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.508 [0.000, 1.000], mean observation: -0.025 [-1.458, 0.791], mean_best_reward: --\n 20830/100000: episode: 1111, duration: 0.875s, episode steps: 11, steps per second: 13, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.103 [-1.302, 0.814], mean_best_reward: --\n 20843/100000: episode: 1112, duration: 1.122s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.104 [-1.353, 2.201], mean_best_reward: --\n 20857/100000: episode: 1113, duration: 1.276s, episode steps: 14, steps per second: 11, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.105 [-2.561, 1.559], mean_best_reward: --\n 20886/100000: episode: 1114, duration: 2.020s, episode steps: 29, steps per second: 14, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.690 [0.000, 1.000], mean observation: 0.018 [-2.820, 2.154], mean_best_reward: --\n 20899/100000: episode: 1115, duration: 1.295s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.091 [-2.321, 1.396], mean_best_reward: --\n 20953/100000: episode: 1116, duration: 4.401s, episode steps: 54, steps per second: 12, episode reward: 54.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.044 [-0.812, 0.968], mean_best_reward: --\n 20964/100000: episode: 1117, duration: 0.781s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.121 [-1.828, 1.030], mean_best_reward: --\n 20996/100000: episode: 1118, duration: 2.516s, episode steps: 32, steps per second: 13, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.060 [-0.754, 1.220], mean_best_reward: --\n 21022/100000: episode: 1119, duration: 1.387s, episode steps: 26, steps per second: 19, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.061 [-0.886, 0.594], mean_best_reward: --\n 21042/100000: episode: 1120, duration: 1.113s, episode steps: 20, steps per second: 18, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.072 [-1.133, 0.620], mean_best_reward: --\n 21054/100000: episode: 1121, duration: 1.083s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.122 [-1.550, 0.826], mean_best_reward: --\n 21078/100000: episode: 1122, duration: 2.094s, episode steps: 24, steps per second: 11, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.113 [-1.308, 0.781], mean_best_reward: --\n 21124/100000: episode: 1123, duration: 3.204s, episode steps: 46, steps per second: 14, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.070 [-0.398, 1.049], mean_best_reward: --\n 21137/100000: episode: 1124, duration: 1.112s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.118 [-1.171, 1.996], mean_best_reward: --\n 21151/100000: episode: 1125, duration: 0.984s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.090 [-1.969, 1.171], mean_best_reward: --\n 21175/100000: episode: 1126, duration: 1.806s, episode steps: 24, steps per second: 13, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.083 [-0.551, 1.017], mean_best_reward: --\n 21187/100000: episode: 1127, duration: 0.993s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.104 [-0.951, 1.641], mean_best_reward: --\n 21199/100000: episode: 1128, duration: 1.195s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.092 [-1.647, 1.015], mean_best_reward: --\n 21209/100000: episode: 1129, duration: 0.510s, episode steps: 10, steps per second: 20, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.136 [-2.734, 1.732], mean_best_reward: --\n 21218/100000: episode: 1130, duration: 0.587s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.143 [-1.394, 2.299], mean_best_reward: --\n 21233/100000: episode: 1131, duration: 1.315s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.100 [-0.823, 1.356], mean_best_reward: --\n 21261/100000: episode: 1132, duration: 1.498s, episode steps: 28, steps per second: 19, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.536 [0.000, 1.000], mean observation: 0.092 [-0.601, 0.941], mean_best_reward: --\n 21280/100000: episode: 1133, duration: 1.202s, episode steps: 19, steps per second: 16, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.044 [-2.566, 1.739], mean_best_reward: --\n 21293/100000: episode: 1134, duration: 0.804s, episode steps: 13, steps per second: 16, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.109 [-2.303, 1.352], mean_best_reward: --\n 21307/100000: episode: 1135, duration: 0.980s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.081 [-1.475, 1.016], mean_best_reward: --\n 21336/100000: episode: 1136, duration: 1.995s, episode steps: 29, steps per second: 15, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.552 [0.000, 1.000], mean observation: -0.070 [-1.644, 0.831], mean_best_reward: --\n 21350/100000: episode: 1137, duration: 0.999s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.107 [-1.164, 2.055], mean_best_reward: --\n 21368/100000: episode: 1138, duration: 1.520s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.074 [-0.771, 1.475], mean_best_reward: --\n 21417/100000: episode: 1139, duration: 3.590s, episode steps: 49, steps per second: 14, episode reward: 49.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.551 [0.000, 1.000], mean observation: 0.169 [-0.850, 1.202], mean_best_reward: --\n 21502/100000: episode: 1140, duration: 6.998s, episode steps: 85, steps per second: 12, episode reward: 85.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.518 [0.000, 1.000], mean observation: 0.062 [-0.758, 0.974], mean_best_reward: --\n 21513/100000: episode: 1141, duration: 0.900s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.145 [-2.807, 1.740], mean_best_reward: --\n 21531/100000: episode: 1142, duration: 0.990s, episode steps: 18, steps per second: 18, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.092 [-0.641, 1.364], mean_best_reward: --\n 21567/100000: episode: 1143, duration: 3.198s, episode steps: 36, steps per second: 11, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.049 [-0.732, 1.069], mean_best_reward: --\n 21579/100000: episode: 1144, duration: 0.994s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.106 [-2.001, 1.212], mean_best_reward: --\n 21603/100000: episode: 1145, duration: 1.898s, episode steps: 24, steps per second: 13, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: -0.078 [-1.488, 0.650], mean_best_reward: --\n 21648/100000: episode: 1146, duration: 2.691s, episode steps: 45, steps per second: 17, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.489 [0.000, 1.000], mean observation: 0.059 [-0.794, 1.575], mean_best_reward: --\n 21668/100000: episode: 1147, duration: 1.518s, episode steps: 20, steps per second: 13, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.051 [-1.185, 1.918], mean_best_reward: --\n 21684/100000: episode: 1148, duration: 0.987s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: -0.061 [-1.770, 1.150], mean_best_reward: --\n 21750/100000: episode: 1149, duration: 4.203s, episode steps: 66, steps per second: 16, episode reward: 66.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.621 [0.000, 1.000], mean observation: 0.104 [-3.481, 3.062], mean_best_reward: --\n 21758/100000: episode: 1150, duration: 0.791s, episode steps: 8, steps per second: 10, episode reward: 8.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.569, 1.543], mean_best_reward: --\n 21781/100000: episode: 1151, duration: 2.303s, episode steps: 23, steps per second: 10, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.348 [0.000, 1.000], mean observation: 0.061 [-1.369, 2.236], mean_best_reward: 74.500000\n 21801/100000: episode: 1152, duration: 1.299s, episode steps: 20, steps per second: 15, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.111 [-1.285, 0.571], mean_best_reward: --\n 21815/100000: episode: 1153, duration: 0.887s, episode steps: 14, steps per second: 16, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.079 [-1.218, 1.998], mean_best_reward: --\n 21834/100000: episode: 1154, duration: 1.619s, episode steps: 19, steps per second: 12, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.421 [0.000, 1.000], mean observation: 0.087 [-0.651, 1.507], mean_best_reward: --\n 21851/100000: episode: 1155, duration: 1.293s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.588 [0.000, 1.000], mean observation: -0.096 [-1.430, 0.768], mean_best_reward: --\n 21867/100000: episode: 1156, duration: 1.201s, episode steps: 16, steps per second: 13, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.688 [0.000, 1.000], mean observation: -0.089 [-2.040, 1.152], mean_best_reward: --\n 21880/100000: episode: 1157, duration: 1.119s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.114 [-1.334, 2.126], mean_best_reward: --\n 21895/100000: episode: 1158, duration: 1.094s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.093 [-2.246, 1.400], mean_best_reward: --\n 21906/100000: episode: 1159, duration: 1.286s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.121 [-2.782, 1.761], mean_best_reward: --\n 21933/100000: episode: 1160, duration: 2.391s, episode steps: 27, steps per second: 11, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.519 [0.000, 1.000], mean observation: -0.056 [-0.962, 0.540], mean_best_reward: --\n 21944/100000: episode: 1161, duration: 0.820s, episode steps: 11, steps per second: 13, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.127 [-2.725, 1.722], mean_best_reward: --\n 21957/100000: episode: 1162, duration: 1.397s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.085 [-2.176, 1.398], mean_best_reward: --\n 21968/100000: episode: 1163, duration: 1.291s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.131 [-1.362, 2.287], mean_best_reward: --\n 21979/100000: episode: 1164, duration: 0.999s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.125 [-1.790, 0.960], mean_best_reward: --\n 21990/100000: episode: 1165, duration: 0.999s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.125 [-2.821, 1.793], mean_best_reward: --\n 22005/100000: episode: 1166, duration: 0.811s, episode steps: 15, steps per second: 19, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.075 [-2.345, 1.411], mean_best_reward: --\n 22017/100000: episode: 1167, duration: 0.885s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.109 [-2.553, 1.568], mean_best_reward: --\n 22031/100000: episode: 1168, duration: 1.390s, episode steps: 14, steps per second: 10, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.078 [-2.428, 1.538], mean_best_reward: --\n 22040/100000: episode: 1169, duration: 0.610s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.000 [0.000, 0.000], mean observation: 0.146 [-1.758, 2.814], mean_best_reward: --\n 22050/100000: episode: 1170, duration: 0.805s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.104 [-2.147, 1.390], mean_best_reward: --\n 22069/100000: episode: 1171, duration: 1.708s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.737 [0.000, 1.000], mean observation: -0.051 [-2.653, 1.724], mean_best_reward: --\n 22080/100000: episode: 1172, duration: 1.005s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.123 [-1.787, 2.781], mean_best_reward: --\n 22093/100000: episode: 1173, duration: 1.189s, episode steps: 13, steps per second: 11, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.100 [-1.419, 0.797], mean_best_reward: --\n 22114/100000: episode: 1174, duration: 1.514s, episode steps: 21, steps per second: 14, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.059 [-1.533, 0.813], mean_best_reward: --\n 22133/100000: episode: 1175, duration: 1.191s, episode steps: 19, steps per second: 16, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.063 [-2.211, 1.375], mean_best_reward: --\n 22145/100000: episode: 1176, duration: 0.902s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.092 [-2.979, 1.943], mean_best_reward: --\n 22162/100000: episode: 1177, duration: 0.993s, episode steps: 17, steps per second: 17, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.061 [-2.191, 1.374], mean_best_reward: --\n 22175/100000: episode: 1178, duration: 0.913s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.066 [-1.418, 2.178], mean_best_reward: --\n 22190/100000: episode: 1179, duration: 1.086s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.079 [-1.020, 1.607], mean_best_reward: --\n 22200/100000: episode: 1180, duration: 0.794s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.163 [-1.153, 2.114], mean_best_reward: --\n 22214/100000: episode: 1181, duration: 1.203s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.286 [0.000, 1.000], mean observation: 0.095 [-1.183, 2.028], mean_best_reward: --\n 22229/100000: episode: 1182, duration: 1.581s, episode steps: 15, steps per second: 9, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.114 [-1.024, 0.596], mean_best_reward: --\n 22265/100000: episode: 1183, duration: 2.800s, episode steps: 36, steps per second: 13, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.046 [-0.610, 1.120], mean_best_reward: --\n 22276/100000: episode: 1184, duration: 0.997s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.364 [0.000, 1.000], mean observation: 0.105 [-0.987, 1.602], mean_best_reward: --\n 22290/100000: episode: 1185, duration: 1.016s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.077 [-2.485, 1.570], mean_best_reward: --\n 22307/100000: episode: 1186, duration: 1.507s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.062 [-1.156, 1.875], mean_best_reward: --\n 22324/100000: episode: 1187, duration: 1.602s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.765 [0.000, 1.000], mean observation: -0.083 [-2.878, 1.769], mean_best_reward: --\n 22335/100000: episode: 1188, duration: 0.698s, episode steps: 11, steps per second: 16, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.140 [-2.293, 1.327], mean_best_reward: --\n 22373/100000: episode: 1189, duration: 2.894s, episode steps: 38, steps per second: 13, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.474 [0.000, 1.000], mean observation: 0.051 [-0.592, 1.169], mean_best_reward: --\n 22391/100000: episode: 1190, duration: 1.305s, episode steps: 18, steps per second: 14, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.088 [-0.941, 1.684], mean_best_reward: --\n 22404/100000: episode: 1191, duration: 0.889s, episode steps: 13, steps per second: 15, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.092 [-2.265, 1.405], mean_best_reward: --\n 22416/100000: episode: 1192, duration: 1.099s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.090 [-1.805, 1.195], mean_best_reward: --\n 22429/100000: episode: 1193, duration: 0.914s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.125 [-1.333, 2.361], mean_best_reward: --\n 22450/100000: episode: 1194, duration: 1.496s, episode steps: 21, steps per second: 14, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.058 [-2.328, 1.407], mean_best_reward: --\n 22461/100000: episode: 1195, duration: 0.992s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.098 [-2.219, 1.391], mean_best_reward: --\n 22488/100000: episode: 1196, duration: 2.098s, episode steps: 27, steps per second: 13, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.630 [0.000, 1.000], mean observation: -0.051 [-2.296, 1.406], mean_best_reward: --\n 22533/100000: episode: 1197, duration: 2.995s, episode steps: 45, steps per second: 15, episode reward: 45.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.467 [0.000, 1.000], mean observation: -0.134 [-1.107, 0.781], mean_best_reward: --\n 22543/100000: episode: 1198, duration: 0.799s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.109 [-2.968, 1.974], mean_best_reward: --\n 22553/100000: episode: 1199, duration: 0.812s, episode steps: 10, steps per second: 12, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.149 [-2.588, 1.524], mean_best_reward: --\n 22566/100000: episode: 1200, duration: 1.403s, episode steps: 13, steps per second: 9, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.113 [-2.036, 1.145], mean_best_reward: --\n 22583/100000: episode: 1201, duration: 1.890s, episode steps: 17, steps per second: 9, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.093 [-1.945, 1.144], mean_best_reward: 57.000000\n 22598/100000: episode: 1202, duration: 1.006s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.078 [-1.357, 2.037], mean_best_reward: --\n 22627/100000: episode: 1203, duration: 2.195s, episode steps: 29, steps per second: 13, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.414 [0.000, 1.000], mean observation: 0.054 [-1.019, 2.041], mean_best_reward: --\n 22642/100000: episode: 1204, duration: 1.396s, episode steps: 15, steps per second: 11, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.077 [-2.784, 1.776], mean_best_reward: --\n 22658/100000: episode: 1205, duration: 1.487s, episode steps: 16, steps per second: 11, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.375 [0.000, 1.000], mean observation: 0.121 [-0.759, 1.719], mean_best_reward: --\n 22672/100000: episode: 1206, duration: 0.908s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.115 [-1.536, 2.600], mean_best_reward: --\n 22690/100000: episode: 1207, duration: 1.389s, episode steps: 18, steps per second: 13, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.389 [0.000, 1.000], mean observation: 0.087 [-0.757, 1.536], mean_best_reward: --\n 22718/100000: episode: 1208, duration: 2.310s, episode steps: 28, steps per second: 12, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.321 [0.000, 1.000], mean observation: -0.044 [-2.143, 2.556], mean_best_reward: --\n 22729/100000: episode: 1209, duration: 0.989s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.142 [-1.342, 2.260], mean_best_reward: --\n 22740/100000: episode: 1210, duration: 0.915s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.149 [-1.757, 2.821], mean_best_reward: --\n 22752/100000: episode: 1211, duration: 0.886s, episode steps: 12, steps per second: 14, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.583 [0.000, 1.000], mean observation: -0.104 [-1.634, 1.014], mean_best_reward: --\n 22764/100000: episode: 1212, duration: 0.696s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.141 [-1.546, 2.570], mean_best_reward: --\n 22789/100000: episode: 1213, duration: 1.503s, episode steps: 25, steps per second: 17, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.087 [-0.563, 1.394], mean_best_reward: --\n 22800/100000: episode: 1214, duration: 0.600s, episode steps: 11, steps per second: 18, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.111 [-1.400, 2.213], mean_best_reward: --\n 22820/100000: episode: 1215, duration: 1.597s, episode steps: 20, steps per second: 13, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.062 [-0.975, 1.613], mean_best_reward: --\n 22831/100000: episode: 1216, duration: 1.015s, episode steps: 11, steps per second: 11, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.909 [0.000, 1.000], mean observation: -0.156 [-2.904, 1.735], mean_best_reward: --\n 22841/100000: episode: 1217, duration: 0.899s, episode steps: 10, steps per second: 11, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.113 [-2.664, 1.760], mean_best_reward: --\n 22854/100000: episode: 1218, duration: 1.304s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.308 [0.000, 1.000], mean observation: 0.093 [-1.196, 1.902], mean_best_reward: --\n 22869/100000: episode: 1219, duration: 1.112s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.400 [0.000, 1.000], mean observation: 0.095 [-0.590, 1.306], mean_best_reward: --\n 22925/100000: episode: 1220, duration: 3.690s, episode steps: 56, steps per second: 15, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.014 [-0.889, 0.728], mean_best_reward: --\n 22965/100000: episode: 1221, duration: 3.304s, episode steps: 40, steps per second: 12, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.625 [0.000, 1.000], mean observation: 0.078 [-2.236, 1.911], mean_best_reward: --\n 22989/100000: episode: 1222, duration: 2.003s, episode steps: 24, steps per second: 12, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: 0.056 [-0.830, 1.215], mean_best_reward: --\n 23003/100000: episode: 1223, duration: 1.200s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.098 [-1.558, 2.494], mean_best_reward: --\n 23075/100000: episode: 1224, duration: 4.886s, episode steps: 72, steps per second: 15, episode reward: 72.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.458 [0.000, 1.000], mean observation: -0.100 [-1.056, 0.506], mean_best_reward: --\n 23103/100000: episode: 1225, duration: 2.396s, episode steps: 28, steps per second: 12, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.464 [0.000, 1.000], mean observation: 0.073 [-0.648, 1.522], mean_best_reward: --\n 23115/100000: episode: 1226, duration: 0.824s, episode steps: 12, steps per second: 15, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.667 [0.000, 1.000], mean observation: -0.132 [-1.765, 1.001], mean_best_reward: --\n 23127/100000: episode: 1227, duration: 1.000s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.126 [-1.343, 2.171], mean_best_reward: --\n 23142/100000: episode: 1228, duration: 1.102s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.100 [-2.328, 1.373], mean_best_reward: --\n 23156/100000: episode: 1229, duration: 1.208s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.714 [0.000, 1.000], mean observation: -0.096 [-2.044, 1.161], mean_best_reward: --\n 23165/100000: episode: 1230, duration: 0.687s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.143 [-1.343, 2.307], mean_best_reward: --\n 23187/100000: episode: 1231, duration: 1.319s, episode steps: 22, steps per second: 17, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.015 [-1.758, 2.441], mean_best_reward: --\n 23196/100000: episode: 1232, duration: 0.592s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.134 [-1.613, 2.444], mean_best_reward: --\n 23210/100000: episode: 1233, duration: 0.906s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.643 [0.000, 1.000], mean observation: -0.092 [-1.672, 0.968], mean_best_reward: --\n 23220/100000: episode: 1234, duration: 0.795s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.162 [-2.060, 1.135], mean_best_reward: --\n 23237/100000: episode: 1235, duration: 1.687s, episode steps: 17, steps per second: 10, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.706 [0.000, 1.000], mean observation: -0.081 [-2.311, 1.394], mean_best_reward: --\n 23251/100000: episode: 1236, duration: 1.311s, episode steps: 14, steps per second: 11, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.103 [-0.755, 1.555], mean_best_reward: --\n 23262/100000: episode: 1237, duration: 1.296s, episode steps: 11, steps per second: 8, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.113 [-1.356, 2.196], mean_best_reward: --\n 23288/100000: episode: 1238, duration: 1.992s, episode steps: 26, steps per second: 13, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: 0.111 [-0.817, 1.262], mean_best_reward: --\n 23299/100000: episode: 1239, duration: 0.902s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.151 [-1.357, 2.270], mean_best_reward: --\n 23327/100000: episode: 1240, duration: 2.505s, episode steps: 28, steps per second: 11, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.321 [0.000, 1.000], mean observation: 0.031 [-1.940, 2.935], mean_best_reward: --\n 23337/100000: episode: 1241, duration: 0.708s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.135 [-3.074, 1.955], mean_best_reward: --\n 23352/100000: episode: 1242, duration: 1.194s, episode steps: 15, steps per second: 13, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.107 [-0.952, 1.803], mean_best_reward: --\n 23439/100000: episode: 1243, duration: 6.115s, episode steps: 87, steps per second: 14, episode reward: 87.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.045 [-1.179, 1.418], mean_best_reward: --\n 23449/100000: episode: 1244, duration: 0.616s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.300 [0.000, 1.000], mean observation: 0.104 [-1.022, 1.642], mean_best_reward: --\n 23458/100000: episode: 1245, duration: 0.686s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.778 [0.000, 1.000], mean observation: -0.140 [-2.161, 1.403], mean_best_reward: --\n 23492/100000: episode: 1246, duration: 2.814s, episode steps: 34, steps per second: 12, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.070 [-0.963, 0.436], mean_best_reward: --\n 23511/100000: episode: 1247, duration: 1.397s, episode steps: 19, steps per second: 14, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.063 [-1.348, 0.826], mean_best_reward: --\n 23528/100000: episode: 1248, duration: 0.999s, episode steps: 17, steps per second: 17, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.067 [-1.917, 1.220], mean_best_reward: --\n 23544/100000: episode: 1249, duration: 1.086s, episode steps: 16, steps per second: 15, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.110 [-0.563, 0.970], mean_best_reward: --\n 23581/100000: episode: 1250, duration: 2.512s, episode steps: 37, steps per second: 15, episode reward: 37.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.127 [-0.606, 1.289], mean_best_reward: --\n 23595/100000: episode: 1251, duration: 1.282s, episode steps: 14, steps per second: 11, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.102 [-1.612, 2.694], mean_best_reward: 38.000000\n 23655/100000: episode: 1252, duration: 4.211s, episode steps: 60, steps per second: 14, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.517 [0.000, 1.000], mean observation: 0.085 [-0.761, 0.803], mean_best_reward: --\n 23701/100000: episode: 1253, duration: 3.591s, episode steps: 46, steps per second: 13, episode reward: 46.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.478 [0.000, 1.000], mean observation: 0.018 [-0.926, 1.157], mean_best_reward: --\n 23724/100000: episode: 1254, duration: 1.398s, episode steps: 23, steps per second: 16, episode reward: 23.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.565 [0.000, 1.000], mean observation: -0.042 [-1.716, 1.210], mean_best_reward: --\n 23735/100000: episode: 1255, duration: 1.201s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.097 [-1.370, 2.166], mean_best_reward: --\n 23762/100000: episode: 1256, duration: 2.402s, episode steps: 27, steps per second: 11, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: 0.063 [-0.775, 1.111], mean_best_reward: --\n 23776/100000: episode: 1257, duration: 0.992s, episode steps: 14, steps per second: 14, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.111 [-0.787, 1.544], mean_best_reward: --\n 23793/100000: episode: 1258, duration: 1.209s, episode steps: 17, steps per second: 14, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.092 [-0.800, 1.281], mean_best_reward: --\n 23814/100000: episode: 1259, duration: 1.790s, episode steps: 21, steps per second: 12, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.381 [0.000, 1.000], mean observation: 0.077 [-1.021, 1.962], mean_best_reward: --\n 23824/100000: episode: 1260, duration: 0.599s, episode steps: 10, steps per second: 17, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.141 [-1.600, 2.568], mean_best_reward: --\n 23837/100000: episode: 1261, duration: 1.118s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.769 [0.000, 1.000], mean observation: -0.123 [-2.334, 1.333], mean_best_reward: --\n 23851/100000: episode: 1262, duration: 1.201s, episode steps: 14, steps per second: 12, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.089 [-0.949, 1.488], mean_best_reward: --\n 23867/100000: episode: 1263, duration: 1.003s, episode steps: 16, steps per second: 16, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.087 [-0.609, 1.146], mean_best_reward: --\n 23876/100000: episode: 1264, duration: 0.615s, episode steps: 9, steps per second: 15, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.153 [-1.568, 2.478], mean_best_reward: --\n 23901/100000: episode: 1265, duration: 2.083s, episode steps: 25, steps per second: 12, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.440 [0.000, 1.000], mean observation: 0.060 [-0.736, 1.213], mean_best_reward: --\n 23921/100000: episode: 1266, duration: 1.299s, episode steps: 20, steps per second: 15, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.111 [-0.604, 1.078], mean_best_reward: --\n 23935/100000: episode: 1267, duration: 0.913s, episode steps: 14, steps per second: 15, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.786 [0.000, 1.000], mean observation: -0.093 [-2.474, 1.585], mean_best_reward: --\n 23964/100000: episode: 1268, duration: 2.101s, episode steps: 29, steps per second: 14, episode reward: 29.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: 0.066 [-0.569, 1.122], mean_best_reward: --\n 23973/100000: episode: 1269, duration: 0.699s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.111 [0.000, 1.000], mean observation: 0.146 [-1.572, 2.538], mean_best_reward: --\n 23985/100000: episode: 1270, duration: 0.980s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.102 [-1.024, 1.763], mean_best_reward: --\n 23997/100000: episode: 1271, duration: 0.703s, episode steps: 12, steps per second: 17, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.109 [-1.602, 2.553], mean_best_reward: --\n 24018/100000: episode: 1272, duration: 1.113s, episode steps: 21, steps per second: 19, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.082 [-0.750, 1.230], mean_best_reward: --\n 24028/100000: episode: 1273, duration: 0.703s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.115 [-2.025, 1.225], mean_best_reward: --\n 24038/100000: episode: 1274, duration: 0.608s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.143 [-2.473, 1.545], mean_best_reward: --\n 24051/100000: episode: 1275, duration: 0.908s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.385 [0.000, 1.000], mean observation: 0.105 [-0.999, 1.445], mean_best_reward: --\n 24069/100000: episode: 1276, duration: 1.605s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.444 [0.000, 1.000], mean observation: 0.073 [-0.604, 1.242], mean_best_reward: --\n 24099/100000: episode: 1277, duration: 1.782s, episode steps: 30, steps per second: 17, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.433 [0.000, 1.000], mean observation: 0.022 [-1.026, 1.653], mean_best_reward: --\n 24123/100000: episode: 1278, duration: 1.499s, episode steps: 24, steps per second: 16, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.102 [-1.015, 0.385], mean_best_reward: --\n 24140/100000: episode: 1279, duration: 1.596s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.353 [0.000, 1.000], mean observation: 0.087 [-0.972, 1.762], mean_best_reward: --\n 24155/100000: episode: 1280, duration: 1.101s, episode steps: 15, steps per second: 14, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.058 [-1.774, 2.697], mean_best_reward: --\n 24165/100000: episode: 1281, duration: 0.689s, episode steps: 10, steps per second: 15, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.124 [-1.155, 1.994], mean_best_reward: --\n 24175/100000: episode: 1282, duration: 0.622s, episode steps: 10, steps per second: 16, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.127 [-1.192, 2.088], mean_best_reward: --\n 24191/100000: episode: 1283, duration: 1.299s, episode steps: 16, steps per second: 12, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.118 [-0.625, 1.373], mean_best_reward: --\n 24203/100000: episode: 1284, duration: 0.890s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.135 [-2.046, 1.156], mean_best_reward: --\n 24227/100000: episode: 1285, duration: 1.904s, episode steps: 24, steps per second: 13, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.058 [-0.585, 1.004], mean_best_reward: --\n 24236/100000: episode: 1286, duration: 0.697s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.160 [-2.872, 1.754], mean_best_reward: --\n 24246/100000: episode: 1287, duration: 1.080s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.126 [-2.460, 1.541], mean_best_reward: --\n 24278/100000: episode: 1288, duration: 2.898s, episode steps: 32, steps per second: 11, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.469 [0.000, 1.000], mean observation: 0.023 [-0.860, 1.188], mean_best_reward: --\n 24308/100000: episode: 1289, duration: 1.700s, episode steps: 30, steps per second: 18, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: 0.001 [-1.959, 1.381], mean_best_reward: --\n 24319/100000: episode: 1290, duration: 0.799s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.102 [-1.403, 2.190], mean_best_reward: --\n 24330/100000: episode: 1291, duration: 1.099s, episode steps: 11, steps per second: 10, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.132 [-2.295, 1.344], mean_best_reward: --\n 24350/100000: episode: 1292, duration: 1.606s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.350 [0.000, 1.000], mean observation: 0.066 [-1.200, 1.995], mean_best_reward: --\n 24360/100000: episode: 1293, duration: 1.102s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.200 [0.000, 1.000], mean observation: 0.118 [-1.225, 1.967], mean_best_reward: --\n 24372/100000: episode: 1294, duration: 0.908s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.095 [-1.606, 2.557], mean_best_reward: --\n 24436/100000: episode: 1295, duration: 4.615s, episode steps: 64, steps per second: 14, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.143 [-1.395, 0.734], mean_best_reward: --\n 24474/100000: episode: 1296, duration: 3.097s, episode steps: 38, steps per second: 12, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.658 [0.000, 1.000], mean observation: 0.031 [-2.828, 2.299], mean_best_reward: --\n 24487/100000: episode: 1297, duration: 1.300s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.231 [0.000, 1.000], mean observation: 0.085 [-1.377, 2.246], mean_best_reward: --\n 24501/100000: episode: 1298, duration: 1.397s, episode steps: 14, steps per second: 10, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.117 [-0.966, 1.779], mean_best_reward: --\n 24512/100000: episode: 1299, duration: 0.891s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.273 [0.000, 1.000], mean observation: 0.107 [-1.413, 2.072], mean_best_reward: --\n 24542/100000: episode: 1300, duration: 2.395s, episode steps: 30, steps per second: 13, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.091 [-0.743, 1.272], mean_best_reward: --\n 24561/100000: episode: 1301, duration: 1.690s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.579 [0.000, 1.000], mean observation: -0.096 [-1.435, 0.590], mean_best_reward: 74.500000\n 24617/100000: episode: 1302, duration: 4.299s, episode steps: 56, steps per second: 13, episode reward: 56.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.017 [-2.521, 1.578], mean_best_reward: --\n 24630/100000: episode: 1303, duration: 1.107s, episode steps: 13, steps per second: 12, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.102 [-1.366, 0.803], mean_best_reward: --\n 24642/100000: episode: 1304, duration: 1.192s, episode steps: 12, steps per second: 10, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.750 [0.000, 1.000], mean observation: -0.094 [-1.948, 1.214], mean_best_reward: --\n 24660/100000: episode: 1305, duration: 1.707s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.222 [0.000, 1.000], mean observation: 0.053 [-1.977, 2.967], mean_best_reward: --\n 24692/100000: episode: 1306, duration: 2.913s, episode steps: 32, steps per second: 11, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.438 [0.000, 1.000], mean observation: 0.062 [-1.016, 1.947], mean_best_reward: --\n 24716/100000: episode: 1307, duration: 2.421s, episode steps: 24, steps per second: 10, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.708 [0.000, 1.000], mean observation: -0.039 [-2.886, 1.898], mean_best_reward: --\n 24737/100000: episode: 1308, duration: 1.904s, episode steps: 21, steps per second: 11, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.068 [-0.930, 1.398], mean_best_reward: --\n 24754/100000: episode: 1309, duration: 1.711s, episode steps: 17, steps per second: 10, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.065 [-1.688, 1.183], mean_best_reward: --\n 24770/100000: episode: 1310, duration: 1.303s, episode steps: 16, steps per second: 12, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.250 [0.000, 1.000], mean observation: 0.100 [-1.593, 2.648], mean_best_reward: --\n 24789/100000: episode: 1311, duration: 1.292s, episode steps: 19, steps per second: 15, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.078 [-2.327, 1.347], mean_best_reward: --\n 24802/100000: episode: 1312, duration: 0.901s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.615 [0.000, 1.000], mean observation: -0.098 [-1.363, 0.796], mean_best_reward: --\n 24830/100000: episode: 1313, duration: 1.382s, episode steps: 28, steps per second: 20, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.679 [0.000, 1.000], mean observation: 0.020 [-2.746, 1.986], mean_best_reward: --\n 24840/100000: episode: 1314, duration: 0.714s, episode steps: 10, steps per second: 14, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.800 [0.000, 1.000], mean observation: -0.148 [-2.088, 1.192], mean_best_reward: --\n 24861/100000: episode: 1315, duration: 1.390s, episode steps: 21, steps per second: 15, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.429 [0.000, 1.000], mean observation: 0.087 [-0.602, 1.325], mean_best_reward: --\n 24872/100000: episode: 1316, duration: 0.898s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.120 [-1.767, 2.786], mean_best_reward: --\n 24884/100000: episode: 1317, duration: 1.002s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.083 [0.000, 1.000], mean observation: 0.100 [-1.950, 3.014], mean_best_reward: --\n 24908/100000: episode: 1318, duration: 1.604s, episode steps: 24, steps per second: 15, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.542 [0.000, 1.000], mean observation: 0.083 [-0.623, 1.097], mean_best_reward: --\n 24932/100000: episode: 1319, duration: 2.186s, episode steps: 24, steps per second: 11, episode reward: 24.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.076 [-0.753, 1.081], mean_best_reward: --\n 24942/100000: episode: 1320, duration: 1.009s, episode steps: 10, steps per second: 10, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.123 [-1.587, 2.574], mean_best_reward: --\n 25043/100000: episode: 1321, duration: 8.105s, episode steps: 101, steps per second: 12, episode reward: 101.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: 0.310 [-1.133, 1.896], mean_best_reward: --\n 25053/100000: episode: 1322, duration: 1.097s, episode steps: 10, steps per second: 9, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.100 [0.000, 1.000], mean observation: 0.134 [-1.552, 2.599], mean_best_reward: --\n 25065/100000: episode: 1323, duration: 1.082s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.917 [0.000, 1.000], mean observation: -0.096 [-3.024, 1.996], mean_best_reward: --\n 25079/100000: episode: 1324, duration: 1.117s, episode steps: 14, steps per second: 13, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.357 [0.000, 1.000], mean observation: 0.112 [-1.149, 1.894], mean_best_reward: --\n 25099/100000: episode: 1325, duration: 1.995s, episode steps: 20, steps per second: 10, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-1.017, 0.586], mean_best_reward: --\n 25111/100000: episode: 1326, duration: 1.085s, episode steps: 12, steps per second: 11, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.116 [-0.775, 1.258], mean_best_reward: --\n 25129/100000: episode: 1327, duration: 1.502s, episode steps: 18, steps per second: 12, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.611 [0.000, 1.000], mean observation: -0.083 [-1.837, 1.023], mean_best_reward: --\n 25165/100000: episode: 1328, duration: 3.096s, episode steps: 36, steps per second: 12, episode reward: 36.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.556 [0.000, 1.000], mean observation: -0.053 [-1.771, 1.124], mean_best_reward: --\n 25178/100000: episode: 1329, duration: 0.898s, episode steps: 13, steps per second: 14, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.538 [0.000, 1.000], mean observation: -0.123 [-1.117, 0.579], mean_best_reward: --\n 25193/100000: episode: 1330, duration: 1.208s, episode steps: 15, steps per second: 12, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.733 [0.000, 1.000], mean observation: -0.099 [-2.241, 1.377], mean_best_reward: --\n 25225/100000: episode: 1331, duration: 2.311s, episode steps: 32, steps per second: 14, episode reward: 32.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.099 [-0.890, 0.367], mean_best_reward: --\n 25236/100000: episode: 1332, duration: 0.897s, episode steps: 11, steps per second: 12, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.182 [0.000, 1.000], mean observation: 0.112 [-1.606, 2.500], mean_best_reward: --\n 25263/100000: episode: 1333, duration: 2.199s, episode steps: 27, steps per second: 12, episode reward: 27.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.593 [0.000, 1.000], mean observation: -0.052 [-1.836, 0.977], mean_best_reward: --\n 25273/100000: episode: 1334, duration: 0.793s, episode steps: 10, steps per second: 13, episode reward: 10.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.900 [0.000, 1.000], mean observation: -0.137 [-2.589, 1.584], mean_best_reward: --\n 25412/100000: episode: 1335, duration: 10.099s, episode steps: 139, steps per second: 14, episode reward: 139.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.482 [0.000, 1.000], mean observation: -0.031 [-1.262, 0.972], mean_best_reward: --\n 25421/100000: episode: 1336, duration: 0.705s, episode steps: 9, steps per second: 13, episode reward: 9.000, mean reward: 1.000 [1.000, 1.000], mean action: 1.000 [1.000, 1.000], mean observation: -0.167 [-2.847, 1.725], mean_best_reward: --\n 25438/100000: episode: 1337, duration: 1.291s, episode steps: 17, steps per second: 13, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.412 [0.000, 1.000], mean observation: 0.087 [-0.977, 1.635], mean_best_reward: --\n 25453/100000: episode: 1338, duration: 0.991s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.111 [-1.135, 1.979], mean_best_reward: --\n 25472/100000: episode: 1339, duration: 1.697s, episode steps: 19, steps per second: 11, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.316 [0.000, 1.000], mean observation: 0.037 [-1.535, 2.252], mean_best_reward: --\n 25487/100000: episode: 1340, duration: 1.599s, episode steps: 15, steps per second: 9, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.533 [0.000, 1.000], mean observation: -0.112 [-1.200, 0.785], mean_best_reward: --\n 25500/100000: episode: 1341, duration: 1.311s, episode steps: 13, steps per second: 10, episode reward: 13.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.846 [0.000, 1.000], mean observation: -0.080 [-2.772, 1.805], mean_best_reward: --\n 25518/100000: episode: 1342, duration: 1.583s, episode steps: 18, steps per second: 11, episode reward: 18.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.278 [0.000, 1.000], mean observation: 0.058 [-1.580, 2.507], mean_best_reward: --\n 25540/100000: episode: 1343, duration: 2.006s, episode steps: 22, steps per second: 11, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.318 [0.000, 1.000], mean observation: 0.076 [-1.589, 2.698], mean_best_reward: --\n 25562/100000: episode: 1344, duration: 1.705s, episode steps: 22, steps per second: 13, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.455 [0.000, 1.000], mean observation: -0.122 [-0.878, 0.542], mean_best_reward: --\n 25576/100000: episode: 1345, duration: 1.391s, episode steps: 14, steps per second: 10, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.214 [0.000, 1.000], mean observation: 0.086 [-1.562, 2.486], mean_best_reward: --\n 25616/100000: episode: 1346, duration: 2.703s, episode steps: 40, steps per second: 15, episode reward: 40.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.525 [0.000, 1.000], mean observation: 0.062 [-0.621, 0.830], mean_best_reward: --\n 25627/100000: episode: 1347, duration: 1.203s, episode steps: 11, steps per second: 9, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.091 [0.000, 1.000], mean observation: 0.131 [-1.789, 2.792], mean_best_reward: --\n 25644/100000: episode: 1348, duration: 1.607s, episode steps: 17, steps per second: 11, episode reward: 17.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.647 [0.000, 1.000], mean observation: -0.059 [-1.770, 1.019], mean_best_reward: --\n 25678/100000: episode: 1349, duration: 2.882s, episode steps: 34, steps per second: 12, episode reward: 34.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.441 [0.000, 1.000], mean observation: -0.072 [-1.360, 1.329], mean_best_reward: --\n 25690/100000: episode: 1350, duration: 0.911s, episode steps: 12, steps per second: 13, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.128 [-2.601, 1.553], mean_best_reward: --\n 25711/100000: episode: 1351, duration: 1.489s, episode steps: 21, steps per second: 14, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.333 [0.000, 1.000], mean observation: 0.071 [-1.376, 2.276], mean_best_reward: 72.000000\n 25723/100000: episode: 1352, duration: 1.014s, episode steps: 12, steps per second: 12, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.167 [0.000, 1.000], mean observation: 0.118 [-1.765, 2.733], mean_best_reward: --\n 25761/100000: episode: 1353, duration: 3.601s, episode steps: 38, steps per second: 11, episode reward: 38.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.161 [-0.881, 0.505], mean_best_reward: --\n 25789/100000: episode: 1354, duration: 2.484s, episode steps: 28, steps per second: 11, episode reward: 28.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.064 [-1.594, 0.815], mean_best_reward: --\n 25837/100000: episode: 1355, duration: 3.994s, episode steps: 48, steps per second: 12, episode reward: 48.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.077 [-0.403, 1.100], mean_best_reward: --\n 25879/100000: episode: 1356, duration: 3.795s, episode steps: 42, steps per second: 11, episode reward: 42.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: 0.050 [-0.799, 1.088], mean_best_reward: --\n 25891/100000: episode: 1357, duration: 1.411s, episode steps: 12, steps per second: 9, episode reward: 12.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.833 [0.000, 1.000], mean observation: -0.099 [-2.449, 1.589], mean_best_reward: --\n 25916/100000: episode: 1358, duration: 1.796s, episode steps: 25, steps per second: 14, episode reward: 25.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.520 [0.000, 1.000], mean observation: 0.084 [-0.606, 0.927], mean_best_reward: --\n 25935/100000: episode: 1359, duration: 1.303s, episode steps: 19, steps per second: 15, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.526 [0.000, 1.000], mean observation: -0.088 [-1.043, 0.548], mean_best_reward: --\n 25961/100000: episode: 1360, duration: 2.098s, episode steps: 26, steps per second: 12, episode reward: 26.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.692 [0.000, 1.000], mean observation: -0.023 [-2.695, 1.887], mean_best_reward: --\n 26096/100000: episode: 1361, duration: 10.803s, episode steps: 135, steps per second: 12, episode reward: 135.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.504 [0.000, 1.000], mean observation: -0.059 [-1.306, 1.224], mean_best_reward: --\n 26112/100000: episode: 1362, duration: 1.201s, episode steps: 16, steps per second: 13, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.094 [-1.191, 0.640], mean_best_reward: --\n 26134/100000: episode: 1363, duration: 1.682s, episode steps: 22, steps per second: 13, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.682 [0.000, 1.000], mean observation: -0.045 [-2.433, 1.546], mean_best_reward: --\n 26148/100000: episode: 1364, duration: 1.506s, episode steps: 14, steps per second: 9, episode reward: 14.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.571 [0.000, 1.000], mean observation: -0.102 [-1.239, 0.822], mean_best_reward: --\n 26195/100000: episode: 1365, duration: 3.811s, episode steps: 47, steps per second: 12, episode reward: 47.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.447 [0.000, 1.000], mean observation: -0.131 [-0.883, 0.306], mean_best_reward: --\n 26216/100000: episode: 1366, duration: 1.991s, episode steps: 21, steps per second: 11, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.476 [0.000, 1.000], mean observation: 0.072 [-1.021, 1.457], mean_best_reward: --\n 26236/100000: episode: 1367, duration: 1.701s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.550 [0.000, 1.000], mean observation: -0.092 [-1.393, 0.632], mean_best_reward: --\n 26271/100000: episode: 1368, duration: 2.494s, episode steps: 35, steps per second: 14, episode reward: 35.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.514 [0.000, 1.000], mean observation: 0.099 [-0.557, 0.706], mean_best_reward: --\n 26292/100000: episode: 1369, duration: 1.796s, episode steps: 21, steps per second: 12, episode reward: 21.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.524 [0.000, 1.000], mean observation: -0.061 [-1.383, 0.813], mean_best_reward: --\n 26308/100000: episode: 1370, duration: 1.112s, episode steps: 16, steps per second: 14, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.094 [-1.245, 0.801], mean_best_reward: --\n 26372/100000: episode: 1371, duration: 4.695s, episode steps: 64, steps per second: 14, episode reward: 64.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.531 [0.000, 1.000], mean observation: 0.200 [-1.249, 1.671], mean_best_reward: --\n 26394/100000: episode: 1372, duration: 2.293s, episode steps: 22, steps per second: 10, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.545 [0.000, 1.000], mean observation: -0.049 [-1.130, 0.618], mean_best_reward: --\n 26454/100000: episode: 1373, duration: 5.103s, episode steps: 60, steps per second: 12, episode reward: 60.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.483 [0.000, 1.000], mean observation: -0.150 [-1.190, 0.536], mean_best_reward: --\n 26469/100000: episode: 1374, duration: 0.988s, episode steps: 15, steps per second: 15, episode reward: 15.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.600 [0.000, 1.000], mean observation: -0.090 [-1.334, 0.789], mean_best_reward: --\n 26499/100000: episode: 1375, duration: 1.812s, episode steps: 30, steps per second: 17, episode reward: 30.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.038 [-1.303, 0.795], mean_best_reward: --\n 26510/100000: episode: 1376, duration: 0.793s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.727 [0.000, 1.000], mean observation: -0.138 [-1.826, 0.962], mean_best_reward: --\n 26521/100000: episode: 1377, duration: 0.799s, episode steps: 11, steps per second: 14, episode reward: 11.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.818 [0.000, 1.000], mean observation: -0.144 [-2.496, 1.546], mean_best_reward: --\n 26543/100000: episode: 1378, duration: 1.300s, episode steps: 22, steps per second: 17, episode reward: 22.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.591 [0.000, 1.000], mean observation: -0.041 [-1.547, 0.841], mean_best_reward: --\n 26562/100000: episode: 1379, duration: 1.198s, episode steps: 19, steps per second: 16, episode reward: 19.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.684 [0.000, 1.000], mean observation: -0.067 [-2.340, 1.416], mean_best_reward: --\n 26578/100000: episode: 1380, duration: 1.607s, episode steps: 16, steps per second: 10, episode reward: 16.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.562 [0.000, 1.000], mean observation: -0.104 [-1.108, 0.550], mean_best_reward: --\n 26598/100000: episode: 1381, duration: 1.685s, episode steps: 20, steps per second: 12, episode reward: 20.000, mean reward: 1.000 [1.000, 1.000], mean action: 0.500 [0.000, 1.000], mean observation: -0.078 [-1.501, 1.020], mean_best_reward: --\n"}]},{"cell_type":"code","source":"","metadata":{"id":"IsXFf_wgYt3S"},"execution_count":null,"outputs":[]}]}